{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEPS Dataset Analysis\n",
    "\n",
    "So we have two files - one with basic demographic info and one with medications. They're small and pandas will deal with them fine - no need for SQL in an offline analysis setting. We'll just want to join them on id to make predictions.\n",
    "\n",
    "Some obvious things to keep in mind from looking at them -  \n",
    "1) a significant proportion of subjects are younger than sixteen. For these individuals, we have basically zero info - best to drop them from our tables and not make predictions for them. A cursory search of meds reveals no medication info for them.  \n",
    "2) We have tons of duplicate meds - we'll probably want to consolidate these. \n",
    "\n",
    "Since this is an informal ad hoc analysis, I'll just be checking results of operations rather than building out proper tests. Of course, this isn't intended to look like production code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# python=3.6.2, but I've tested this file with Python 2.7.13 as of 2017/06/12\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "sns.set_palette('pastel')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Summary of data: \n",
      "\n",
      "\n",
      "Preview: \n",
      "   Unnamed: 0        id  panel  pooledWeight  age     sex      race  \\\n",
      "0           1  10007101     15   3603.881236   28    Male     White   \n",
      "1           2  10007102     15   2544.550424   25  Female     White   \n",
      "2           3  10007103     15   4050.397468    4    Male     White   \n",
      "3           4  10007104     15   3064.059720    3  Female     White   \n",
      "4           5  10008101     15   3635.552466   51    Male  Multiple   \n",
      "\n",
      "                   married highBPDiagnosed diabetesDiagnosed  chdDiagnosed  \\\n",
      "0                  MARRIED             Yes                No            No   \n",
      "1                  MARRIED              No                No            No   \n",
      "2  UNDER 16 - INAPPLICABLE    Inapplicable      Inapplicable  Inapplicable   \n",
      "3  UNDER 16 - INAPPLICABLE    Inapplicable      Inapplicable  Inapplicable   \n",
      "4                  MARRIED              No                No            No   \n",
      "\n",
      "    miDiagnosed anginaDiagnosed strokeDiagnosed emphysemaDiagnosed  \\\n",
      "0            No              No              No                 No   \n",
      "1            No              No              No                 No   \n",
      "2  Inapplicable    Inapplicable    Inapplicable       Inapplicable   \n",
      "3  Inapplicable    Inapplicable    Inapplicable       Inapplicable   \n",
      "4            No              No              No                 No   \n",
      "\n",
      "  asthmaDiagnosed otherHDDiagnosed heartFailureDiagnosed  \n",
      "0              No               No                    No  \n",
      "1             Yes               No                    No  \n",
      "2              No     Inapplicable                    No  \n",
      "3              No     Inapplicable                    No  \n",
      "4              No               No                    No  \n",
      "\n",
      "\n",
      " Stats: \n",
      "         Unnamed: 0            id         panel  pooledWeight           age\n",
      "count  61489.000000  6.148900e+04  61489.000000  61489.000000  61489.000000\n",
      "mean   30745.000000  5.534638e+07     13.534453   5063.701982     33.578396\n",
      "std    17750.489688  2.759592e+07      1.061329   3815.885387     22.887576\n",
      "min        1.000000  1.000710e+07     12.000000    127.710358     -1.000000\n",
      "25%    15373.000000  4.045510e+07     13.000000   2217.419038     14.000000\n",
      "50%    30745.000000  4.965010e+07     14.000000   3989.180418     32.000000\n",
      "75%    46117.000000  8.161711e+07     14.000000   6905.677619     51.000000\n",
      "max    61489.000000  8.968810e+07     15.000000  38828.153564     85.000000\n",
      "\n",
      "\n",
      "\n",
      "Preview: \n",
      "   Unnamed: 0        id  rxStartMonth  rxStartYear  \\\n",
      "0           1  10007104             3         2011   \n",
      "1           2  10007104             3         2011   \n",
      "2           3  10008102             3         2011   \n",
      "3           4  10008102             3         2011   \n",
      "4           5  10008102             9         2011   \n",
      "\n",
      "                           rxName        rxNDC  rxQuantity rxForm  \n",
      "0                     AMOXICILLIN    143988775        75.0   SUSR  \n",
      "1              OTIC EDGE SOLUTION  68032032814        14.0    SOL  \n",
      "2  NASAL DECONGESTANT 0.05% SPRAY  63981056903        15.0    SPR  \n",
      "3  NASAL DECONGESTANT 0.05% SPRAY  63981056903        15.0    SPR  \n",
      "4                    DIPHENHYDRAM    603333921        30.0    CAP  \n",
      "\n",
      "\n",
      " Stats: \n",
      "         Unnamed: 0            id  rxStartMonth   rxStartYear         rxNDC  \\\n",
      "count  1.148347e+06  1.148347e+06  1.148347e+06  1.148347e+06  1.148347e+06   \n",
      "mean   1.240252e+06  5.523596e+07  5.184600e-01  1.172427e+03  2.226109e+10   \n",
      "std    9.590400e+05  2.728584e+07  4.087907e+00  9.895660e+02  2.834269e+10   \n",
      "min    1.000000e+00  1.000710e+07 -9.000000e+00 -1.400000e+01 -9.000000e+00   \n",
      "25%    2.870875e+05  4.063810e+07 -1.000000e+00 -1.000000e+00  1.490472e+08   \n",
      "50%    1.093262e+06  4.947510e+07 -1.000000e+00  2.000000e+03  5.910385e+08   \n",
      "75%    2.103602e+06  8.118210e+07  1.000000e+00  2.008000e+03  5.486835e+10   \n",
      "max    3.336212e+06  8.968810e+07  1.200000e+01  2.011000e+03  9.920707e+10   \n",
      "\n",
      "         rxQuantity  \n",
      "count  1.148347e+06  \n",
      "mean   5.942380e+01  \n",
      "std    3.702845e+02  \n",
      "min   -9.000000e+00  \n",
      "25%    3.000000e+01  \n",
      "50%    3.000000e+01  \n",
      "75%    6.800000e+01  \n",
      "max    1.200000e+05  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data.\n",
    "\n",
    "subjects = pd.read_csv(\"./input/meps_base_data.csv\")\n",
    "meds = pd.read_csv(\"./input/meps_meds.csv\")\n",
    "\n",
    "\n",
    "def display_summary(dfs_to_describe, preview_size=5):\n",
    "    print(\"\\n\\n Summary of data: \\n\\n\")\n",
    "    \n",
    "    for df in dfs_to_describe:\n",
    "\n",
    "        print(\"Preview: \")\n",
    "        print(df.head(preview_size))\n",
    "        print(\"\\n\\n Stats: \")\n",
    "        print(df.describe())\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    \n",
    "display_summary([subjects,meds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup  \n",
    "\n",
    "Let's think about what data we can drop -  \n",
    "\n",
    "1) from our subjects,  \n",
    "    a) `panel` seems to be panel year and is unlikely to be terribly useful to us, at least for now. With a sufficiently large amount of data, it could be used in tree algorithms and the like as-is. Or we could subdivide by groups of years and treat as a categorical so that a classifier could find trends for each period of time bin.  \n",
    "    b) `pooledWeight` seems to be used for a weighting function / to correct for demographics. I couldn't immediately find details on how to use the weighting function, so I'm ignoring it for now.  \n",
    "    \n",
    "2)  from our medications\n",
    "    a) we have tons of duplicates of drugs - presumably we have one record per prescription written; let's just keep one record of a prescription of a given drug per person (we'll assume that they've stayed on it)  \n",
    "    b) right now, let's drop a few columns - rxNDC is apparently a \"NATIONAL DRUG CODE (IMPUTED)\"; this might initially seem useful, but it actually isn't because there are multiple codes for many drugs    \n",
    "    \n",
    "Let's also drop `rxStartMonth` - we could use this and rxStartYear to build a more standardized datetime scheme (ignoring that months are missing very frequently), but for now let's just use year; year could be useful - lets us control for differing prescriptions used to treat diseases as medicine progresses.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Summary of data: \n",
      "\n",
      "\n",
      "Preview: \n",
      "         id  age     sex      race        married highBPDiagnosed  \\\n",
      "0  10007101   28    Male     White        MARRIED             Yes   \n",
      "1  10007102   25  Female     White        MARRIED              No   \n",
      "4  10008101   51    Male  Multiple        MARRIED              No   \n",
      "5  10008102   53  Female     Asian        MARRIED              No   \n",
      "7  10009101   61  Female     Black  NEVER MARRIED             Yes   \n",
      "\n",
      "  diabetesDiagnosed chdDiagnosed miDiagnosed anginaDiagnosed strokeDiagnosed  \\\n",
      "0                No           No          No              No              No   \n",
      "1                No           No          No              No              No   \n",
      "4                No           No          No              No              No   \n",
      "5                No           No          No              No              No   \n",
      "7                No           No          No              No              No   \n",
      "\n",
      "  emphysemaDiagnosed asthmaDiagnosed otherHDDiagnosed heartFailureDiagnosed  \n",
      "0                 No              No               No                    No  \n",
      "1                 No             Yes               No                    No  \n",
      "4                 No              No               No                    No  \n",
      "5                 No              No               No                    No  \n",
      "7                 No              No               No                    No  \n",
      "\n",
      "\n",
      " Stats: \n",
      "                 id           age\n",
      "count  4.426600e+04  44266.000000\n",
      "mean   5.512156e+07     44.062667\n",
      "std    2.755210e+07     18.037354\n",
      "min    1.000710e+07     16.000000\n",
      "25%    4.040310e+07     29.000000\n",
      "50%    4.949510e+07     43.000000\n",
      "75%    8.143210e+07     57.000000\n",
      "max    8.968810e+07     85.000000\n",
      "\n",
      "\n",
      "\n",
      "Preview: \n",
      "           id  rxStartYear                          rxName  rxQuantity rxForm\n",
      "0  10007104.0       2011.0                     AMOXICILLIN        75.0   SUSR\n",
      "1  10007104.0       2011.0              OTIC EDGE SOLUTION        14.0    SOL\n",
      "2  10008102.0       2011.0  NASAL DECONGESTANT 0.05% SPRAY        15.0    SPR\n",
      "4  10008102.0       2011.0                    DIPHENHYDRAM        30.0    CAP\n",
      "5  10008102.0       2011.0                    CHLD ALLERGY       100.0   LIQD\n",
      "\n",
      "\n",
      " Stats: \n",
      "                 id    rxStartYear     rxQuantity\n",
      "count  1.966740e+05  196674.000000  196674.000000\n",
      "mean   5.500321e+07    2006.451618      62.511634\n",
      "std    2.709755e+07       5.427340     464.765955\n",
      "min    2.005437e+03    1940.000000      -9.000000\n",
      "25%    4.080210e+07    2006.000000      21.000000\n",
      "50%    4.907110e+07    2008.000000      30.000000\n",
      "75%    8.126610e+07    2009.000000      63.000000\n",
      "max    8.968810e+07    2011.000000  120000.000000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a simple drop function.\n",
    "def drop_cols(df, cols):\n",
    "    # Find which cols in requested list are present.\n",
    "    cols_drop = list(set(df.columns) & set(cols))\n",
    "    if not cols_drop:\n",
    "        print('No cols found to drop.')\n",
    "    else:\n",
    "        # Inplace to avoid namespace/scope issues.\n",
    "        df.drop(cols, axis='columns', inplace=True)\n",
    "        \n",
    "# Drop fields mentioned above.\n",
    "drop_cols(subjects,['Unnamed: 0', 'panel','pooledWeight'])\n",
    "drop_cols(meds,['Unnamed: 0', 'rxNDC','rxStartMonth'])\n",
    "\n",
    "# Drop individuals younger than 16.\n",
    "younglings = subjects[subjects.age < 16].index\n",
    "try: subjects.drop(younglings, axis=0, inplace=True)\n",
    "except ValueError: print('Already dropped underage subjects.')\n",
    "\n",
    "# Impute mean prescription year for prescriptions with invalid years (e.g -8).  \n",
    "valid_by_year = meds[meds.rxStartYear > 1900]\n",
    "avg_rx_year = valid_by_year.rxStartYear.mean()\n",
    "# Apply that value to prescriptions with invalid years.  \n",
    "meds.loc[meds.rxStartYear < 1900] = avg_rx_year\n",
    "\n",
    "meds.drop_duplicates(subset = ['id','rxName'], inplace=True)\n",
    "\n",
    "display_summary([subjects,meds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JOIN\n",
    "\n",
    "I'm going to inner join, so we keep all prescriptions for people who we have demographic info.  \n",
    "It would also be reasonable to left join so that we keep info on all people even if they don't have any prescriptions. I went ahead and also tried a left join, and it only affected the number of subjects pretty marginally (by <10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cols before merge:\n",
      "['id' 'age' 'sex' 'race' 'married' 'highBPDiagnosed' 'diabetesDiagnosed'\n",
      " 'chdDiagnosed' 'miDiagnosed' 'anginaDiagnosed' 'strokeDiagnosed'\n",
      " 'emphysemaDiagnosed' 'asthmaDiagnosed' 'otherHDDiagnosed'\n",
      " 'heartFailureDiagnosed']\n",
      "['id' 'rxStartYear' 'rxName' 'rxQuantity' 'rxForm']\n",
      "Cols after merge:\n",
      "['id' 'age' 'sex' 'race' 'married' 'highBPDiagnosed' 'diabetesDiagnosed'\n",
      " 'chdDiagnosed' 'miDiagnosed' 'anginaDiagnosed' 'strokeDiagnosed'\n",
      " 'emphysemaDiagnosed' 'asthmaDiagnosed' 'otherHDDiagnosed'\n",
      " 'heartFailureDiagnosed' 'rxStartYear' 'rxName' 'rxQuantity' 'rxForm']\n",
      "(169868, 19)\n"
     ]
    }
   ],
   "source": [
    "subj_and_meds = pd.merge(subjects, meds, how='inner', on='id', sort=True)\n",
    "print('Cols before merge:')\n",
    "print(subjects.columns.values)\n",
    "print(meds.columns.values)\n",
    "print('Cols after merge:')\n",
    "print(subj_and_meds.columns.values)\n",
    "print(subj_and_meds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visually inspect result of JOIN to make sure it worked as expected.\n",
    "# subj_and_meds.to_csv(\"joined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Encoding  \n",
    "\n",
    "We currently don't have much of our info in an easily-interpretable form for our model(s) - we'll want to re-encode a whole bunch of categorial variables -  \n",
    "\n",
    "1) We need to turn our sex column and diagnoses into a boolean value - e.g. \"isFemale\"   \n",
    "2) We need to one-hot encode race and married. (If we were ignoring logisitic regression and its kin, factorizing with e.g. pd.factorize() would also work and would result in a slightly smaller memory footprint and faster model training for tree-based models.)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of diagnosis col before value correction:\n",
      "0    No\n",
      "1    No\n",
      "2    No\n",
      "3    No\n",
      "4    No\n",
      "Name: diabetesDiagnosed, dtype: object\n",
      "\n",
      " Example of diagnosis col after value correction:\n",
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: diabetesDiagnosed, dtype: bool\n",
      "\n",
      " Cols incl. newly-created bools:\n",
      "Index(['id', 'age', 'race', 'married', 'highBPDiagnosed', 'diabetesDiagnosed',\n",
      "       'chdDiagnosed', 'miDiagnosed', 'anginaDiagnosed', 'strokeDiagnosed',\n",
      "       'emphysemaDiagnosed', 'asthmaDiagnosed', 'otherHDDiagnosed',\n",
      "       'heartFailureDiagnosed', 'rxStartYear', 'rxName', 'rxQuantity',\n",
      "       'rxForm', 'isFemale'],\n",
      "      dtype='object')\n",
      "Columns including one-hot encoded cols: \n",
      "Index(['id', 'age', 'highBPDiagnosed', 'diabetesDiagnosed', 'chdDiagnosed',\n",
      "       'miDiagnosed', 'anginaDiagnosed', 'strokeDiagnosed',\n",
      "       'emphysemaDiagnosed', 'asthmaDiagnosed', 'otherHDDiagnosed',\n",
      "       'heartFailureDiagnosed', 'rxStartYear', 'rxName', 'rxQuantity',\n",
      "       'rxForm', 'isFemale', 'd__Amer Indian/Alaska Native', 'd__Asian',\n",
      "       'd__Black', 'd__Multiple', 'd__Native Hawaiian/Pacific Islander',\n",
      "       'd__White', 'd__DIVORCED', 'd__DIVORCED IN ROUND', 'd__MARRIED',\n",
      "       'd__MARRIED IN ROUND', 'd__NEVER MARRIED', 'd__SEPARATED',\n",
      "       'd__SEPARATED IN ROUND', 'd__WIDOWED', 'd__WIDOWED IN ROUND'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      " Now including dummies: \n",
      "\n",
      "\n",
      "No cols found to drop.\n",
      "Index(['id', 'age', 'highBPDiagnosed', 'diabetesDiagnosed', 'chdDiagnosed',\n",
      "       'miDiagnosed', 'anginaDiagnosed', 'strokeDiagnosed',\n",
      "       'emphysemaDiagnosed', 'asthmaDiagnosed', 'otherHDDiagnosed',\n",
      "       'heartFailureDiagnosed', 'rxStartYear', 'rxName', 'rxQuantity',\n",
      "       'rxForm', 'isFemale', 'd__Amer Indian/Alaska Native', 'd__Asian',\n",
      "       'd__Black', 'd__Multiple', 'd__Native Hawaiian/Pacific Islander',\n",
      "       'd__White', 'd__DIVORCED', 'd__DIVORCED IN ROUND', 'd__MARRIED',\n",
      "       'd__MARRIED IN ROUND', 'd__NEVER MARRIED', 'd__SEPARATED',\n",
      "       'd__SEPARATED IN ROUND', 'd__WIDOWED', 'd__WIDOWED IN ROUND'],\n",
      "      dtype='object')\n",
      "         id  age  highBPDiagnosed  diabetesDiagnosed  chdDiagnosed  \\\n",
      "0  10007101   28             True              False         False   \n",
      "1  10007101   28             True              False         False   \n",
      "2  10007102   25            False              False         False   \n",
      "\n",
      "   miDiagnosed  anginaDiagnosed  strokeDiagnosed  emphysemaDiagnosed  \\\n",
      "0        False            False            False               False   \n",
      "1        False            False            False               False   \n",
      "2        False            False            False               False   \n",
      "\n",
      "   asthmaDiagnosed  otherHDDiagnosed  heartFailureDiagnosed  rxStartYear  \\\n",
      "0            False             False                  False       2005.0   \n",
      "1            False             False                  False       2010.0   \n",
      "2             True             False                  False       2009.0   \n",
      "\n",
      "         rxName  rxQuantity rxForm  isFemale  d__Amer Indian/Alaska Native  \\\n",
      "0      ATENOLOL        30.0   TABS     False                             0   \n",
      "1  AZITHROMYCIN        30.0   SUSR     False                             0   \n",
      "2      TREXIMET        18.0   TABS      True                             0   \n",
      "\n",
      "   d__Asian  d__Black  d__Multiple  d__Native Hawaiian/Pacific Islander  \\\n",
      "0         0         0            0                                    0   \n",
      "1         0         0            0                                    0   \n",
      "2         0         0            0                                    0   \n",
      "\n",
      "   d__White  d__DIVORCED  d__DIVORCED IN ROUND  d__MARRIED  \\\n",
      "0         1            0                     0           1   \n",
      "1         1            0                     0           1   \n",
      "2         1            0                     0           1   \n",
      "\n",
      "   d__MARRIED IN ROUND  d__NEVER MARRIED  d__SEPARATED  d__SEPARATED IN ROUND  \\\n",
      "0                    0                 0             0                      0   \n",
      "1                    0                 0             0                      0   \n",
      "2                    0                 0             0                      0   \n",
      "\n",
      "   d__WIDOWED  d__WIDOWED IN ROUND  \n",
      "0           0                    0  \n",
      "1           0                    0  \n",
      "2           0                    0  \n"
     ]
    }
   ],
   "source": [
    "# Start with re-encoding sex.\n",
    "\n",
    "try: subj_and_meds['isFemale'] = subj_and_meds.sex == \"Female\"\n",
    "except AttributeError: print(\"isFemale already created. Skipping.\")\n",
    "drop_cols(subj_and_meds, ['sex'])\n",
    "\n",
    "print('Example of diagnosis col before value correction:')\n",
    "print(subj_and_meds.diabetesDiagnosed.iloc[:5])\n",
    "\n",
    "# Now we'll re-encode all the diagnoses.\n",
    "# or == True so I can run this code multiple times without getting glitches\n",
    "# (Don't want to force you to restart the notebook if you run a cell twice.)\n",
    "yn_to_bool = lambda value: value == \"Yes\" or value == True \n",
    "diagnoses = ['highBPDiagnosed', 'diabetesDiagnosed', 'chdDiagnosed', 'miDiagnosed', 'anginaDiagnosed',\n",
    "             'strokeDiagnosed', 'emphysemaDiagnosed', 'asthmaDiagnosed', 'otherHDDiagnosed', 'heartFailureDiagnosed']\n",
    "# for diagnosis in diagnoses:\n",
    "#     subj_and_meds[diagnosis] = subj_and_meds[diagnosis].apply(yn_to_bool)\n",
    "subj_and_meds[diagnoses] = subj_and_meds[diagnoses].applymap(yn_to_bool)\n",
    "\n",
    "print('\\n Example of diagnosis col after value correction:')\n",
    "print(subj_and_meds.diabetesDiagnosed.iloc[:5])\n",
    "    \n",
    "print('\\n Cols incl. newly-created bools:')\n",
    "print(subj_and_meds.columns)\n",
    "\n",
    "\n",
    "# For the sake of code cleanliness, I'll create both sets here, but you could argue that \n",
    "# I should create them separately to have more descriptive variable names (e.g. \"race_\"\n",
    "# and \"marital_\" prefixes rather than \"d_\")\n",
    "try: \n",
    "    subj_and_meds = pd.get_dummies(subj_and_meds, columns = ['race', 'married'], prefix='d_')\n",
    "except ValueError: \n",
    "    print(\"Couldn't find dummy sources - likely already created.\")\n",
    "print('Columns including one-hot encoded cols: ')\n",
    "print(subj_and_meds.columns)\n",
    "\n",
    "\n",
    "print(\"\\n\\n Now including dummies: \\n\\n\")\n",
    "drop_cols(subj_and_meds, ['race', 'married'])\n",
    "print(subj_and_meds.columns)\n",
    "print(subj_and_meds.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visually inspect result of concat above to make sure it worked as expected\n",
    "# subj_and_meds.to_csv(\"with_dummies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that we have categories that we might want to merge - e.g. \"widowed in round\" is pretty rare and should, perhaps, be merged into \"widowed.\"  \n",
    "One other caveat - we have no default for our dummy variables, which will cause multicollinearity issues with regression models - we should fix this later if we want to use them.\n",
    "\n",
    "## Let's actually do some analyses :)\n",
    "\n",
    "#### 1) What are the most common medications for each disease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " For diagnosis highBPDiagnosed, most common prescription is: \n",
      "\n",
      "count          93623\n",
      "unique          5650\n",
      "top       LISINOPRIL\n",
      "freq            2476\n",
      "Name: rxName, dtype: object\n",
      "\n",
      "\n",
      " For diagnosis diabetesDiagnosed, most common prescription is: \n",
      "\n",
      "count         40116\n",
      "unique         3823\n",
      "top       METFORMIN\n",
      "freq           1158\n",
      "Name: rxName, dtype: object\n",
      "\n",
      "\n",
      " For diagnosis chdDiagnosed, most common prescription is: \n",
      "\n",
      "count          22039\n",
      "unique          2867\n",
      "top       LISINOPRIL\n",
      "freq             466\n",
      "Name: rxName, dtype: object\n",
      "\n",
      "\n",
      " For diagnosis miDiagnosed, most common prescription is: \n",
      "\n",
      "count          14725\n",
      "unique          2333\n",
      "top       LISINOPRIL\n",
      "freq             339\n",
      "Name: rxName, dtype: object\n",
      "\n",
      "\n",
      " For diagnosis anginaDiagnosed, most common prescription is: \n",
      "\n",
      "count          11603\n",
      "unique          2164\n",
      "top       LISINOPRIL\n",
      "freq             194\n",
      "Name: rxName, dtype: object\n",
      "\n",
      "\n",
      " For diagnosis strokeDiagnosed, most common prescription is: \n",
      "\n",
      "count          14319\n",
      "unique          2464\n",
      "top       LISINOPRIL\n",
      "freq             283\n",
      "Name: rxName, dtype: object\n",
      "\n",
      "\n",
      " For diagnosis emphysemaDiagnosed, most common prescription is: \n",
      "\n",
      "count           9594\n",
      "unique          1997\n",
      "top       LISINOPRIL\n",
      "freq             145\n",
      "Name: rxName, dtype: object\n",
      "\n",
      "\n",
      " For diagnosis asthmaDiagnosed, most common prescription is: \n",
      "\n",
      "count         28376\n",
      "unique         3568\n",
      "top       ALBUTEROL\n",
      "freq            536\n",
      "Name: rxName, dtype: object\n",
      "\n",
      "\n",
      " For diagnosis otherHDDiagnosed, most common prescription is: \n",
      "\n",
      "count          31770\n",
      "unique          3715\n",
      "top       LISINOPRIL\n",
      "freq             584\n",
      "Name: rxName, dtype: object\n",
      "\n",
      "\n",
      " For diagnosis heartFailureDiagnosed, most common prescription is: \n",
      "\n",
      "count           4940\n",
      "unique          1350\n",
      "top       FUROSEMIDE\n",
      "freq             173\n",
      "Name: rxName, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO: this could be expressed in a more efficient manner as a .groupby(), \n",
    "#   although I think the for may be more readable.\n",
    "# Also, you'd have to convert the one-hot-encoded diseases into a single categorical column, \n",
    "#   and handling individuals with multiple diseases would be a pain.\n",
    "\n",
    "for diagnosis in diagnoses:\n",
    "    current_diagnosis = subj_and_meds[subj_and_meds[diagnosis] == True]\n",
    "    print(\"\\n\\n For diagnosis %s, most common prescription is: \\n\" % diagnosis)\n",
    "    print(current_diagnosis['rxName'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gut check - lisinopril is an ACE inhibitor, and ACE inhibitors are a frontline treatment for hypertension. Metformin is a frontline treatment for type 2 diabetes (and type 2 is far more common than type 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Which medications are most indicative of each disease?\n",
    "\n",
    "Let's figure out which drug is the most specific for each disease. So I'll calculate:\n",
    "\n",
    "instances of drug for disease / instances of drug overall\n",
    "\n",
    "In other words, I'm getting the precision of a given drug in making a diagnosis for a given disease.\n",
    "\n",
    "One caveat: if we have drugs with exceedingly low counts - say one doctor has homeopathic leanings and prescribes a diabetic patient ginseng - it could seemingly be a perfect identifier by this metric. Instead, I'll only look at the drugs that are reasonably common - using exceedingly rare drugs to diagnoses common diseases seems like it would only be marginally useful, anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of drugs extracted: 100\n",
      "\n",
      "Most common drugs that we've extracted:\n",
      "         rxName  count\n",
      "0  AZITHROMYCIN   3358\n",
      "1    LISINOPRIL   2865\n",
      "2   AMOXICILLIN   2687\n",
      "3   SIMVASTATIN   2312\n",
      "4     IBUPROFEN   2243\n",
      "5    PREDNISONE   1712\n",
      "6  HYDROCO/APAP   1703\n",
      "7       LIPITOR   1630\n",
      "8    OMEPRAZOLE   1496\n",
      "9     METFORMIN   1422\n",
      "Least common drugs that we've extracted:\n",
      "                   rxName  count\n",
      "90             LORATADINE    309\n",
      "91           ADVAIR DISKU    308\n",
      "92         APAP/OXYCODONE    306\n",
      "93             DIOVAN HCT    303\n",
      "94             SMZ/TMP DS    302\n",
      "95            GLIMEPIRIDE    302\n",
      "96           PENICILLN VK    294\n",
      "97  ACETAMINOPHEN/CODEINE    294\n",
      "98                NORVASC    293\n",
      "99     PRAVASTATIN SODIUM    293\n"
     ]
    }
   ],
   "source": [
    "# We'll take our list of prescriptions, and get their counts.\n",
    "# All of our most common drugs per diagnosis above have high frequencies (>1000), \n",
    "#   but I'll go ahead and grab the top 100 to be safe.\n",
    "# Use counts as the index, so we have them sorted by descending frequency.\n",
    "# Using function-chaining syntax here. I find it pretty readable, but some people don't.\n",
    "drugs = (\n",
    "    subj_and_meds['rxName']\n",
    "        # Get value_counts() for drugs - it's sorted and descending by default, \n",
    "        # but I've specified them as kwargs for maximum explicitness.\n",
    "        .value_counts(sort=True, ascending=False)  \n",
    "        # Get the top 100 most-common.\n",
    "        .iloc[0:100]\n",
    "        .reset_index(drop=False)\n",
    ")\n",
    "\n",
    "# Fix column names.\n",
    "drugs.rename(columns={'index': 'rxName', 'rxName': 'count'}, inplace=True)\n",
    "\n",
    "# Confirm size of list.\n",
    "print('Number of drugs extracted: %i' % drugs.rxName.size)\n",
    "print('')\n",
    "# Confirm that our most common drugs are here, indexed by count.\n",
    "print(\"Most common drugs that we've extracted:\")\n",
    "print(drugs.head(10))\n",
    "print(\"Least common drugs that we've extracted:\")\n",
    "print(drugs.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drug_precisions = list()\n",
    "\n",
    "# Iterate through each dx and each drug - O(n^2)\n",
    "# There's plenty of room to optimize this code if it gets run frequently\n",
    "for drug in drugs.rxName:\n",
    "    # get number of users for the drug\n",
    "    users = float(subj_and_meds[subj_and_meds['rxName'] == drug]['rxName'].size)\n",
    "    # checked that these counts are equal to the counts above - commented it out now\n",
    "    for diagnosis in diagnoses:\n",
    "        # get number of users of the drug who have the diagnosis\n",
    "        fil_dx_and_rx = ((subj_and_meds[diagnosis] == True) & (subj_and_meds['rxName'] == drug))\n",
    "        users_with_dx = subj_and_meds[fil_dx_and_rx]['rxName'].size\n",
    "        this_precision = users_with_dx / users\n",
    "        drug_precisions.append({'rxName': drug, 'diagnosis': diagnosis, 'precision': this_precision})\n",
    "        \n",
    "# print(drug_precisions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           diagnosis  precision        rxName\n",
      "0    highBPDiagnosed   0.335616  AZITHROMYCIN\n",
      "1  diabetesDiagnosed   0.097677  AZITHROMYCIN\n",
      "2       chdDiagnosed   0.060453  AZITHROMYCIN\n",
      "3        miDiagnosed   0.035736  AZITHROMYCIN\n",
      "4    anginaDiagnosed   0.033949  AZITHROMYCIN\n"
     ]
    }
   ],
   "source": [
    "# Convert list of dict rows to pd.DataFrame. \n",
    "# (Want to do this all at once since extending a DataFrame is an expensive operation.)\n",
    "precisions = pd.DataFrame(drug_precisions)\n",
    "print(precisions.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 diagnosis  precision        rxName\n",
      "669  heartFailureDiagnosed   0.204188    CARVEDILOL\n",
      "916     emphysemaDiagnosed   0.211039  ADVAIR DISKU\n",
      "324        anginaDiagnosed   0.296117        PLAVIX\n",
      "325        strokeDiagnosed   0.307443        PLAVIX\n",
      "323            miDiagnosed   0.401294        PLAVIX\n",
      "322           chdDiagnosed   0.529126        PLAVIX\n",
      "698       otherHDDiagnosed   0.552632      WARFARIN\n",
      "917        asthmaDiagnosed   0.675325  ADVAIR DISKU\n",
      "501      diabetesDiagnosed   0.965591         ACTOS\n",
      "930        highBPDiagnosed   0.970297    DIOVAN HCT\n"
     ]
    }
   ],
   "source": [
    "# Now get the max precision for each diagnosis. \n",
    "# It would be cleaner to just groupby diagnosis and get the max precision,\n",
    "#   but that wouldn't yield the drug name for each max.\n",
    "# Instead, we'll sort our precisions and then take the max for each diagnosis.\n",
    "\n",
    "sorted_ = precisions.sort_values(by='precision')\n",
    "max_precisions = sorted_.drop_duplicates('diagnosis', keep='last').sort_values(by='precision')\n",
    "print(max_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABC0AAAFKCAYAAAA5RaVOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIayaSFJEmSJElqpP8HEX/CUvf3KOIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e5fb290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# FIXME!\n",
    "\n",
    "formatted_dx = ['High BP, Diovan', 'Diabetes, Actos', 'CHD, Plavix', 'MI, Plavix',\n",
    "                'Angina, Plavix', 'Stroke, Plavix', 'Emphysema, Advair', 'Asthma, Advair',\n",
    "                'Other HD, Warfarin', 'Heart Failure, Carvedilol']\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "plt.title('Diagnostic Sensitivities')\n",
    "\n",
    "ax = sns.barplot(x=max_precisions.diagnosis, y=max_precisions.precision, data=precisions)\n",
    "ax.set_ylabel(\"Precision\"); ax.set_xlabel(\"Diagnosis, Drug\")\n",
    "ax.set_xticklabels(formatted_dx)\n",
    "ax.set_yticklabels(['0%', '20%', '40%', '60%', '80%', '100%'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gut check - these make sense - Diovan HCT is a combo ACE inhibitor and diuretic, two extremely common classes of drugs for high blood pressure; Plavix is an anticoagulant, so it's used for various heart diseases; advair is a bronchodilator, so it's used for respiratory disorders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Let's model one disease.\n",
    "\n",
    "First we have to pick a disease. If I want to pick one that's easy to model, it would be nice to choose one that has lots of instances and that is also well-distinguished by a small number of criteria. As you can see below, HBP would be an obvious choice, but HBP is really common, even in people who are mostly healthy. It would be cool to train a more complex model on it, but for the sake of simplicity, I'll model diabetes, which is a classic stats/ML problem. \n",
    "\n",
    "By the way, with regards to model performance, we probably care more about recall than precision in this instance - we really want to pick up cases of the disease based on medicines and don't care all that much if we falsely flag the potential presence of a disease. Also, raw accuracy isn't far off from worthless, since (most) of these disease are pretty rare. Diabetes affects around 10% of people in a random sample of Americans, so a model that predicts y=0 is 90% accurate in a random samples (and 74% accurate in our sample - our sample isn't a random sample of Americans, as we'll soon find out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects with highBPDiagnosed:\n",
      "93623\n",
      "Number of subjects with diabetesDiagnosed:\n",
      "40116\n",
      "Number of subjects with chdDiagnosed:\n",
      "22039\n",
      "Number of subjects with miDiagnosed:\n",
      "14725\n",
      "Number of subjects with anginaDiagnosed:\n",
      "11603\n",
      "Number of subjects with strokeDiagnosed:\n",
      "14319\n",
      "Number of subjects with emphysemaDiagnosed:\n",
      "9594\n",
      "Number of subjects with asthmaDiagnosed:\n",
      "28376\n",
      "Number of subjects with otherHDDiagnosed:\n",
      "31770\n",
      "Number of subjects with heartFailureDiagnosed:\n",
      "4940\n",
      "Number with disease highBPDiagnosed = 93623\n",
      "Number with disease diabetesDiagnosed = 40116\n",
      "Number with disease chdDiagnosed = 22039\n",
      "Number with disease miDiagnosed = 14725\n",
      "Number with disease anginaDiagnosed = 11603\n",
      "Number with disease strokeDiagnosed = 14319\n",
      "Number with disease emphysemaDiagnosed = 9594\n",
      "Number with disease asthmaDiagnosed = 28376\n",
      "Number with disease otherHDDiagnosed = 31770\n",
      "Number with disease heartFailureDiagnosed = 4940\n"
     ]
    }
   ],
   "source": [
    "# Let's pick.\n",
    "# Again, syntax here is slightly awkward because of one-hot encoded \n",
    "#   disease columns and potential for each subject to have multiple \n",
    "#   diagnoses.\n",
    "for diagnosis in diagnoses:\n",
    "    subj_with_dx = (\n",
    "        subj_and_meds\n",
    "            .groupby([diagnosis])  # For this diagnosis, True or False\n",
    "            .id  # Arbitrary column to count.\n",
    "            .count()  # Take the count.\n",
    "            [True]  # Only care about the count that have this disease.\n",
    "    )\n",
    "    print('Number of subjects with %s:' % diagnosis)\n",
    "    print(subj_with_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'age', 'diabetesDiagnosed', 'rxStartYear', 'rxName', 'rxQuantity',\n",
      "       'rxForm', 'isFemale', 'd__Amer Indian/Alaska Native', 'd__Asian',\n",
      "       'd__Black', 'd__Multiple', 'd__Native Hawaiian/Pacific Islander',\n",
      "       'd__White', 'd__DIVORCED', 'd__DIVORCED IN ROUND', 'd__MARRIED',\n",
      "       'd__MARRIED IN ROUND', 'd__NEVER MARRIED', 'd__SEPARATED',\n",
      "       'd__SEPARATED IN ROUND', 'd__WIDOWED', 'd__WIDOWED IN ROUND'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "diabetes_df = subj_and_meds.copy()\n",
    "\n",
    "# Drop other diagnoses from the set - while it would be lovely to have diagnosis info\n",
    "#   for the other diseases to train our model, it's not terribly realistic.\n",
    "other_diag = [diag for diag in diagnoses if diag is not 'diabetesDiagnosed']\n",
    "drop_cols(diabetes_df, other_diag)\n",
    "print(diabetes_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     169868\n",
       "unique         2\n",
       "top        False\n",
       "freq      129752\n",
       "Name: diabetesDiagnosed, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df['diabetesDiagnosed'].describe()\n",
    "\n",
    "# We have (169868-129752)/169868 = 23.6% with diabetes in our sample; \n",
    "#   this is more than 2x the population proportion!\n",
    "# In our sample, diabetesDiagnosed=True isn't a terribly rare class.\n",
    "#   (But we should still be wary of class imbalance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do our non-drug variables carry interesting information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6ffc22306bdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# it just shows diabetes incidence increases pretty dramatically with age.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# This isn't exactly shocking :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'diabetesDiagnosed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiabetes_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/linearmodels.py\u001b[0m in \u001b[0;36mlmplot\u001b[0;34m(x, y, data, hue, col, row, palette, col_wrap, size, aspect, markers, sharex, sharey, hue_order, col_order, row_order, legend, legend_out, x_estimator, x_bins, x_ci, scatter, fit_reg, ci, n_boot, units, order, logistic, lowess, robust, logx, x_partial, y_partial, truncate, x_jitter, y_jitter, scatter_kws, line_kws)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mscatter_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscatter_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_kws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         )\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0mfacets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mregplot_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;31m# Add a legend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/axisgrid.py\u001b[0m in \u001b[0;36mmap_dataframe\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# Draw the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_facet_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;31m# Finalize the annotations and layout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/axisgrid.py\u001b[0m in \u001b[0;36m_facet_plot\u001b[0;34m(self, func, ax, plot_args, plot_kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;31m# Draw the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mplot_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplot_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# Sort out the supporting information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/linearmodels.py\u001b[0m in \u001b[0;36mregplot\u001b[0;34m(x, y, data, x_estimator, x_bins, x_ci, scatter, fit_reg, ci, n_boot, units, order, logistic, lowess, robust, logx, x_partial, y_partial, truncate, dropna, x_jitter, y_jitter, label, color, marker, scatter_kws, line_kws, ax)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0mscatter_kws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"marker\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0mline_kws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline_kws\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m     \u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscatter_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/linearmodels.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, ax, scatter_kws, line_kws)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscatter_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_reg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m# Label the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/linearmodels.py\u001b[0m in \u001b[0;36mlineplot\u001b[0;34m(self, ax, kws)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# Fit the regression model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_bands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Get set default aesthetics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/linearmodels.py\u001b[0m in \u001b[0;36mfit_regression\u001b[0;34m(self, ax, x_range, grid)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_boots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneralized_linear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamilies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinomial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             yhat, yhat_boots = self.fit_statsmodels(grid, GLM,\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEFCAYAAAD62n4IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ12SpJow1CVJqglDXZKkmvj/7QFakqvOm4AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110895e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAHtCAYAAAA9X4aBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGgCaEmkAaOr/A6Uw/sqawiN5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bc29080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# let's see if our other fields have any useful info...\n",
    "ax = sns.barplot('isFemale','diabetesDiagnosed', data=diabetes_df)\n",
    "ax.set_ylabel('Proportion w/ diabetes')\n",
    "\n",
    "# Below regression takes around a while to run - feel free to comment out this code\n",
    "# it just shows diabetes incidence increases pretty dramatically with age.\n",
    "# This isn't exactly shocking :)\n",
    "sns.lmplot(x='age', y='diabetesDiagnosed', data=diabetes_df, size=7, logistic=True, n_boot=80, ci=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our train-test split - 100k+ rows so we have a good amount of data. Of course, we'l use cross-validation to get the most out of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset --> \n",
      "\n",
      "        age  rxStartYear  rxQuantity  isFemale  d__Amer Indian/Alaska Native  \\\n",
      "147565   59       2008.0        90.0      True                             0   \n",
      "1231     66       2010.0        60.0     False                             0   \n",
      "152541   61       2008.0        10.0      True                             0   \n",
      "\n",
      "        d__Asian  d__Black  d__Multiple  d__Native Hawaiian/Pacific Islander  \\\n",
      "147565         0         0            0                                    0   \n",
      "1231           0         0            0                                    0   \n",
      "152541         0         0            1                                    0   \n",
      "\n",
      "        d__White  d__DIVORCED  d__DIVORCED IN ROUND  d__MARRIED  \\\n",
      "147565         1            0                     0           0   \n",
      "1231           1            0                     0           1   \n",
      "152541         0            0                     0           1   \n",
      "\n",
      "        d__MARRIED IN ROUND  d__NEVER MARRIED  d__SEPARATED  \\\n",
      "147565                    0                 0             1   \n",
      "1231                      0                 0             0   \n",
      "152541                    0                 0             0   \n",
      "\n",
      "        d__SEPARATED IN ROUND  d__WIDOWED  d__WIDOWED IN ROUND  \n",
      "147565                      0           0                    0  \n",
      "1231                        0           0                    0  \n",
      "152541                      0           0                    0  \n",
      "\n",
      "Dataset --> \n",
      "\n",
      "       age  rxStartYear  rxQuantity  isFemale  d__Amer Indian/Alaska Native  \\\n",
      "66379   62       2009.0        30.0      True                             0   \n",
      "73405   45       2008.0        60.0      True                             0   \n",
      "29974   47       2008.0        30.0      True                             0   \n",
      "\n",
      "       d__Asian  d__Black  d__Multiple  d__Native Hawaiian/Pacific Islander  \\\n",
      "66379         0         1            0                                    0   \n",
      "73405         0         0            0                                    0   \n",
      "29974         0         1            0                                    0   \n",
      "\n",
      "       d__White  d__DIVORCED  d__DIVORCED IN ROUND  d__MARRIED  \\\n",
      "66379         0            1                     0           0   \n",
      "73405         1            0                     0           1   \n",
      "29974         0            0                     0           0   \n",
      "\n",
      "       d__MARRIED IN ROUND  d__NEVER MARRIED  d__SEPARATED  \\\n",
      "66379                    0                 0             0   \n",
      "73405                    0                 0             0   \n",
      "29974                    0                 0             1   \n",
      "\n",
      "       d__SEPARATED IN ROUND  d__WIDOWED  d__WIDOWED IN ROUND  \n",
      "66379                      0           0                    0  \n",
      "73405                      0           0                    0  \n",
      "29974                      0           0                    0  \n",
      "\n",
      "Dataset --> \n",
      "\n",
      "147565    False\n",
      "1231       True\n",
      "152541    False\n",
      "Name: diabetesDiagnosed, dtype: bool\n",
      "\n",
      "Dataset --> \n",
      "\n",
      "66379    True\n",
      "73405    True\n",
      "29974    True\n",
      "Name: diabetesDiagnosed, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# For now, drop our qualitative variables - obviously there's a tremendous amount of info\n",
    "#   in rxName, so we'll want to add them back later.\n",
    "# Need to drop 'id' since some people have multiple rx and show up twice. \n",
    "# Don't want model to memorize their disease status via ID col on training and apply on test.\n",
    "dropped_qual = diabetes_df.copy().drop(['rxName', 'rxForm', 'id'], axis=1)\n",
    "\n",
    "# TODO: refactor train-test split into function; \n",
    "# we'll still need the variables as globals so we can refer to them outside the function.\n",
    "\n",
    "# Split training and test\n",
    "# Large dataset (>100k rows), so 20% should be adequate for our test.\n",
    "# Since we have a nontrivial class imbalance, we'll want to stratify on our outcome.\n",
    "# (Of course, it's best practice to always stratify classification outcomes.)\n",
    "all_x = dropped_qual.copy().drop(['diabetesDiagnosed'], axis=1)\n",
    "all_y = dropped_qual.copy()['diabetesDiagnosed']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    all_x, all_y, test_size=.2, random_state=2001, stratify=all_y.values)\n",
    "\n",
    "for dataset in [X_train, X_test, Y_train, Y_test]:\n",
    "    print(\"\\nDataset --> \\n\")\n",
    "    print(dataset.head(3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split our train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "We're doing this to gain some insight into the underlying correlates of diabetes, not because it'll be an ideal model. (It won't.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, define a convenience function to print various model performance statistics:\n",
    "\n",
    "def score_classif_on_test(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Get classifier accuracy on the test set in terms of accuracy of its\n",
    "      probabilities (AUC and log-loss) and predictions (precision, recall, and F1).\n",
    "    \n",
    "    :model: sklearn model (i.e. with .fit() and .predict_proba() methods)\n",
    "    :param x_test: np array of x-values for test set\n",
    "    :param y_test: np array of y-values for test set\n",
    "    \"\"\"\n",
    "    probs = model.predict_proba(x_test)[:,1]\n",
    "\n",
    "    print('AUC Score:')\n",
    "    print(roc_auc_score(y_test, probs))\n",
    "    print('')\n",
    "\n",
    "    print('LogLoss:')\n",
    "    print(log_loss(y_test, probs))\n",
    "    print('')\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(classification_report(y_test, y_pred, digits=3))\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Normalized dataset --> \n",
      "\n",
      "              id  age  rxStartYear  rxQuantity isFemale  \\\n",
      "147565  85146101   59       2008.0        90.0     True   \n",
      "1231    10255101   66       2010.0        60.0    False   \n",
      "152541  86112101   61       2008.0        10.0     True   \n",
      "\n",
      "        d__Amer Indian/Alaska Native  d__Asian  d__Black  d__Multiple  \\\n",
      "147565                           0.0       0.0       0.0          0.0   \n",
      "1231                             0.0       0.0       0.0          0.0   \n",
      "152541                           0.0       0.0       0.0          1.0   \n",
      "\n",
      "        d__Native Hawaiian/Pacific Islander  d__White  d__DIVORCED  \\\n",
      "147565                                  0.0       1.0          0.0   \n",
      "1231                                    0.0       1.0          0.0   \n",
      "152541                                  0.0       0.0          0.0   \n",
      "\n",
      "        d__DIVORCED IN ROUND  d__MARRIED  d__MARRIED IN ROUND  \\\n",
      "147565                   0.0         0.0                  0.0   \n",
      "1231                     0.0         1.0                  0.0   \n",
      "152541                   0.0         1.0                  0.0   \n",
      "\n",
      "        d__NEVER MARRIED  d__SEPARATED  d__SEPARATED IN ROUND  d__WIDOWED  \\\n",
      "147565               0.0           1.0                    0.0         0.0   \n",
      "1231                 0.0           0.0                    0.0         0.0   \n",
      "152541               0.0           0.0                    0.0         0.0   \n",
      "\n",
      "        d__WIDOWED IN ROUND  \n",
      "147565                  0.0  \n",
      "1231                    0.0  \n",
      "152541                  0.0  \n",
      "\n",
      " Normalized dataset --> \n",
      "\n",
      "             id  age  rxStartYear  rxQuantity isFemale  \\\n",
      "66379  45239101   62       2009.0        30.0     True   \n",
      "73405  46556102   45       2008.0        60.0     True   \n",
      "29974  16762101   47       2008.0        30.0     True   \n",
      "\n",
      "       d__Amer Indian/Alaska Native  d__Asian  d__Black  d__Multiple  \\\n",
      "66379                           0.0       0.0       1.0          0.0   \n",
      "73405                           0.0       0.0       0.0          0.0   \n",
      "29974                           0.0       0.0       1.0          0.0   \n",
      "\n",
      "       d__Native Hawaiian/Pacific Islander  d__White  d__DIVORCED  \\\n",
      "66379                                  0.0       0.0          1.0   \n",
      "73405                                  0.0       1.0          0.0   \n",
      "29974                                  0.0       0.0          0.0   \n",
      "\n",
      "       d__DIVORCED IN ROUND  d__MARRIED  d__MARRIED IN ROUND  \\\n",
      "66379                   0.0         0.0                  0.0   \n",
      "73405                   0.0         1.0                  0.0   \n",
      "29974                   0.0         0.0                  0.0   \n",
      "\n",
      "       d__NEVER MARRIED  d__SEPARATED  d__SEPARATED IN ROUND  d__WIDOWED  \\\n",
      "66379               0.0           0.0                    0.0         0.0   \n",
      "73405               0.0           0.0                    0.0         0.0   \n",
      "29974               0.0           1.0                    0.0         0.0   \n",
      "\n",
      "       d__WIDOWED IN ROUND  \n",
      "66379                  0.0  \n",
      "73405                  0.0  \n",
      "29974                  0.0  \n",
      "\n",
      " Normalized dataset --> \n",
      "\n",
      "147565    False\n",
      "1231       True\n",
      "152541    False\n",
      "Name: diabetesDiagnosed, dtype: bool\n",
      "\n",
      " Normalized dataset --> \n",
      "\n",
      "66379    True\n",
      "73405    True\n",
      "29974    True\n",
      "Name: diabetesDiagnosed, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# TODO: change model to sklearn.linear_model.LogisticRegressionCV\n",
    "# We'll use SciKit-Learn's LogisticRegressionCV model. \n",
    "# It uses CV to set its regularization strengths.\n",
    "\n",
    "# logistic = LogisticRegression(random_state=2001, solver='liblinear', class_weight='balanced', n_jobs=-1)\n",
    "logistic = LogisticRegressionCV(random_state=2001, solver='liblinear', class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "\n",
    "# Need to normalize for a logistic regression's coefficients to be meaningful.\n",
    "diabetes_norm = dropped_qual.copy()\n",
    "diabetes_norm = (diabetes_norm - diabetes_norm.mean()) / diabetes_norm.std(ddof=0)\n",
    "# Don't want to normalize our outcome, so set it to the non-normalized version :)\n",
    "diabetes_norm['diabetesDiagnosed'] = dropped_qual['diabetesDiagnosed']\n",
    "\n",
    "# Split train and test again.\n",
    "all_x_norm = diabetes_norm.drop(['diabetesDiagnosed'], axis=1)\n",
    "all_y_norm = diabetes_norm['diabetesDiagnosed']\n",
    "X_train_norm, X_test_norm, Y_train_norm, Y_test_norm = train_test_split(\n",
    "    all_x, all_y, test_size=.2, random_state=2001)\n",
    "\n",
    "for dataset in [X_train, X_test, Y_train, Y_test]:\n",
    "    print(\"\\n Normalized dataset --> \\n\")\n",
    "    print(dataset.head(3))\n",
    "\n",
    "logistic.fit(X_train_norm, Y_train_norm)\n",
    "Y_pred = logistic.predict(X_test_norm)\n",
    "score_classif_on_test(random_forest, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.593306646259\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.766     0.644     0.700     25951\n",
      "       True      0.240     0.364     0.289      8023\n",
      "\n",
      "avg / total      0.642     0.578     0.603     33974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, with more interpretable metrics based on classification accuracy.\n",
    "\n",
    "print(logistic.score(X_test_norm, Y_test_norm))\n",
    "print(classification_report(Y_test, Y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score:\n",
      "(33974,)\n",
      "0.528046267198\n",
      "LogLoss:\n",
      "0.691388348598\n"
     ]
    }
   ],
   "source": [
    "# Then, with more useful metrics based on predicted probabilities. \n",
    "probs = logistic.predict_proba(X_test_norm)\n",
    "\n",
    "print('AUC Score:')\n",
    "print(roc_auc_score(Y_test_norm, probs[:, 1:]))\n",
    "print('')\n",
    "\n",
    "print('LogLoss:')\n",
    "print(log_loss(Y_test_norm, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC is barely over .50, indicating the logistic regression classifier isn't much better than chance. Not shocking - we haven't given it drug info, just demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('age', 6.2033913779375457e-05), ('rxQuantity', 2.0160215799022857e-05), ('rxStartYear', 1.080605020961186e-05), ('d__NEVER MARRIED', -6.7139635208516233e-07), ('d__White', -5.0018513020634522e-07), ('d__Black', 4.5915361365064964e-07), ('d__WIDOWED', 3.4427461081279073e-07), ('isFemale', -3.1160106014185239e-07), ('d__DIVORCED', 1.537909523861212e-07), ('d__WIDOWED IN ROUND', 7.7935604065506666e-08), ('d__SEPARATED', 7.2772061093636999e-08), ('d__MARRIED', 5.5146081097351865e-08), ('d__Amer Indian/Alaska Native', 4.1613911307577813e-08), ('d__MARRIED IN ROUND', -3.095100368847204e-08), ('d__Asian', -1.845101762170867e-08), ('d__SEPARATED IN ROUND', 1.8231307298067554e-08), ('d__Native Hawaiian/Pacific Islander', 1.6660190432015383e-08), ('d__Multiple', 1.1147354693465299e-08), ('d__DIVORCED IN ROUND', -9.8643387242391467e-09), ('id', -5.900488616708205e-10)]\n"
     ]
    }
   ],
   "source": [
    "# Now look at importances.\n",
    "\n",
    "# Flatten logistic regression coefficients to a list and show corresponding quantities.\n",
    "# I'm loosely calling them 'importances.'\n",
    "def print_coef_importances(importances, columns):\n",
    "    coefficients = zip(columns, importances.flatten())\n",
    "    sorted_coef = sorted(coefficients, key=(lambda tup: abs(tup[1])), reverse=True)\n",
    "    print(sorted_coef)\n",
    "\n",
    "print_coef_importances(logistic.coef_, X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to \n",
    "\n",
    "## Tree-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=10,\n",
       "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
       "            random_state=2001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest is a great black box classifier \n",
    "#   (and in a slightly altered form, regressor). \n",
    "# We should be able to get good results with little tuning.\n",
    "# Note: we should be using balanced class weights here to compensate for class imbalance,\n",
    "#   but I'm intentionally omitting that to show how it skews results.\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    min_samples_split=20, \n",
    "    min_samples_leaf=10, \n",
    "    max_features='auto', \n",
    "    random_state=2001, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "Y_pred = None  # Make sure we aren't using our old logistic y-hats.\n",
    "# No need to normalize. \n",
    "random_forest.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816094660623\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.812     0.987     0.891     25951\n",
      "       True      0.864     0.262     0.403      8023\n",
      "\n",
      "avg / total      0.825     0.816     0.776     33974\n",
      "\n",
      "AUC Score:\n",
      "0.863780424102\n",
      "\n",
      "LogLoss:\n",
      "0.401109195697\n"
     ]
    }
   ],
   "source": [
    "# Score it.\n",
    "\n",
    "score_classif_on_test(random_forest, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity's sake, we'll look at F1-score, since it captures both precision and recall pretty well. We'll also pay attention to accuracy and recall since they can be easily explained to non-data scientists.\n",
    "\n",
    "We'll also look at log-loss and AUC, which describe model quality rather than the performance of a particular cutoff for predicting classes. (In other words, they describe the accuracy of our predicted probabilities of class membership rather than our ultimate class labels.) \n",
    "\n",
    "With our first naive model, we're already getting .86 AUC, which is very respectable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('age', 0.3577093357527954), ('id', 0.29247474708816801), ('rxStartYear', 0.10669557814615584), ('rxQuantity', 0.1065093341713839), ('d__NEVER MARRIED', 0.021151961353537304), ('isFemale', 0.020272319287124554), ('d__Black', 0.016388611164839129), ('d__White', 0.015970265269041023), ('d__WIDOWED', 0.012336259083978635), ('d__MARRIED', 0.0097865329548047619), ('d__DIVORCED', 0.0076916834185343274), ('d__SEPARATED', 0.0052070261919044359), ('d__Asian', 0.0046158120198111705), ('d__Amer Indian/Alaska Native', 0.0044406406277527448), ('d__DIVORCED IN ROUND', 0.0041289514773978561), ('d__WIDOWED IN ROUND', 0.0038531097734076591), ('d__SEPARATED IN ROUND', 0.0032982258117143187), ('d__Multiple', 0.0030978237470242025), ('d__MARRIED IN ROUND', 0.0026873381131272394), ('d__Native Hawaiian/Pacific Islander', 0.0016844445474972995)]\n"
     ]
    }
   ],
   "source": [
    "print_coef_importances(random_forest.feature_importances_, X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I included a logistic regression here so we can inspect it for information. For example, it shows that ethnicity has a pretty significant impact on the probability of having diabetes. However, as a predictive model, it's not terribly impressive. In particular, its recall for the positive class is awful.\n",
    "\n",
    "Our RF accuracy is around what we'd expect, given how little information it has. And it's using age correctly while also giving some weight to rxStartYear and rxQuantitiy. (I suspect some information about type of drug / administration method may be contained in the quantity.) \n",
    "\n",
    "It's not shocking that our recall is much better for False than True. That's just saying it's way easier to predict that someone doesn't have diabetes than that she does. Also, since False is around 3/4 of the dataset, our model's going to benefit more from assuming results will be False. \n",
    "\n",
    "We can tweak our decision threshold using predict_proba in the RF model, but let's hold off for now. First, we'll engineer our variables a bit more, and then we can think about setting thresholds.\n",
    "\n",
    "Also one major concern - one of our most important features is 'id'. That implies leakage some sort of data leakage from the ID, as it should be random. In other words, smaller ids, presumably older ones, may be less likely to have diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792664979102\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.796     0.980     0.878     25951\n",
      "       True      0.744     0.186     0.298      8023\n",
      "\n",
      "avg / total      0.784     0.793     0.741     33974\n",
      "\n",
      "AUC Score:\n",
      "0.785238158187\n",
      "\n",
      "LogLoss:\n",
      "0.447398101952\n"
     ]
    }
   ],
   "source": [
    "Y_pred = None  # Make sure we aren't using our old logistic y-hats.\n",
    "# No need to normalize. \n",
    "X_train_clean, X_test_clean = X_train.copy().drop('id', axis=1), X_test.copy().drop('id', axis=1)\n",
    "random_forest.fit(X_train_clean, Y_train)\n",
    "\n",
    "# Score it.\n",
    "score_classif_on_test(random_forest, X_test_clean, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our AUC has dropped, it wouldn't be proper to let data leak in from our ID column. This is meant to be a useful model, not an attempt to game a Kaggle competition :)\n",
    "\n",
    "The next order of business is to deal with the rxName column.\n",
    "\n",
    "### Incorporating prescription names into our model\n",
    "\n",
    "First, let's just try label encoding our prescriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dataset --> \n",
      "\n",
      "\n",
      "        age  rxStartYear  rxQuantity  isFemale  d__Amer Indian/Alaska Native  \\\n",
      "18901    51       2011.0        31.0      True                             0   \n",
      "161166   69       2008.0         9.0     False                             0   \n",
      "136061   24       2009.0       120.0      True                             0   \n",
      "36871    66       2011.0        90.0     False                             0   \n",
      "\n",
      "        d__Asian  d__Black  d__Multiple  d__Native Hawaiian/Pacific Islander  \\\n",
      "18901          0         0            0                                    0   \n",
      "161166         0         0            0                                    0   \n",
      "136061         0         0            0                                    0   \n",
      "36871          0         1            0                                    0   \n",
      "\n",
      "        d__White  d__DIVORCED  d__DIVORCED IN ROUND  d__MARRIED  \\\n",
      "18901          1            0                     0           0   \n",
      "161166         1            0                     0           1   \n",
      "136061         1            0                     0           0   \n",
      "36871          0            0                     0           0   \n",
      "\n",
      "        d__MARRIED IN ROUND  d__NEVER MARRIED  d__SEPARATED  \\\n",
      "18901                     0                 1             0   \n",
      "161166                    0                 0             0   \n",
      "136061                    1                 0             0   \n",
      "36871                     0                 0             1   \n",
      "\n",
      "        d__SEPARATED IN ROUND  d__WIDOWED  d__WIDOWED IN ROUND  rxEnc  \n",
      "18901                       0           0                    0   5331  \n",
      "161166                      0           0                    0   5748  \n",
      "136061                      0           0                    0   3682  \n",
      "36871                       0           0                    0   3923  \n",
      "\n",
      "\n",
      "Dataset --> \n",
      "\n",
      "\n",
      "        age  rxStartYear  rxQuantity  isFemale  d__Amer Indian/Alaska Native  \\\n",
      "149602   56       2008.0        60.0      True                             0   \n",
      "57205    61       2009.0        30.0     False                             0   \n",
      "125708   21       2009.0        15.0     False                             0   \n",
      "87514    34       2010.0        10.0     False                             0   \n",
      "\n",
      "        d__Asian  d__Black  d__Multiple  d__Native Hawaiian/Pacific Islander  \\\n",
      "149602         0         0            0                                    0   \n",
      "57205          0         0            0                                    0   \n",
      "125708         0         0            0                                    0   \n",
      "87514          0         0            0                                    0   \n",
      "\n",
      "        d__White  d__DIVORCED  d__DIVORCED IN ROUND  d__MARRIED  \\\n",
      "149602         1            0                     0           1   \n",
      "57205          1            0                     0           0   \n",
      "125708         1            0                     0           0   \n",
      "87514          1            0                     0           0   \n",
      "\n",
      "        d__MARRIED IN ROUND  d__NEVER MARRIED  d__SEPARATED  \\\n",
      "149602                    0                 0             0   \n",
      "57205                     1                 0             0   \n",
      "125708                    0                 1             0   \n",
      "87514                     0                 1             0   \n",
      "\n",
      "        d__SEPARATED IN ROUND  d__WIDOWED  d__WIDOWED IN ROUND  rxEnc  \n",
      "149602                      0           0                    0   5567  \n",
      "57205                       0           0                    0   7323  \n",
      "125708                      0           0                    0   1649  \n",
      "87514                       0           0                    0   6715  \n",
      "\n",
      "\n",
      "Dataset --> \n",
      "\n",
      "\n",
      "18901     False\n",
      "161166    False\n",
      "136061    False\n",
      "36871     False\n",
      "Name: diabetesDiagnosed, dtype: bool\n",
      "\n",
      "\n",
      "Dataset --> \n",
      "\n",
      "\n",
      "149602    False\n",
      "57205      True\n",
      "125708    False\n",
      "87514     False\n",
      "Name: diabetesDiagnosed, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "with_rx = diabetes_df.drop(['rxForm'], axis=1)\n",
    "\n",
    "# I'm just using SKLearn.LabelEncoder because I originally wrote this notebook with it,\n",
    "#   but at present, I prefer to convert to pd.categorical dtype and use df.col.cat.codes to train models\n",
    "#   as it makes converting between labels and codes very simple.\n",
    "label_encoder = LabelEncoder()\n",
    "with_rx['rxEnc'] = label_encoder.fit_transform(with_rx['rxName'])\n",
    "\n",
    "all_x = with_rx.drop(['diabetesDiagnosed', 'id', 'rxName'],axis=1)\n",
    "all_y = with_rx['diabetesDiagnosed']\n",
    "\n",
    "X_train_enc, X_test_enc, Y_train_enc, Y_test_enc = train_test_split(all_x, all_y, test_size=.2, random_state=2001)\n",
    "\n",
    "for dataset in [X_train_enc, X_test_enc, Y_train_enc, Y_test_enc]:\n",
    "    print(\"\\n\\nDataset --> \\n\\n\")\n",
    "    print(dataset.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's also pickle our dataset for easy replication and so we don't have to re-run the above script every time.\n",
    "\n",
    "X_train_enc.to_pickle('./input/mep_x_train_2017_06_13.pkl.xz', compression='xz')\n",
    "Y_train_enc.to_pickle('./input/mep_y_train_2017_06_13.pkl.xz', compression='xz')\n",
    "X_test_enc.to_pickle('./input/mep_x_test_2017_06_13.pkl.xz', compression='xz')\n",
    "Y_test_enc.to_pickle('./input/mep_y_test_2017_06_13.pkl.xz', compression='xz')\n",
    "\n",
    "# If using pd < 0.20, omit compression.\n",
    "# Also, you'll need backports.lzma to run above. Otherwise, below code will work.\n",
    "# X_train_enc.to_pickle('./input/mep_x_train_2017_06_13.pkl')\n",
    "# Y_train_enc.to_pickle('./input/mep_y_train_2017_06_13.pkl')\n",
    "# X_test_enc.to_pickle('./input/mep_x_test_2017_06_13.pkl')\n",
    "# Y_test_enc.to_pickle('./input/mep_y_test_2017_06_13.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_enc = pd.read_pickle('./input/mep_x_train_2017_06_13.pkl.xz')\n",
    "Y_train_enc = pd.read_pickle('./input/mep_y_train_2017_06_13.pkl.xz')\n",
    "X_test_enc = pd.read_pickle('./input/mep_x_test_2017_06_13.pkl.xz')\n",
    "Y_test_enc = pd.read_pickle('./input/mep_y_test_2017_06_13.pkl.xz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest benefits significantly from inclusion of drug labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800582798611\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.802     0.981     0.883     25951\n",
      "       True      0.781     0.216     0.339      8023\n",
      "\n",
      "avg / total      0.797     0.801     0.754     33974\n",
      "\n",
      "\n",
      "[('age', 0.36056278570399025), ('rxEnc', 0.262825718365092), ('rxQuantity', 0.12031847313529757), ('rxStartYear', 0.11745041065500818), ('d__NEVER MARRIED', 0.024852817359040564), ('isFemale', 0.020551927606031355), ('d__Black', 0.018332382124160117), ('d__White', 0.017191011270011249), ('d__WIDOWED', 0.011467710559893939), ('d__MARRIED', 0.0092743445731561618), ('d__DIVORCED', 0.0065972091731750383), ('d__SEPARATED', 0.0050327254063759694), ('d__Amer Indian/Alaska Native', 0.0044432517984435201), ('d__Asian', 0.004301073009678559), ('d__DIVORCED IN ROUND', 0.0035001306260339132), ('d__WIDOWED IN ROUND', 0.003468788582554466), ('d__SEPARATED IN ROUND', 0.0029908531775029857), ('d__MARRIED IN ROUND', 0.0026996744238864594), ('d__Multiple', 0.0025743736179944599), ('d__Native Hawaiian/Pacific Islander', 0.0015643388326734323)]\n",
      "\n",
      "AUC Score:\n",
      "0.794428783614\n",
      "\n",
      "LogLoss:\n",
      "0.440975034957\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    min_samples_split=4, \n",
    "    min_samples_leaf=2, \n",
    "    max_features=.3, \n",
    "    random_state=2001, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_forest.fit(X_train_enc, Y_train_enc)\n",
    "score_classif_on_test(random_forest, X_test_enc, Y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.725643138871\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.883     0.739     0.804     25951\n",
      "       True      0.447     0.683     0.540      8023\n",
      "\n",
      "avg / total      0.780     0.726     0.742     33974\n",
      "\n",
      "\n",
      "[('age', 0.42200980348967621), ('rxEnc', 0.22196416618372841), ('rxQuantity', 0.11296335231539321), ('rxStartYear', 0.10945869683185157), ('d__NEVER MARRIED', 0.031287475585166299), ('isFemale', 0.019323284523234806), ('d__White', 0.016180200186202105), ('d__Black', 0.014517297219627578), ('d__WIDOWED', 0.0097029079282310252), ('d__MARRIED', 0.0092063148798403067), ('d__DIVORCED', 0.0068408432582869683), ('d__SEPARATED', 0.0043443012651767721), ('d__Asian', 0.0040051568069508674), ('d__Amer Indian/Alaska Native', 0.0036746526986122696), ('d__DIVORCED IN ROUND', 0.0036323460229164329), ('d__MARRIED IN ROUND', 0.0028012895201026196), ('d__WIDOWED IN ROUND', 0.0025395978832523703), ('d__SEPARATED IN ROUND', 0.0023741794270886797), ('d__Multiple', 0.0021776235601624902), ('d__Native Hawaiian/Pacific Islander', 0.0009965104144989106)]\n",
      "\n",
      "AUC Score:\n",
      "0.793821847772\n",
      "\n",
      "LogLoss:\n",
      "0.523457754493\n"
     ]
    }
   ],
   "source": [
    "# Let's go ahead and try balanced class weights.\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    min_samples_split=4, \n",
    "    min_samples_leaf=2, \n",
    "    max_features=.3, \n",
    "    random_state=2001, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_forest.fit(X_train_enc, Y_train_enc)\n",
    "score_classif_on_test(random_forest, X_test_enc, Y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2.000000\n",
      "mean     0.705857\n",
      "std      0.000883\n",
      "min      0.705232\n",
      "25%      0.705544\n",
      "50%      0.705857\n",
      "75%      0.706169\n",
      "max      0.706481\n",
      "dtype: float64\n",
      "Performance of model vs. dummy: \n",
      "Error of 0.706, 0.500 for model, dummy; -41.171 percent less error.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7058565107121059"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And, finally, just add some more trees.\n",
    "# Since we're now tuning parameters, we should really be using CV.\n",
    "# Repeated k-fold is the gold standard, so we'll use it.\n",
    "# SKL 20.x implements it but wasn't available at the time of writing (2017/06),\n",
    "#   so use a homebrewed version:\n",
    "\n",
    "\n",
    "def get_repeated_k_fold_score(model, xs, ys, folds=None, iters=None,\n",
    "                              agg_inner=None, agg_outer=None, seed=None,\n",
    "                              score_func=None, fit_kwargs=None,\n",
    "                              print_description=None):\n",
    "    \"\"\"\n",
    "    Do k-fold cv a number of times and aggregate results.\n",
    "\n",
    "    :param model: SKLearn model\n",
    "    :params xs: pd.DataFrame with cols for x's, rows for examples\n",
    "    :param yx: pd.Series with col of y's, rows for examples\n",
    "    :param folds: int, number of folds in inner k-fold cv\n",
    "    :param iters: int, number of times to repeat k-fold cv process\n",
    "    :param agg_inner: agg function for the inner loop - each \n",
    "      k-fold cross-validation\n",
    "    :param agg_outer: agg function for the outer loop - combining\n",
    "      the various k-fold cross-validations\n",
    "    :param score_func: SKLearn-style scoring metric \n",
    "      (e.g. sklearn.metrics.mean_squared_error) \n",
    "      that accepts y_true, y_pred as params\n",
    "    :param fit_kwargs: kwargs to add to .fit() method of model when called\n",
    "    :param print_description: Bool, whether or not to print the cv result's \n",
    "      .describe() method's result - e.g. count, mean, std, quantiles, min/max\n",
    "    :return: aggregated final score of the inner and outer loops\n",
    "    \"\"\"\n",
    "    # Set defaults\n",
    "    folds = folds or 10\n",
    "    iters = iters or 10\n",
    "    seed = seed or 1\n",
    "    agg_inner = agg_inner or np.mean\n",
    "    agg_outer = agg_outer or np.mean\n",
    "    fit_kwargs = fit_kwargs or dict()\n",
    "    if print_description is None:\n",
    "        print_description = True\n",
    "    # If supplied a metric, convert to SKLearn scorer object.\n",
    "    # Will use greater_is_better=True so no sign flip happens.\n",
    "    if score_func is not None:\n",
    "        scorer = make_scorer(score_func, greater_is_better=True)\n",
    "    else:\n",
    "        scorer = None\n",
    "    dummy = DummyRegressor(strategy='mean')\n",
    "    all_scores = []\n",
    "    all_dummy_scores = []\n",
    "    # model.fit(xs, ys, **fit_kwargs)\n",
    "    for i in range(seed, seed + iters):\n",
    "        # Set up CV.\n",
    "        kfold = KFold(n_splits=folds, shuffle=True, random_state=i)\n",
    "        # Get CV scores and aggregate over all folds.\n",
    "        scores = cross_val_score(model, xs, ys, scoring=scorer, cv=kfold)\n",
    "        score = agg_inner(scores)\n",
    "        all_scores.append(score)\n",
    "        # Repeat for dummy model.\n",
    "        dummy_scores = cross_val_score(dummy, xs, ys, scoring=scorer, cv=kfold)\n",
    "        dummy_score = agg_inner(dummy_scores)\n",
    "        all_dummy_scores.append(dummy_score)\n",
    "    all_scores = pd.Series(all_scores)\n",
    "    if print_description:\n",
    "        print(all_scores.describe())\n",
    "    score = agg_outer(all_scores)\n",
    "    dummy_score = agg_outer(all_dummy_scores)\n",
    "    print('Performance of model vs. dummy: ')\n",
    "    percent_error = 100 * (dummy_score - score) / float(dummy_score)\n",
    "    to_interpolate = (score, dummy_score, percent_error)\n",
    "    print('Error of %.3f, %.3f for model, dummy; %.3f percent less error.' % to_interpolate)\n",
    "    return score\n",
    "\n",
    "get_repeated_k_fold_score(random_forest, X_train_enc, Y_train_enc, folds=10, iters=2,\n",
    "                              agg_inner=None, agg_outer=None, seed=2001,\n",
    "                              score_func=roc_auc_score, fit_kwargs=None,\n",
    "                              print_description=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score:\n",
      "0.815120011624\n",
      "\n",
      "LogLoss:\n",
      "0.418319933018\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.832     0.959     0.891     25951\n",
      "       True      0.736     0.373     0.495      8023\n",
      "\n",
      "avg / total      0.809     0.820     0.797     33974\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.25555714,  0.38064172,  0.00166667, ...,  0.00896465,\n",
       "        0.16441307,  0.26412454])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From here, we could tune our hyperparameters a bit, \n",
    "#   but the advantage of random forests is that such a step is largely unnecessary.\n",
    "# Test our final RF\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=300, \n",
    "    min_samples_split=4, \n",
    "    min_samples_leaf=2, \n",
    "    max_features=.3, \n",
    "    random_state=2001, \n",
    "    n_jobs=-1\n",
    ")\n",
    "random_forest.fit(X_train_enc, Y_train_enc)\n",
    "score_classif_on_test(random_forest, X_test_enc, Y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have CV set up, we could tune our model a bit. But that's not the point of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back to our new encoded features, note that logistic regression doesn't benefit too much from encoded drug names without further processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614214019751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.883     0.739     0.804     25951\n",
      "       True      0.447     0.683     0.540      8023\n",
      "\n",
      "avg / total      0.780     0.726     0.742     33974\n",
      "\n",
      "[('d__Amer Indian/Alaska Native', 4.6100036502151118), ('d__Native Hawaiian/Pacific Islander', 4.3882438096225886), ('d__Black', 4.1946082918323464), ('d__Multiple', 4.00312068983503), ('d__Asian', 3.6767788000322703), ('d__White', 3.6540942163760453), ('d__SEPARATED', 3.0683279944537447), ('d__SEPARATED IN ROUND', 2.9380129072322618), ('d__WIDOWED IN ROUND', 2.8784851462807701), ('d__DIVORCED', 2.7712634888388572), ('d__MARRIED IN ROUND', 2.6874017503882723), ('d__MARRIED', 2.6280901673176009), ('d__DIVORCED IN ROUND', 2.6196596715053411), ('d__WIDOWED', 2.5243085519937476), ('d__NEVER MARRIED', 2.4112997799638709), ('isFemale', -0.180838939934476), ('age', 0.036794668758996611), ('rxStartYear', -0.01639758467133311), ('rxQuantity', 1.1047039198874027e-05), ('rxEnc', 6.6698788536737992e-06)]\n",
      "\n",
      "AUC Score:\n",
      "0.683131604706\n",
      "\n",
      "LogLoss:\n",
      "0.640942677814\n"
     ]
    }
   ],
   "source": [
    "logistic.fit(X_train_enc, Y_train_enc)\n",
    "\n",
    "score_classif_on_test(logistic, X_test_enc, Y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to one-hot encode. But here it gets a little messy and subjective. There are at least a few way to do this:  \n",
    "\n",
    "1) pick the n most common drugs to use  \n",
    "2) pick the n most common drugs in diabetes patients to use  \n",
    "3) pick the n drugs that have the highest precision or recall when thought of as a \"test\" for diabetes  \n",
    "\n",
    "I don't really think it's an option to encode every drug - we'd have literally thousands of columns.  \n",
    "I'll start with (1) as the simplest solution, but there's certainly value to the others, and, ideally, we'd try them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AZITHROMYCIN                   3358\n",
       "LISINOPRIL                     2865\n",
       "AMOXICILLIN                    2687\n",
       "SIMVASTATIN                    2312\n",
       "IBUPROFEN                      2243\n",
       "PREDNISONE                     1712\n",
       "HYDROCO/APAP                   1703\n",
       "LIPITOR                        1630\n",
       "OMEPRAZOLE                     1496\n",
       "METFORMIN                      1422\n",
       "HYDROCHLOROTHIAZIDE            1281\n",
       "NAPROXEN                       1198\n",
       "ATENOLOL                       1102\n",
       "FUROSEMIDE                     1082\n",
       "LEVOTHYROXIN                    954\n",
       "APAP/HYDROCODONE BITARTRATE     951\n",
       "AMLODIPINE                      928\n",
       "HYDROCHLOROT                    852\n",
       "AMLODIPINE BESYLATE             832\n",
       "NEXIUM                          829\n",
       "Name: rxName, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common drugs overall:\n",
    "diabetes_df['rxName'].value_counts()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METFORMIN                  1158\n",
      "LISINOPRIL                  975\n",
      "SIMVASTATIN                 701\n",
      "LIPITOR                     518\n",
      "ACTOS                       449\n",
      "FUROSEMIDE                  447\n",
      "GLIPIZIDE                   431\n",
      "METFORMIN HCL               426\n",
      "ONETOUCH                    366\n",
      "LANTUS                      350\n",
      "GLYBURIDE                   342\n",
      "AZITHROMYCIN                328\n",
      "OMEPRAZOLE                  315\n",
      "HYDROCHLOROTHIAZIDE         300\n",
      "METFORMIN HYDROCHLORIDE     288\n",
      "GLIMEPIRIDE                 284\n",
      "INSULIN SYRG                268\n",
      "AMOXICILLIN                 250\n",
      "ASPIRIN                     247\n",
      "HYDROCO/APAP                245\n",
      "Name: rxName, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mote that most of these aren't all that specific for diabetes; \n",
    "#   people with type 2 diabetes are often overweight and have other health issues.\n",
    "\n",
    "have_diabetes = diabetes_df[diabetes_df['diabetesDiagnosed'] == True]\n",
    "print(have_diabetes['rxName'].value_counts()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZITHROMYCIN    3358\n",
      "LISINOPRIL      2865\n",
      "AMOXICILLIN     2687\n",
      "SIMVASTATIN     2312\n",
      "IBUPROFEN       2243\n",
      "PREDNISONE      1712\n",
      "HYDROCO/APAP    1703\n",
      "LIPITOR         1630\n",
      "OMEPRAZOLE      1496\n",
      "METFORMIN       1422\n",
      "Name: rxName, dtype: int64\n",
      "817     3358\n",
      "3923    2865\n",
      "486     2687\n",
      "6295    2312\n",
      "3443    2243\n",
      "5632    1712\n",
      "3295    1703\n",
      "3906    1630\n",
      "5028    1496\n",
      "4235    1422\n",
      "Name: rxEnc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# confirming that our label encoder worked correctly \n",
    "\n",
    "print(with_rx['rxName'].value_counts()[:10])\n",
    "print(with_rx['rxEnc'].value_counts()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering prescriptions by frequency (so we can feed them into the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         ATENOLOL\n",
      "1     AZITHROMYCIN\n",
      "2            OTHER\n",
      "3     HYDROCO/APAP\n",
      "4     CARISOPRODOL\n",
      "5            OTHER\n",
      "6            OTHER\n",
      "7            OTHER\n",
      "8          NORVASC\n",
      "9         SEROQUEL\n",
      "10       CLONIDINE\n",
      "11       COMBIVENT\n",
      "12         DIGOXIN\n",
      "13       LORAZEPAM\n",
      "14     SIMVASTATIN\n",
      "15    HYDROCHLOROT\n",
      "16      CARVEDILOL\n",
      "17         LIPITOR\n",
      "18           OTHER\n",
      "19           OTHER\n",
      "Name: rxName, dtype: object\n",
      "(169868, 22)\n",
      "(169868, 22)\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Re-initalize with_rx df so we can run code nonsequentially. \n",
    "with_rx = diabetes_df.drop(['rxForm'], axis=1)\n",
    "\n",
    "filtered_rx = with_rx\n",
    "\n",
    "filter_by_len = with_rx.groupby('rxName')['rxName'].filter(lambda x: len(x) >= 100)\n",
    "common_drugs = set(filter_by_len)\n",
    "filtered_rx['rxName'] = [rxName if rxName in common_drugs else 'OTHER' for rxName in filtered_rx['rxName']]\n",
    "\n",
    "print(filtered_rx.rxName.head(20))\n",
    "print(filtered_rx.shape)\n",
    "print(with_rx.shape)\n",
    "print('ATENOLOL' in common_drugs)\n",
    "print('this is not there' in common_drugs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we have all rx with frequency >=100. Now we have to re-encode to labels and then one-hot encode. If we didn't re-encode, our one-hot encoder would still be based on the entire list of rx's, and we'd have an inordinate number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cols found to drop.\n",
      "0        ATENOLOL\n",
      "1    AZITHROMYCIN\n",
      "2           OTHER\n",
      "3    HYDROCO/APAP\n",
      "4    CARISOPRODOL\n",
      "5           OTHER\n",
      "6           OTHER\n",
      "7           OTHER\n",
      "8         NORVASC\n",
      "9        SEROQUEL\n",
      "Name: rxName, dtype: object\n",
      "0     37\n",
      "1     43\n",
      "2    220\n",
      "3    138\n",
      "4     56\n",
      "5    220\n",
      "6    220\n",
      "7    220\n",
      "8    213\n",
      "9    263\n",
      "Name: rxEnc, dtype: int64\n",
      "\n",
      "\n",
      " We'll have to add columns = \n",
      "329\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "drop_cols(filtered_rx, ['rxEnc'])\n",
    "filtered_rx['rxEnc'] = label_encoder.fit_transform(filtered_rx['rxName'])\n",
    "\n",
    "print(filtered_rx['rxName'].head(10))\n",
    "print(filtered_rx['rxEnc'].head(10))\n",
    "\n",
    "print(\"\\n\\n We'll have to add columns = \")\n",
    "print(filtered_rx.rxEnc.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, our others are correctly encoded and our maxencoding seems reasonable.\n",
    "\n",
    "Now it's time for\n",
    "\n",
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    169868.000000\n",
      "mean          0.001189\n",
      "std           0.034464\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rx_ohe = filtered_rx\n",
    "\n",
    "# we'll just use the dummies feature in pd\n",
    "onehot_df = pd.get_dummies(filtered_rx.rxEnc)\n",
    "\n",
    "print(onehot_df[0].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above columns are about 99.9% sparse! We might want to encode them as a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                          u'id',                          u'age',\n",
       "                  u'diabetesDiagnosed',                  u'rxStartYear',\n",
       "                             u'rxName',                   u'rxQuantity',\n",
       "                           u'isFemale', u'd__Amer Indian/Alaska Native',\n",
       "                           u'd__Asian',                     u'd__Black',\n",
       "       ...\n",
       "                                   320,                             321,\n",
       "                                   322,                             323,\n",
       "                                   324,                             325,\n",
       "                                   326,                             327,\n",
       "                                   328,                             329],\n",
       "      dtype='object', length=353)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get cols back into the data frame\n",
    "rx_ohe = pd.concat([filtered_rx, onehot_df], axis=1)\n",
    "\n",
    "rx_ohe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training and test, again.\n",
    "X_train_ohe, X_test_ohe, Y_train_ohe, Y_test_ohe = train_test_split(rx_ohe.drop(\n",
    "        ['diabetesDiagnosed', 'id', 'rxName'],axis=1), rx_ohe['diabetesDiagnosed'], test_size=.2, random_state=2001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our models are affected by one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675899217048\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.861     0.687     0.764     25951\n",
      "       True      0.387     0.641     0.483      8023\n",
      "\n",
      "avg / total      0.749     0.676     0.698     33974\n",
      "\n",
      "\n",
      "AUC Score:\n",
      "0.742683123944\n",
      "\n",
      "LogLoss:\n",
      "0.582382484001\n"
     ]
    }
   ],
   "source": [
    "logistic.fit(X_train_ohe, Y_train_ohe)\n",
    "\n",
    "score_classif_on_test(logistic, X_test_ohe, Y_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic couldn't make sense of our encoded prescriptions, but it's gained lots of information from one-hot encoding. At this point, it comes very close to where RF was with a single label-encoded column.\n",
    "\n",
    "Now we can try RF again. It will be painfully slow with 300+ columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795108023783\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.791     0.994     0.881     25951\n",
      "       True      0.890     0.151     0.258      8023\n",
      "\n",
      "avg / total      0.814     0.795     0.734     33974\n",
      "\n",
      "\n",
      "AUC Score:\n",
      "0.77374064631\n",
      "\n",
      "LogLoss:\n",
      "0.454300897467\n"
     ]
    }
   ],
   "source": [
    "random_forest.fit(X_train_ohe, Y_train_ohe)\n",
    "score_classif_on_test(model, X_test_ohe, Y_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, RF was able to exploit the encodings effectively, so one-hot encoding was actually harmful (we lost the raw encodings, which actually dropped our AUC by around 2%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "While we aren't dealing with very many columns and probably don't have all that many features to extract, let's try making a few.\n",
    "\n",
    "For instance, we could combine 'd\\__SEPARATED' with 'd\\__SEPARATED IN ROUND' and 'd\\__WIDOWED', 'd\\__WIDOWED IN ROUND'.\n",
    "In these cases, the fact that a separation or widowing happened in a particular year probably isn't all that relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have RF running well, we can tune it using CV.\n",
    "\n",
    "Interactions between these parameters are pretty minimal, so no need to grid search. \n",
    "We could use random search, but I'll do something even simpler and search over each parameter separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try increasing trees\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    min_samples_split=13, \n",
    "    min_samples_leaf=3, \n",
    "    max_features=.2, \n",
    "    class_weight='balanced',\n",
    "    random_state=2001, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Optimize params using grid search.\n",
    "\n",
    "\n",
    "def test_params(model, param_value_dict, verbose=False):\n",
    "    \"\"\"\n",
    "    Run a grid search for a model using given param values and print results.\n",
    "    CV is stratified K-Fold in this instance.\n",
    "    (SKL is smart enough to know to prefer stratified CV in classification problems.)\n",
    "    \n",
    "    :param model: SciKit-Learn model, i.e. one that has \n",
    "      .fit() and .predict() methods\n",
    "    :param param_value_dict: dict, with keys of param names \n",
    "      and values of [values] to try for given param\n",
    "    :return: None, prints results to stdout\n",
    "    \"\"\"\n",
    "    # First, convert ROC_AUC from SKL metric to scorer API.\n",
    "    roc_scorer = make_scorer(roc_auc_score)\n",
    "    # Init GridSearchCV instance.\n",
    "    gsearch = GridSearchCV(\n",
    "        estimator = model, \n",
    "        param_grid = param_value_dict, \n",
    "        scoring=roc_scorer,\n",
    "        n_jobs=1,  # assume model itself is multithreaded - thus only 1 job\n",
    "        iid=False, \n",
    "        cv=10\n",
    "    )\n",
    "    gsearch.fit(X_train_enc, Y_train_enc)\n",
    "    if verbose:\n",
    "        print('Raw grid search scores:')\n",
    "        gsearch1.grid_scores_ \n",
    "    print('Best params for this search:')\n",
    "    print(gsearch.best_params_)\n",
    "    print('Best AUC for these params:')\n",
    "    print(gsearch.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup param dict.\n",
    "params = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700, 1000, 2000]\n",
    "}\n",
    "# Use simple 10-fold stratified CV.\n",
    "test_params(random_forest, params, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for this search:\n",
      "{'min_samples_split': 13}\n",
      "Best AUC for these params:\n",
      "0.71458857454\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'min_samples_split': [7, 10, 13, 16, 20]\n",
    "}\n",
    "test_params(random_forest, params, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for this search:\n",
      "{'min_samples_leaf': 5}\n",
      "Best AUC for these params:\n",
      "0.712809402922\n",
      "Best params for this search:\n",
      "{'max_features': 0.2}\n",
      "Best AUC for these params:\n",
      "0.710624613055\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 15, 20, 30, 50]\n",
    "}\n",
    "test_params(random_forest, params, verbose=False)\n",
    "\n",
    "params = {\n",
    "    'max_features': [.10, .20, .30, .40, .50, .60, .70, .80, .90, 'auto']  # auto means sqrt here\n",
    "}\n",
    "test_params(random_forest, params, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    min_samples_split=13, \n",
    "    min_samples_leaf=5, \n",
    "    max_features=.2, \n",
    "    class_weight='balanced',\n",
    "    random_state=2001, \n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of model vs. dummy: \n",
      "Error of 0.713, 0.500 for model, dummy; -42.537 percent less error.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7126871479083617"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get final CV score for RF model with different seed\n",
    "\n",
    "get_repeated_k_fold_score(random_forest, X_train_enc, Y_train_enc, folds=10, iters=3,\n",
    "                              agg_inner=None, agg_outer=None, seed=2002,\n",
    "                              score_func=roc_auc_score, fit_kwargs=None,\n",
    "                              print_description=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And test performance:\n",
    "# TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO CONCLUSION RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus \n",
    "\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load train and test from pkl.\n",
    "\n",
    "X_train_enc = pd.read_pickle('./input/mep_x_train_2017_06_13.pkl.xz')\n",
    "Y_train_enc = pd.read_pickle('./input/mep_y_train_2017_06_13.pkl.xz')\n",
    "X_test_enc = pd.read_pickle('./input/mep_x_test_2017_06_13.pkl.xz')\n",
    "Y_test_enc = pd.read_pickle('./input/mep_y_test_2017_06_13.pkl.xz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation\n",
    "\n",
    "1) We'll use cross-validation to establish num_rounds, \n",
    "2) then tune hyperparameters on that num_rounds, \n",
    "3) and finally fit our resulting model using test set validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build xgboost datamatrix object for further operations.\n",
    "dtrain = xgb.DMatrix(X_train_enc, Y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.736994+0.00730653\ttrain-logloss:0.657191+0.00104062\ttest-auc:0.725014+0.00916592\ttest-logloss:0.657793+0.00106194\n",
      "[20]\ttrain-auc:0.814671+0.00134697\ttrain-logloss:0.439679+0.00094271\ttest-auc:0.792183+0.00467564\ttest-logloss:0.449996+0.00181048\n",
      "[40]\ttrain-auc:0.835481+0.00118035\ttrain-logloss:0.405057+0.00121355\ttest-auc:0.806548+0.0043459\ttest-logloss:0.422157+0.0027617\n",
      "[60]\ttrain-auc:0.848699+0.00126471\ttrain-logloss:0.389739+0.00142101\ttest-auc:0.814377+0.00439097\ttest-logloss:0.411935+0.00314276\n",
      "[80]\ttrain-auc:0.858763+0.00122946\ttrain-logloss:0.379272+0.000966371\ttest-auc:0.818938+0.00364885\ttest-logloss:0.406244+0.00262638\n",
      "[100]\ttrain-auc:0.867795+0.00111532\ttrain-logloss:0.370414+0.000832694\ttest-auc:0.822548+0.00332232\ttest-logloss:0.402015+0.0025701\n",
      "[120]\ttrain-auc:0.875949+0.00105389\ttrain-logloss:0.362421+0.000790587\ttest-auc:0.825392+0.0027878\ttest-logloss:0.398723+0.0022755\n",
      "[140]\ttrain-auc:0.882934+0.000993645\ttrain-logloss:0.355472+0.000809815\ttest-auc:0.827775+0.00294327\ttest-logloss:0.396065+0.00236317\n",
      "[160]\ttrain-auc:0.889095+0.00105758\ttrain-logloss:0.349056+0.00109076\ttest-auc:0.829621+0.00263039\ttest-logloss:0.393816+0.00215368\n",
      "[180]\ttrain-auc:0.894988+0.00105705\ttrain-logloss:0.342835+0.00119382\ttest-auc:0.831563+0.00274183\ttest-logloss:0.391745+0.00237056\n",
      "[200]\ttrain-auc:0.899987+0.00119759\ttrain-logloss:0.337376+0.00128627\ttest-auc:0.832837+0.00273348\ttest-logloss:0.390252+0.00233655\n",
      "[220]\ttrain-auc:0.904653+0.00099134\ttrain-logloss:0.332038+0.00103194\ttest-auc:0.834171+0.00288882\ttest-logloss:0.388763+0.00251629\n",
      "[240]\ttrain-auc:0.908966+0.000713698\ttrain-logloss:0.327142+0.000825729\ttest-auc:0.834938+0.00328169\ttest-logloss:0.38776+0.00283505\n",
      "[260]\ttrain-auc:0.913047+0.000777197\ttrain-logloss:0.322338+0.000942822\ttest-auc:0.83581+0.0032644\ttest-logloss:0.386776+0.0028029\n",
      "[280]\ttrain-auc:0.916867+0.000836348\ttrain-logloss:0.317715+0.000957122\ttest-auc:0.836375+0.00343579\ttest-logloss:0.385985+0.00304294\n",
      "[300]\ttrain-auc:0.920103+0.000756778\ttrain-logloss:0.313565+0.000865947\ttest-auc:0.837073+0.00348187\ttest-logloss:0.385215+0.00310886\n",
      "[320]\ttrain-auc:0.923314+0.000864092\ttrain-logloss:0.309422+0.000978745\ttest-auc:0.837599+0.0034479\ttest-logloss:0.384565+0.00309501\n",
      "[340]\ttrain-auc:0.926441+0.000764257\ttrain-logloss:0.305276+0.000947671\ttest-auc:0.838243+0.00357501\ttest-logloss:0.383861+0.00326165\n",
      "[360]\ttrain-auc:0.9293+0.000639208\ttrain-logloss:0.301396+0.000774953\ttest-auc:0.838554+0.00358548\ttest-logloss:0.38337+0.0033492\n",
      "[380]\ttrain-auc:0.931946+0.000685413\ttrain-logloss:0.297697+0.000929908\ttest-auc:0.838731+0.00352917\ttest-logloss:0.383062+0.00328209\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-08f92544928a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'logloss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'auc'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# callbacks=[xgb.callback.print_evaluation(show_stdv=True)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n",
      "\u001b[0;32m/Users/jan/anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    405\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jan/anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jan/anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cross validate to get n_rounds for our simple starter gradient-boosted tree model.\n",
    "# We'll decrease the learning rate later to improve performance, but use this one\n",
    "#   to set parameters w/ CV.\n",
    "xgb_params = {\n",
    "    'max_depth':9, \n",
    "    'eta':.1, \n",
    "    'silent':0, \n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric':'auc', \n",
    "    'subsample':0.80, \n",
    "    'colsample_bytree':0.80\n",
    "}\n",
    "\n",
    "# It would be better to use repeated, stratified k-fold validation rather than \n",
    "#   simple stratified k-fold, \n",
    "#   but given our low number of columns and large number of training examples,\n",
    "#   I'm not very worried about overfitting and 1x10-fold CV should be adequate.\n",
    "xgb.cv(\n",
    "    xgb_params, \n",
    "    dtrain, \n",
    "    nfold=10, \n",
    "    stratified=True, \n",
    "    num_boost_round=1000, \n",
    "    early_stopping_rounds=20,\n",
    "    metrics={'logloss', 'auc'},\n",
    "    seed = 2001, \n",
    "    verbose_eval=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_for_gridsearch = xgb.XGBClassifier(\n",
    "    learning_rate =0.1, \n",
    "    n_estimators=339, \n",
    "    max_depth=9,\n",
    "    min_child_weight=1, \n",
    "    gamma=0, \n",
    "    subsample=0.9, \n",
    "    colsample_bytree=0.7,\n",
    "    objective= 'binary:logistic', \n",
    "    nthread=-1, \n",
    "    scale_pos_weight=1, \n",
    "    seed=2001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.79871, std: 0.00252, params: {'max_depth': 3, 'min_child_weight': 1},\n",
       "  mean: 0.79881, std: 0.00254, params: {'max_depth': 3, 'min_child_weight': 2},\n",
       "  mean: 0.81963, std: 0.00211, params: {'max_depth': 5, 'min_child_weight': 1},\n",
       "  mean: 0.81897, std: 0.00159, params: {'max_depth': 5, 'min_child_weight': 2},\n",
       "  mean: 0.83228, std: 0.00208, params: {'max_depth': 7, 'min_child_weight': 1},\n",
       "  mean: 0.83066, std: 0.00199, params: {'max_depth': 7, 'min_child_weight': 2},\n",
       "  mean: 0.83755, std: 0.00192, params: {'max_depth': 9, 'min_child_weight': 1},\n",
       "  mean: 0.83562, std: 0.00221, params: {'max_depth': 9, 'min_child_weight': 2},\n",
       "  mean: 0.83746, std: 0.00229, params: {'max_depth': 11, 'min_child_weight': 1},\n",
       "  mean: 0.83510, std: 0.00283, params: {'max_depth': 11, 'min_child_weight': 2},\n",
       "  mean: 0.83429, std: 0.00258, params: {'max_depth': 13, 'min_child_weight': 1},\n",
       "  mean: 0.83287, std: 0.00275, params: {'max_depth': 13, 'min_child_weight': 2}],\n",
       " {'max_depth': 9, 'min_child_weight': 1},\n",
       " 0.8375502124420201)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize with grid search.\n",
    "\n",
    "params1 = {\n",
    " 'max_depth':range(3, 14, 2),\n",
    " 'min_child_weight':range(1, 3, 1)\n",
    "}\n",
    "\n",
    "test_params(classifier_for_gridsearch, params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params2 = {\n",
    " 'max_depth': range(8,11,1),\n",
    " 'gamma': [i/10.0 for i in range(0,2)]\n",
    "}\n",
    "\n",
    "test_params(classifier_for_gridsearch, params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([mean: 0.83789, std: 0.00229, params: {'subsample': 0.7, 'colsample_bytree': 0.7}, mean: 0.83984, std: 0.00215, params: {'subsample': 0.8, 'colsample_bytree': 0.7}, mean: 0.84272, std: 0.00246, params: {'subsample': 0.9, 'colsample_bytree': 0.7}, mean: 0.84001, std: 0.00202, params: {'subsample': 1.0, 'colsample_bytree': 0.7}, mean: 0.83446, std: 0.00193, params: {'subsample': 0.7, 'colsample_bytree': 0.8}, mean: 0.83755, std: 0.00192, params: {'subsample': 0.8, 'colsample_bytree': 0.8}, mean: 0.84020, std: 0.00204, params: {'subsample': 0.9, 'colsample_bytree': 0.8}, mean: 0.83908, std: 0.00218, params: {'subsample': 1.0, 'colsample_bytree': 0.8}, mean: 0.83090, std: 0.00167, params: {'subsample': 0.7, 'colsample_bytree': 0.9}, mean: 0.83495, std: 0.00179, params: {'subsample': 0.8, 'colsample_bytree': 0.9}, mean: 0.83721, std: 0.00253, params: {'subsample': 0.9, 'colsample_bytree': 0.9}, mean: 0.83773, std: 0.00220, params: {'subsample': 1.0, 'colsample_bytree': 0.9}, mean: 0.82819, std: 0.00174, params: {'subsample': 0.7, 'colsample_bytree': 1.0}, mean: 0.83353, std: 0.00225, params: {'subsample': 0.8, 'colsample_bytree': 1.0}, mean: 0.83679, std: 0.00197, params: {'subsample': 0.9, 'colsample_bytree': 1.0}, mean: 0.83684, std: 0.00271, params: {'subsample': 1.0, 'colsample_bytree': 1.0}], [mean: 0.83789, std: 0.00229, params: {'subsample': 0.7, 'colsample_bytree': 0.7}, mean: 0.83984, std: 0.00215, params: {'subsample': 0.8, 'colsample_bytree': 0.7}, mean: 0.84272, std: 0.00246, params: {'subsample': 0.9, 'colsample_bytree': 0.7}, mean: 0.84001, std: 0.00202, params: {'subsample': 1.0, 'colsample_bytree': 0.7}, mean: 0.83446, std: 0.00193, params: {'subsample': 0.7, 'colsample_bytree': 0.8}, mean: 0.83755, std: 0.00192, params: {'subsample': 0.8, 'colsample_bytree': 0.8}, mean: 0.84020, std: 0.00204, params: {'subsample': 0.9, 'colsample_bytree': 0.8}, mean: 0.83908, std: 0.00218, params: {'subsample': 1.0, 'colsample_bytree': 0.8}, mean: 0.83090, std: 0.00167, params: {'subsample': 0.7, 'colsample_bytree': 0.9}, mean: 0.83495, std: 0.00179, params: {'subsample': 0.8, 'colsample_bytree': 0.9}, mean: 0.83721, std: 0.00253, params: {'subsample': 0.9, 'colsample_bytree': 0.9}, mean: 0.83773, std: 0.00220, params: {'subsample': 1.0, 'colsample_bytree': 0.9}, mean: 0.82819, std: 0.00174, params: {'subsample': 0.7, 'colsample_bytree': 1.0}, mean: 0.83353, std: 0.00225, params: {'subsample': 0.8, 'colsample_bytree': 1.0}, mean: 0.83679, std: 0.00197, params: {'subsample': 0.9, 'colsample_bytree': 1.0}, mean: 0.83684, std: 0.00271, params: {'subsample': 1.0, 'colsample_bytree': 1.0}], [mean: 0.83789, std: 0.00229, params: {'subsample': 0.7, 'colsample_bytree': 0.7}, mean: 0.83984, std: 0.00215, params: {'subsample': 0.8, 'colsample_bytree': 0.7}, mean: 0.84272, std: 0.00246, params: {'subsample': 0.9, 'colsample_bytree': 0.7}, mean: 0.84001, std: 0.00202, params: {'subsample': 1.0, 'colsample_bytree': 0.7}, mean: 0.83446, std: 0.00193, params: {'subsample': 0.7, 'colsample_bytree': 0.8}, mean: 0.83755, std: 0.00192, params: {'subsample': 0.8, 'colsample_bytree': 0.8}, mean: 0.84020, std: 0.00204, params: {'subsample': 0.9, 'colsample_bytree': 0.8}, mean: 0.83908, std: 0.00218, params: {'subsample': 1.0, 'colsample_bytree': 0.8}, mean: 0.83090, std: 0.00167, params: {'subsample': 0.7, 'colsample_bytree': 0.9}, mean: 0.83495, std: 0.00179, params: {'subsample': 0.8, 'colsample_bytree': 0.9}, mean: 0.83721, std: 0.00253, params: {'subsample': 0.9, 'colsample_bytree': 0.9}, mean: 0.83773, std: 0.00220, params: {'subsample': 1.0, 'colsample_bytree': 0.9}, mean: 0.82819, std: 0.00174, params: {'subsample': 0.7, 'colsample_bytree': 1.0}, mean: 0.83353, std: 0.00225, params: {'subsample': 0.8, 'colsample_bytree': 1.0}, mean: 0.83679, std: 0.00197, params: {'subsample': 0.9, 'colsample_bytree': 1.0}, mean: 0.83684, std: 0.00271, params: {'subsample': 1.0, 'colsample_bytree': 1.0}])\n"
     ]
    }
   ],
   "source": [
    "params3 = {\n",
    "    'subsample':[i / 10.0 for i in range(7, 11)],\n",
    "    'colsample_bytree':[i / 10.0 for i in range(7, 11)]\n",
    "}\n",
    "\n",
    "test_params(classifier_for_gridsearch, params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training Using SKL Wrapper\n",
    "\n",
    "XGBoost provides a convenient SKL wrapper so you can use it as you would the other models in SKL. \n",
    "\n",
    "While XGBoost's native models tend to be a bit faster and let you access more of XGB's inteface,\n",
    "  we'll use the SKL wrapper here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.743262\n",
      "Will train until validation_0-auc hasn't improved in 40 rounds.\n",
      "[40]\tvalidation_0-auc:0.784491\n",
      "[80]\tvalidation_0-auc:0.795197\n",
      "[120]\tvalidation_0-auc:0.80188\n",
      "[160]\tvalidation_0-auc:0.807302\n",
      "[200]\tvalidation_0-auc:0.811926\n",
      "[240]\tvalidation_0-auc:0.815163\n",
      "[280]\tvalidation_0-auc:0.817509\n",
      "[320]\tvalidation_0-auc:0.819837\n",
      "[360]\tvalidation_0-auc:0.821489\n",
      "[400]\tvalidation_0-auc:0.822823\n",
      "[440]\tvalidation_0-auc:0.824378\n",
      "[480]\tvalidation_0-auc:0.825412\n",
      "[520]\tvalidation_0-auc:0.827066\n",
      "[560]\tvalidation_0-auc:0.828169\n",
      "[600]\tvalidation_0-auc:0.829378\n",
      "[640]\tvalidation_0-auc:0.830374\n",
      "[680]\tvalidation_0-auc:0.831341\n",
      "[720]\tvalidation_0-auc:0.832451\n",
      "[760]\tvalidation_0-auc:0.833484\n",
      "[800]\tvalidation_0-auc:0.833997\n",
      "[840]\tvalidation_0-auc:0.834512\n",
      "[880]\tvalidation_0-auc:0.835258\n",
      "[920]\tvalidation_0-auc:0.835846\n",
      "[960]\tvalidation_0-auc:0.836545\n",
      "[1000]\tvalidation_0-auc:0.836908\n",
      "[1040]\tvalidation_0-auc:0.837567\n",
      "[1080]\tvalidation_0-auc:0.837881\n",
      "[1120]\tvalidation_0-auc:0.838382\n",
      "[1160]\tvalidation_0-auc:0.838929\n",
      "[1200]\tvalidation_0-auc:0.839505\n",
      "[1240]\tvalidation_0-auc:0.839874\n",
      "[1280]\tvalidation_0-auc:0.840457\n",
      "[1320]\tvalidation_0-auc:0.840996\n",
      "[1360]\tvalidation_0-auc:0.841456\n",
      "[1400]\tvalidation_0-auc:0.841561\n",
      "[1440]\tvalidation_0-auc:0.841935\n",
      "[1480]\tvalidation_0-auc:0.842136\n",
      "[1520]\tvalidation_0-auc:0.842423\n",
      "[1560]\tvalidation_0-auc:0.842739\n",
      "[1600]\tvalidation_0-auc:0.842872\n",
      "[1640]\tvalidation_0-auc:0.843259\n",
      "[1680]\tvalidation_0-auc:0.843551\n",
      "[1720]\tvalidation_0-auc:0.843683\n",
      "[1760]\tvalidation_0-auc:0.843946\n",
      "[1800]\tvalidation_0-auc:0.844139\n",
      "[1840]\tvalidation_0-auc:0.844333\n",
      "[1880]\tvalidation_0-auc:0.844585\n",
      "[1920]\tvalidation_0-auc:0.845013\n",
      "[1960]\tvalidation_0-auc:0.845237\n",
      "[1999]\tvalidation_0-auc:0.845424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,\n",
       "       gamma=0, learning_rate=0.02, max_delta_step=0, max_depth=9,\n",
       "       min_child_weight=1, missing=None, n_estimators=2000, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=2016, silent=True, subsample=0.9)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_boost = xgb.XGBClassifier(\n",
    "    missing=np.nan, \n",
    "    n_estimators=2400, \n",
    "    learning_rate=0.02, \n",
    "    silent=True,        \n",
    "    max_depth=9, \n",
    "    min_child_weight=1, \n",
    "    gamma=0, \n",
    "    subsample=0.9, \n",
    "    colsample_bytree=0.7,            \n",
    "    nthread=-1, \n",
    "    seed=2016, \n",
    "    objective='binary:logistic'\n",
    ")\n",
    "\n",
    "# Here I'm using performance on the test set for early stopping.\n",
    "#   Given the large number of training examples and few columns, \n",
    "#   I think this is acceptable. But you might prefer to set the\n",
    "#   number of training rounds purely with CV.\n",
    "skl_boost.fit(X_train_enc, Y_train_enc, early_stopping_rounds=40, verbose=40,\n",
    "            eval_metric=\"auc\", eval_set=[(X_test_enc, Y_test_enc)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skl_boost = xgb.XGBClassifier(\n",
    "    missing=np.nan, \n",
    "    n_estimators=1686, \n",
    "    learning_rate=0.02, \n",
    "    silent=True,        \n",
    "    max_depth=9, \n",
    "    min_child_weight=1, \n",
    "    gamma=0, \n",
    "    subsample=0.9, \n",
    "    colsample_bytree=0.7,            \n",
    "    nthread=-1, \n",
    "    seed=2016, \n",
    "    objective='binary:logistic'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Scores & Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of model vs. dummy: \n",
      "Error of 0.679, 0.500 for model, dummy; -35.791 percent less error.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6789536671988217"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify above w/ k-fold CV & different seed (so we're not overfitting to those particular cv-splits)\n",
    "\n",
    "get_repeated_k_fold_score(skl_boost, X_train_enc, Y_train_enc, folds=10, iters=1,\n",
    "                              agg_inner=None, agg_outer=None, seed=2002,\n",
    "                              score_func=roc_auc_score, fit_kwargs=None,\n",
    "                              print_description=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.221061+0.00489284\ttest-error:0.224403+0.00524578\n",
      "[20]\ttrain-error:0.198558+0.00137553\ttest-error:0.202172+0.00189445\n",
      "[40]\ttrain-error:0.194455+0.00110733\ttest-error:0.199185+0.00156989\n",
      "[60]\ttrain-error:0.191194+0.00107074\ttest-error:0.196616+0.00156405\n",
      "[80]\ttrain-error:0.188156+0.000778677\ttest-error:0.193857+0.00147352\n",
      "[100]\ttrain-error:0.185744+0.000766213\ttest-error:0.19173+0.00125641\n",
      "[120]\ttrain-error:0.183226+0.000649301\ttest-error:0.189435+0.00141424\n",
      "[140]\ttrain-error:0.180722+0.000449606\ttest-error:0.187506+0.00126284\n",
      "[160]\ttrain-error:0.178492+0.000570488\ttest-error:0.185395+0.00135194\n",
      "[180]\ttrain-error:0.176483+0.00060382\ttest-error:0.183459+0.00143589\n",
      "[200]\ttrain-error:0.174496+0.000608873\ttest-error:0.181627+0.0015561\n",
      "[220]\ttrain-error:0.172856+0.00057987\ttest-error:0.180427+0.00166718\n",
      "[240]\ttrain-error:0.171551+0.000562294\ttest-error:0.179522+0.00160945\n",
      "[260]\ttrain-error:0.170408+0.000572816\ttest-error:0.178551+0.00161733\n",
      "[280]\ttrain-error:0.169335+0.000461947\ttest-error:0.17766+0.0014494\n",
      "[300]\ttrain-error:0.168436+0.000456619\ttest-error:0.176888+0.00131774\n",
      "[320]\ttrain-error:0.167581+0.000426162\ttest-error:0.176255+0.00108376\n",
      "[340]\ttrain-error:0.166802+0.000447369\ttest-error:0.175563+0.00106874\n",
      "[360]\ttrain-error:0.166034+0.00040892\ttest-error:0.175173+0.00101594\n",
      "[380]\ttrain-error:0.165305+0.000412802\ttest-error:0.174725+0.000956793\n",
      "[400]\ttrain-error:0.164639+0.00040259\ttest-error:0.174305+0.000870114\n",
      "[420]\ttrain-error:0.16399+0.000442411\ttest-error:0.173716+0.000998041\n",
      "[440]\ttrain-error:0.163335+0.000419896\ttest-error:0.17329+0.000943744\n",
      "[460]\ttrain-error:0.162563+0.000321566\ttest-error:0.172848+0.000934658\n",
      "[480]\ttrain-error:0.162019+0.000289289\ttest-error:0.172495+0.000804425\n",
      "[500]\ttrain-error:0.161368+0.000342763\ttest-error:0.172325+0.000732776\n",
      "[520]\ttrain-error:0.160639+0.000299701\ttest-error:0.171921+0.00082517\n",
      "[540]\ttrain-error:0.160086+0.000344351\ttest-error:0.17159+0.000672181\n",
      "[560]\ttrain-error:0.159527+0.000309463\ttest-error:0.171523+0.000815865\n",
      "[580]\ttrain-error:0.158954+0.000359277\ttest-error:0.17117+0.00092372\n",
      "[600]\ttrain-error:0.15833+0.000420418\ttest-error:0.170971+0.000878218\n",
      "[620]\ttrain-error:0.157786+0.000430296\ttest-error:0.170788+0.000895049\n",
      "[640]\ttrain-error:0.157118+0.000457291\ttest-error:0.170515+0.0008054\n",
      "[660]\ttrain-error:0.15653+0.000401612\ttest-error:0.170162+0.000855623\n",
      "[680]\ttrain-error:0.156005+0.0004078\ttest-error:0.169927+0.000939594\n",
      "[700]\ttrain-error:0.155495+0.000433354\ttest-error:0.169765+0.000950004\n",
      "[720]\ttrain-error:0.154956+0.000464642\ttest-error:0.169404+0.000939624\n",
      "[740]\ttrain-error:0.154428+0.00048018\ttest-error:0.16922+0.00109687\n",
      "[760]\ttrain-error:0.153933+0.000468627\ttest-error:0.169029+0.00101068\n",
      "[780]\ttrain-error:0.153456+0.000463019\ttest-error:0.168852+0.000923257\n",
      "[800]\ttrain-error:0.152862+0.000469569\ttest-error:0.168595+0.000928567\n",
      "[820]\ttrain-error:0.152376+0.000504465\ttest-error:0.168469+0.000916096\n",
      "[840]\ttrain-error:0.151831+0.000480865\ttest-error:0.168271+0.00095934\n",
      "[860]\ttrain-error:0.151347+0.000482976\ttest-error:0.168175+0.00104777\n",
      "[880]\ttrain-error:0.150881+0.000454849\ttest-error:0.167962+0.0010594\n",
      "[900]\ttrain-error:0.1504+0.000488052\ttest-error:0.167866+0.000955878\n",
      "[920]\ttrain-error:0.149924+0.000471824\ttest-error:0.167653+0.00106416\n",
      "[940]\ttrain-error:0.149337+0.000535097\ttest-error:0.167601+0.00106566\n",
      "[960]\ttrain-error:0.148865+0.000464816\ttest-error:0.167322+0.00109959\n",
      "[980]\ttrain-error:0.148348+0.000495603\ttest-error:0.167145+0.00102648\n",
      "[1000]\ttrain-error:0.147854+0.000497573\ttest-error:0.16699+0.00110779\n",
      "[1020]\ttrain-error:0.147366+0.000451596\ttest-error:0.166902+0.00123188\n",
      "[1040]\ttrain-error:0.146884+0.000429251\ttest-error:0.166645+0.00130581\n",
      "[1060]\ttrain-error:0.14647+0.00034647\ttest-error:0.16649+0.00129708\n",
      "[1080]\ttrain-error:0.146049+0.00039342\ttest-error:0.166431+0.00123885\n",
      "[1100]\ttrain-error:0.145537+0.00037747\ttest-error:0.166284+0.00130002\n",
      "[1120]\ttrain-error:0.145126+0.000376342\ttest-error:0.166181+0.00125539\n",
      "[1140]\ttrain-error:0.144686+0.000363335\ttest-error:0.16599+0.00125768\n",
      "[1160]\ttrain-error:0.144204+0.00038557\ttest-error:0.165872+0.00124569\n",
      "[1180]\ttrain-error:0.143735+0.000347527\ttest-error:0.165585+0.00120539\n",
      "[1200]\ttrain-error:0.143311+0.000390592\ttest-error:0.165482+0.00119063\n",
      "[1220]\ttrain-error:0.142865+0.000417379\ttest-error:0.165393+0.00125919\n",
      "[1240]\ttrain-error:0.142338+0.000432796\ttest-error:0.165305+0.00130093\n",
      "[1260]\ttrain-error:0.141932+0.000441362\ttest-error:0.165107+0.0012574\n",
      "[1280]\ttrain-error:0.141469+0.000468851\ttest-error:0.164982+0.00121969\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.224403</td>\n",
       "      <td>0.005246</td>\n",
       "      <td>0.221061</td>\n",
       "      <td>0.004893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.216853</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.213367</td>\n",
       "      <td>0.008318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.214734</td>\n",
       "      <td>0.007105</td>\n",
       "      <td>0.211057</td>\n",
       "      <td>0.007357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.211408</td>\n",
       "      <td>0.006326</td>\n",
       "      <td>0.208266</td>\n",
       "      <td>0.006403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210627</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>0.007425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.208913</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.205374</td>\n",
       "      <td>0.005211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.207154</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.203516</td>\n",
       "      <td>0.005004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.206992</td>\n",
       "      <td>0.005207</td>\n",
       "      <td>0.203350</td>\n",
       "      <td>0.005254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.206455</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.202681</td>\n",
       "      <td>0.005355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.206139</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>0.202377</td>\n",
       "      <td>0.005639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.205087</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>0.201184</td>\n",
       "      <td>0.004917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.204387</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.200678</td>\n",
       "      <td>0.003372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.204292</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>0.200328</td>\n",
       "      <td>0.003036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.203593</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>0.199644</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.203431</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.199418</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.203019</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.199287</td>\n",
       "      <td>0.001973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.203203</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>0.199635</td>\n",
       "      <td>0.002185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.202702</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.199276</td>\n",
       "      <td>0.001597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.202570</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.199218</td>\n",
       "      <td>0.001326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.202415</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.198885</td>\n",
       "      <td>0.001554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.202172</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.198558</td>\n",
       "      <td>0.001376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.201922</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.198316</td>\n",
       "      <td>0.001318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.201768</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.197959</td>\n",
       "      <td>0.001290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.201554</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.197817</td>\n",
       "      <td>0.001384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.201569</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.197651</td>\n",
       "      <td>0.001309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.201429</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.197428</td>\n",
       "      <td>0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.197253</td>\n",
       "      <td>0.001270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.201311</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.197375</td>\n",
       "      <td>0.001487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.201363</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.197284</td>\n",
       "      <td>0.001568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.200627</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.196761</td>\n",
       "      <td>0.001578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>0.165107</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>0.165099</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.142076</td>\n",
       "      <td>0.000419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>0.165099</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.142054</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>0.165107</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.142042</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>0.165077</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.142024</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>0.165019</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.142010</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>0.165085</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.141986</td>\n",
       "      <td>0.000434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>0.165092</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.141966</td>\n",
       "      <td>0.000433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>0.165063</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.141953</td>\n",
       "      <td>0.000442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>0.165107</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.141932</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>0.165085</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.141915</td>\n",
       "      <td>0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>0.165077</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.141890</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>0.165063</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.141861</td>\n",
       "      <td>0.000455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>0.165077</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.141830</td>\n",
       "      <td>0.000444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>0.165077</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.141803</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0.165077</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.141770</td>\n",
       "      <td>0.000444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>0.165055</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.141761</td>\n",
       "      <td>0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>0.165018</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.141744</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>0.165041</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.141725</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>0.165048</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.141704</td>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>0.165048</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.141686</td>\n",
       "      <td>0.000475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>0.165004</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.141670</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>0.164989</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.141642</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>0.164996</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.141614</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>0.165026</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.141595</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>0.165026</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.141581</td>\n",
       "      <td>0.000467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>0.165011</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.141558</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>0.165004</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.141536</td>\n",
       "      <td>0.000470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>0.165004</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.141503</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>0.164982</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.141469</td>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1281 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      test-error-mean  test-error-std  train-error-mean  train-error-std\n",
       "0            0.224403        0.005246          0.221061         0.004893\n",
       "1            0.216853        0.008242          0.213367         0.008318\n",
       "2            0.214734        0.007105          0.211057         0.007357\n",
       "3            0.211408        0.006326          0.208266         0.006403\n",
       "4            0.210627        0.007650          0.207371         0.007425\n",
       "5            0.208913        0.005325          0.205374         0.005211\n",
       "6            0.207154        0.005072          0.203516         0.005004\n",
       "7            0.206992        0.005207          0.203350         0.005254\n",
       "8            0.206455        0.005200          0.202681         0.005355\n",
       "9            0.206139        0.005536          0.202377         0.005639\n",
       "10           0.205087        0.005154          0.201184         0.004917\n",
       "11           0.204387        0.003712          0.200678         0.003372\n",
       "12           0.204292        0.003450          0.200328         0.003036\n",
       "13           0.203593        0.002831          0.199644         0.002334\n",
       "14           0.203431        0.002784          0.199418         0.002300\n",
       "15           0.203019        0.002364          0.199287         0.001973\n",
       "16           0.203203        0.002942          0.199635         0.002185\n",
       "17           0.202702        0.002283          0.199276         0.001597\n",
       "18           0.202570        0.002127          0.199218         0.001326\n",
       "19           0.202415        0.002187          0.198885         0.001554\n",
       "20           0.202172        0.001894          0.198558         0.001376\n",
       "21           0.201922        0.001569          0.198316         0.001318\n",
       "22           0.201768        0.001436          0.197959         0.001290\n",
       "23           0.201554        0.001431          0.197817         0.001384\n",
       "24           0.201569        0.001462          0.197651         0.001309\n",
       "25           0.201429        0.001360          0.197428         0.001421\n",
       "26           0.201172        0.001483          0.197253         0.001270\n",
       "27           0.201311        0.001323          0.197375         0.001487\n",
       "28           0.201363        0.001402          0.197284         0.001568\n",
       "29           0.200627        0.001628          0.196761         0.001578\n",
       "...               ...             ...               ...              ...\n",
       "1251         0.165107        0.001247          0.142096         0.000413\n",
       "1252         0.165099        0.001264          0.142076         0.000419\n",
       "1253         0.165099        0.001291          0.142054         0.000430\n",
       "1254         0.165107        0.001285          0.142042         0.000441\n",
       "1255         0.165077        0.001261          0.142024         0.000437\n",
       "1256         0.165019        0.001232          0.142010         0.000436\n",
       "1257         0.165085        0.001311          0.141986         0.000434\n",
       "1258         0.165092        0.001309          0.141966         0.000433\n",
       "1259         0.165063        0.001269          0.141953         0.000442\n",
       "1260         0.165107        0.001257          0.141932         0.000441\n",
       "1261         0.165085        0.001251          0.141915         0.000445\n",
       "1262         0.165077        0.001307          0.141890         0.000430\n",
       "1263         0.165063        0.001304          0.141861         0.000455\n",
       "1264         0.165077        0.001315          0.141830         0.000444\n",
       "1265         0.165077        0.001333          0.141803         0.000438\n",
       "1266         0.165077        0.001387          0.141770         0.000444\n",
       "1267         0.165055        0.001346          0.141761         0.000451\n",
       "1268         0.165018        0.001323          0.141744         0.000454\n",
       "1269         0.165041        0.001328          0.141725         0.000462\n",
       "1270         0.165048        0.001345          0.141704         0.000469\n",
       "1271         0.165048        0.001353          0.141686         0.000475\n",
       "1272         0.165004        0.001314          0.141670         0.000462\n",
       "1273         0.164989        0.001291          0.141642         0.000462\n",
       "1274         0.164996        0.001296          0.141614         0.000462\n",
       "1275         0.165026        0.001256          0.141595         0.000460\n",
       "1276         0.165026        0.001280          0.141581         0.000467\n",
       "1277         0.165011        0.001272          0.141558         0.000465\n",
       "1278         0.165004        0.001237          0.141536         0.000470\n",
       "1279         0.165004        0.001216          0.141503         0.000462\n",
       "1280         0.164982        0.001220          0.141469         0.000469\n",
       "\n",
       "[1281 rows x 4 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV perf with a different seed is far worse than test performance, \n",
    "#   suggesting we might be overfitting to the test set. \n",
    "# Let's try checking how many rds we get with CV and lower lrate.\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth':9, \n",
    "    'eta':.02, \n",
    "    'silent':0, \n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric':'auc', \n",
    "    'subsample':0.90, \n",
    "    'colsample_bytree':0.70\n",
    "}\n",
    "\n",
    "xgb.cv(\n",
    "    xgb_params, \n",
    "    dtrain, \n",
    "    nfold=10, \n",
    "    stratified=True, \n",
    "    num_boost_round=2500, \n",
    "    early_stopping_rounds=20,\n",
    "    metrics={'error'}, # try metrics={'logloss', 'auc'}\n",
    "    seed = 2003, \n",
    "    verbose_eval=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now try using cv rounds instead of test-early-stopping rounds (1280 instead of 1686)\n",
    "\n",
    "skl_boost = xgb.XGBClassifier(\n",
    "    missing=np.nan, \n",
    "    n_estimators=1280, \n",
    "    learning_rate=0.02, \n",
    "    silent=True,        \n",
    "    max_depth=9, \n",
    "    min_child_weight=1, \n",
    "    gamma=0, \n",
    "    subsample=0.9, \n",
    "    colsample_bytree=0.7,            \n",
    "    nthread=-1, \n",
    "    seed=2016, \n",
    "    objective='binary:logistic'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get CV error.\n",
    "\n",
    "get_repeated_k_fold_score(skl_boost, X_train_enc, Y_train_enc, folds=10, iters=1,\n",
    "                              agg_inner=None, agg_outer=None, seed=2002,\n",
    "                              score_func=roc_auc_score, fit_kwargs=None,\n",
    "                              print_description=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score:\n",
      "0.840451837071\n",
      "\n",
      "LogLoss:\n",
      "0.383405132526\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.834     0.979     0.901     25951\n",
      "       True      0.848     0.370     0.515      8023\n",
      "\n",
      "avg / total      0.837     0.836     0.810     33974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score on test set.\n",
    "skl_boost.fit(X_train_enc, Y_train_enc)\n",
    "score_classif_on_test(skl_boost, X_test_enc, Y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost provides a convenient function to show feature importances.\n",
    "# It's even more useful than with RF, since gradient-boosted trees tend to \n",
    "#   \"pick\" between highly correlated features, choosing just one feature to represent the trend.\n",
    "print(\"\\n\\n Feature importances: \")\n",
    "print(xgb.plot_importance(skl_boost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got an extra 1%+ accuracy from xgboost vs. rf (.823 vs .836). We lost some recall for the true cases.\n",
    "\n",
    "### Tweaking the decision threshold\n",
    "\n",
    "We wanted to be able to correctly recall instances of diabetes - so here we'll tweak the decision threshold up to trade precision for recall (and, specifically, recall for the positive cases).\n",
    "\n",
    "We'll graph AUC to get a better idea of the trade-offs involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAGJCAYAAAA5XRHmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1faccdc4320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from @motuai10\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test_enc, predicted_probs)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.881     0.864     0.872     25968\n",
      "       True      0.584     0.621     0.602      8006\n",
      "\n",
      "avg / total      0.811     0.806     0.808     33974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred_30 = predicted_probs >= .30\n",
    "print(classification_report(Y_test_enc, Y_pred_30, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now recall 62% of true positives - much better than 37%!   \n",
    "\n",
    "Only 58% of our positives actually have diabetes, but that seems like an acceptable trade-off from a medical standpoint; it's OK to spend a little extra time checking on these people if they more likely than not have diabetes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 2 - LightGBM\n",
    "\n",
    "I originally wrote this notebook in Summer 2016, and since then, LightGBM has risen to prominence as a faster and often more accurate alternative to XGBoost. By using binning, it can typically run 5-15x faster, and XGBoost is already quite fast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start by using CV to get rounds for a quicker-training model.\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': ['rmse', 'auc'],\n",
    "    'num_leaves': 31,\n",
    "    'max_depth':-1,\n",
    "    'max_bin':255,\n",
    "    'min_split_gain':0,\n",
    "    'min_child_weight':5,\n",
    "    'min_child_samples':30,\n",
    "    'subsample':.8,\n",
    "    'subsample_freq':1,\n",
    "    'colsample_bytree':.8,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':0,\n",
    "    'seed':0,\n",
    "    'nthread':-1,\n",
    "    'verbose': 20\n",
    "}\n",
    "\n",
    "# Set up training dataset as lgb.Dataset object. \n",
    "# Using LGB's built in cv() method is faster than \n",
    "#   relying on SKLearn wrapper (e.g. because it parallelizes better), \n",
    "#   but we'll have to fall back to SKLearn for some tuning. \n",
    "\n",
    "lgb_train = lgb.Dataset(X_train_enc, Y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's auc: 0.706636\tcv_agg's rmse: 0.482602\n",
      "[2]\tcv_agg's auc: 0.721756\tcv_agg's rmse: 0.467919\n",
      "[3]\tcv_agg's auc: 0.724896\tcv_agg's rmse: 0.456002\n",
      "[4]\tcv_agg's auc: 0.733995\tcv_agg's rmse: 0.44575\n",
      "[5]\tcv_agg's auc: 0.73887\tcv_agg's rmse: 0.436724\n",
      "[6]\tcv_agg's auc: 0.741884\tcv_agg's rmse: 0.429615\n",
      "[7]\tcv_agg's auc: 0.744562\tcv_agg's rmse: 0.423175\n",
      "[8]\tcv_agg's auc: 0.74603\tcv_agg's rmse: 0.417762\n",
      "[9]\tcv_agg's auc: 0.747966\tcv_agg's rmse: 0.413048\n",
      "[10]\tcv_agg's auc: 0.749728\tcv_agg's rmse: 0.408965\n",
      "[11]\tcv_agg's auc: 0.750832\tcv_agg's rmse: 0.405894\n",
      "[12]\tcv_agg's auc: 0.75228\tcv_agg's rmse: 0.403231\n",
      "[13]\tcv_agg's auc: 0.753666\tcv_agg's rmse: 0.400536\n",
      "[14]\tcv_agg's auc: 0.753817\tcv_agg's rmse: 0.399093\n",
      "[15]\tcv_agg's auc: 0.755498\tcv_agg's rmse: 0.396625\n",
      "[16]\tcv_agg's auc: 0.756704\tcv_agg's rmse: 0.394863\n",
      "[17]\tcv_agg's auc: 0.757842\tcv_agg's rmse: 0.393304\n",
      "[18]\tcv_agg's auc: 0.758938\tcv_agg's rmse: 0.392189\n",
      "[19]\tcv_agg's auc: 0.760357\tcv_agg's rmse: 0.390692\n",
      "[20]\tcv_agg's auc: 0.761433\tcv_agg's rmse: 0.389428\n",
      "[21]\tcv_agg's auc: 0.762516\tcv_agg's rmse: 0.388562\n",
      "[22]\tcv_agg's auc: 0.763137\tcv_agg's rmse: 0.387735\n",
      "[23]\tcv_agg's auc: 0.763945\tcv_agg's rmse: 0.387016\n",
      "[24]\tcv_agg's auc: 0.76466\tcv_agg's rmse: 0.386242\n",
      "[25]\tcv_agg's auc: 0.765805\tcv_agg's rmse: 0.385406\n",
      "[26]\tcv_agg's auc: 0.766803\tcv_agg's rmse: 0.384787\n",
      "[27]\tcv_agg's auc: 0.767782\tcv_agg's rmse: 0.384108\n",
      "[28]\tcv_agg's auc: 0.768349\tcv_agg's rmse: 0.38362\n",
      "[29]\tcv_agg's auc: 0.769075\tcv_agg's rmse: 0.383164\n",
      "[30]\tcv_agg's auc: 0.769637\tcv_agg's rmse: 0.382819\n",
      "[31]\tcv_agg's auc: 0.770174\tcv_agg's rmse: 0.382412\n",
      "[32]\tcv_agg's auc: 0.770672\tcv_agg's rmse: 0.382141\n",
      "[33]\tcv_agg's auc: 0.771168\tcv_agg's rmse: 0.38191\n",
      "[34]\tcv_agg's auc: 0.771734\tcv_agg's rmse: 0.381418\n",
      "[35]\tcv_agg's auc: 0.772372\tcv_agg's rmse: 0.380968\n",
      "[36]\tcv_agg's auc: 0.772933\tcv_agg's rmse: 0.380598\n",
      "[37]\tcv_agg's auc: 0.773702\tcv_agg's rmse: 0.380234\n",
      "[38]\tcv_agg's auc: 0.774197\tcv_agg's rmse: 0.379966\n",
      "[39]\tcv_agg's auc: 0.775126\tcv_agg's rmse: 0.379436\n",
      "[40]\tcv_agg's auc: 0.775898\tcv_agg's rmse: 0.379144\n",
      "[41]\tcv_agg's auc: 0.776404\tcv_agg's rmse: 0.378861\n",
      "[42]\tcv_agg's auc: 0.777303\tcv_agg's rmse: 0.378297\n",
      "[43]\tcv_agg's auc: 0.777901\tcv_agg's rmse: 0.377999\n",
      "[44]\tcv_agg's auc: 0.778485\tcv_agg's rmse: 0.377758\n",
      "[45]\tcv_agg's auc: 0.778972\tcv_agg's rmse: 0.377476\n",
      "[46]\tcv_agg's auc: 0.779533\tcv_agg's rmse: 0.377149\n",
      "[47]\tcv_agg's auc: 0.780111\tcv_agg's rmse: 0.376855\n",
      "[48]\tcv_agg's auc: 0.780607\tcv_agg's rmse: 0.376658\n",
      "[49]\tcv_agg's auc: 0.781129\tcv_agg's rmse: 0.37643\n",
      "[50]\tcv_agg's auc: 0.781708\tcv_agg's rmse: 0.37618\n",
      "[51]\tcv_agg's auc: 0.782204\tcv_agg's rmse: 0.376029\n",
      "[52]\tcv_agg's auc: 0.782792\tcv_agg's rmse: 0.375808\n",
      "[53]\tcv_agg's auc: 0.783279\tcv_agg's rmse: 0.375592\n",
      "[54]\tcv_agg's auc: 0.783579\tcv_agg's rmse: 0.375459\n",
      "[55]\tcv_agg's auc: 0.783895\tcv_agg's rmse: 0.375272\n",
      "[56]\tcv_agg's auc: 0.784343\tcv_agg's rmse: 0.375031\n",
      "[57]\tcv_agg's auc: 0.784657\tcv_agg's rmse: 0.3749\n",
      "[58]\tcv_agg's auc: 0.785066\tcv_agg's rmse: 0.374682\n",
      "[59]\tcv_agg's auc: 0.785374\tcv_agg's rmse: 0.374529\n",
      "[60]\tcv_agg's auc: 0.785548\tcv_agg's rmse: 0.374442\n",
      "[61]\tcv_agg's auc: 0.785933\tcv_agg's rmse: 0.374271\n",
      "[62]\tcv_agg's auc: 0.786293\tcv_agg's rmse: 0.374138\n",
      "[63]\tcv_agg's auc: 0.786656\tcv_agg's rmse: 0.373969\n",
      "[64]\tcv_agg's auc: 0.787033\tcv_agg's rmse: 0.373728\n",
      "[65]\tcv_agg's auc: 0.787383\tcv_agg's rmse: 0.373597\n",
      "[66]\tcv_agg's auc: 0.787515\tcv_agg's rmse: 0.373538\n",
      "[67]\tcv_agg's auc: 0.78791\tcv_agg's rmse: 0.37333\n",
      "[68]\tcv_agg's auc: 0.788462\tcv_agg's rmse: 0.37314\n",
      "[69]\tcv_agg's auc: 0.788757\tcv_agg's rmse: 0.37296\n",
      "[70]\tcv_agg's auc: 0.78905\tcv_agg's rmse: 0.372851\n",
      "[71]\tcv_agg's auc: 0.789403\tcv_agg's rmse: 0.372699\n",
      "[72]\tcv_agg's auc: 0.789711\tcv_agg's rmse: 0.372573\n",
      "[73]\tcv_agg's auc: 0.789965\tcv_agg's rmse: 0.372417\n",
      "[74]\tcv_agg's auc: 0.790219\tcv_agg's rmse: 0.372296\n",
      "[75]\tcv_agg's auc: 0.790561\tcv_agg's rmse: 0.372116\n",
      "[76]\tcv_agg's auc: 0.790792\tcv_agg's rmse: 0.372034\n",
      "[77]\tcv_agg's auc: 0.791113\tcv_agg's rmse: 0.371909\n",
      "[78]\tcv_agg's auc: 0.791382\tcv_agg's rmse: 0.371779\n",
      "[79]\tcv_agg's auc: 0.791567\tcv_agg's rmse: 0.371675\n",
      "[80]\tcv_agg's auc: 0.791878\tcv_agg's rmse: 0.371547\n",
      "[81]\tcv_agg's auc: 0.792025\tcv_agg's rmse: 0.371487\n",
      "[82]\tcv_agg's auc: 0.792237\tcv_agg's rmse: 0.37141\n",
      "[83]\tcv_agg's auc: 0.792407\tcv_agg's rmse: 0.371319\n",
      "[84]\tcv_agg's auc: 0.792549\tcv_agg's rmse: 0.371167\n",
      "[85]\tcv_agg's auc: 0.792841\tcv_agg's rmse: 0.371054\n",
      "[86]\tcv_agg's auc: 0.793078\tcv_agg's rmse: 0.370964\n",
      "[87]\tcv_agg's auc: 0.793312\tcv_agg's rmse: 0.370869\n",
      "[88]\tcv_agg's auc: 0.793586\tcv_agg's rmse: 0.370717\n",
      "[89]\tcv_agg's auc: 0.793801\tcv_agg's rmse: 0.370613\n",
      "[90]\tcv_agg's auc: 0.794019\tcv_agg's rmse: 0.370527\n",
      "[91]\tcv_agg's auc: 0.794342\tcv_agg's rmse: 0.370419\n",
      "[92]\tcv_agg's auc: 0.794566\tcv_agg's rmse: 0.370317\n",
      "[93]\tcv_agg's auc: 0.794892\tcv_agg's rmse: 0.370204\n",
      "[94]\tcv_agg's auc: 0.795034\tcv_agg's rmse: 0.370132\n",
      "[95]\tcv_agg's auc: 0.795164\tcv_agg's rmse: 0.370058\n",
      "[96]\tcv_agg's auc: 0.795209\tcv_agg's rmse: 0.370032\n",
      "[97]\tcv_agg's auc: 0.795339\tcv_agg's rmse: 0.369967\n",
      "[98]\tcv_agg's auc: 0.795582\tcv_agg's rmse: 0.369867\n",
      "[99]\tcv_agg's auc: 0.79573\tcv_agg's rmse: 0.369765\n",
      "[100]\tcv_agg's auc: 0.795907\tcv_agg's rmse: 0.369703\n",
      "[101]\tcv_agg's auc: 0.796069\tcv_agg's rmse: 0.369612\n",
      "[102]\tcv_agg's auc: 0.796212\tcv_agg's rmse: 0.369532\n",
      "[103]\tcv_agg's auc: 0.796255\tcv_agg's rmse: 0.369498\n",
      "[104]\tcv_agg's auc: 0.796389\tcv_agg's rmse: 0.369409\n",
      "[105]\tcv_agg's auc: 0.796561\tcv_agg's rmse: 0.369331\n",
      "[106]\tcv_agg's auc: 0.796787\tcv_agg's rmse: 0.369239\n",
      "[107]\tcv_agg's auc: 0.796815\tcv_agg's rmse: 0.369222\n",
      "[108]\tcv_agg's auc: 0.797003\tcv_agg's rmse: 0.369149\n",
      "[109]\tcv_agg's auc: 0.7973\tcv_agg's rmse: 0.36905\n",
      "[110]\tcv_agg's auc: 0.797453\tcv_agg's rmse: 0.368987\n",
      "[111]\tcv_agg's auc: 0.797695\tcv_agg's rmse: 0.368898\n",
      "[112]\tcv_agg's auc: 0.797785\tcv_agg's rmse: 0.368844\n",
      "[113]\tcv_agg's auc: 0.797859\tcv_agg's rmse: 0.368807\n",
      "[114]\tcv_agg's auc: 0.798058\tcv_agg's rmse: 0.368718\n",
      "[115]\tcv_agg's auc: 0.798193\tcv_agg's rmse: 0.368645\n",
      "[116]\tcv_agg's auc: 0.798229\tcv_agg's rmse: 0.368601\n",
      "[117]\tcv_agg's auc: 0.798295\tcv_agg's rmse: 0.368546\n",
      "[118]\tcv_agg's auc: 0.798386\tcv_agg's rmse: 0.3685\n",
      "[119]\tcv_agg's auc: 0.798514\tcv_agg's rmse: 0.368455\n",
      "[120]\tcv_agg's auc: 0.798611\tcv_agg's rmse: 0.368399\n",
      "[121]\tcv_agg's auc: 0.798745\tcv_agg's rmse: 0.368344\n",
      "[122]\tcv_agg's auc: 0.798972\tcv_agg's rmse: 0.368255\n",
      "[123]\tcv_agg's auc: 0.799037\tcv_agg's rmse: 0.368224\n",
      "[124]\tcv_agg's auc: 0.799182\tcv_agg's rmse: 0.368156\n",
      "[125]\tcv_agg's auc: 0.799326\tcv_agg's rmse: 0.368084\n",
      "[126]\tcv_agg's auc: 0.799437\tcv_agg's rmse: 0.368017\n",
      "[127]\tcv_agg's auc: 0.799571\tcv_agg's rmse: 0.367965\n",
      "[128]\tcv_agg's auc: 0.79963\tcv_agg's rmse: 0.367912\n",
      "[129]\tcv_agg's auc: 0.799787\tcv_agg's rmse: 0.367844\n",
      "[130]\tcv_agg's auc: 0.799936\tcv_agg's rmse: 0.367787\n",
      "[131]\tcv_agg's auc: 0.800075\tcv_agg's rmse: 0.36773\n",
      "[132]\tcv_agg's auc: 0.800274\tcv_agg's rmse: 0.367666\n",
      "[133]\tcv_agg's auc: 0.800474\tcv_agg's rmse: 0.367599\n",
      "[134]\tcv_agg's auc: 0.800573\tcv_agg's rmse: 0.367543\n",
      "[135]\tcv_agg's auc: 0.800722\tcv_agg's rmse: 0.367473\n",
      "[136]\tcv_agg's auc: 0.800779\tcv_agg's rmse: 0.367451\n",
      "[137]\tcv_agg's auc: 0.800903\tcv_agg's rmse: 0.367406\n",
      "[138]\tcv_agg's auc: 0.801071\tcv_agg's rmse: 0.367348\n",
      "[139]\tcv_agg's auc: 0.80133\tcv_agg's rmse: 0.36725\n",
      "[140]\tcv_agg's auc: 0.80139\tcv_agg's rmse: 0.36722\n",
      "[141]\tcv_agg's auc: 0.801524\tcv_agg's rmse: 0.367144\n",
      "[142]\tcv_agg's auc: 0.801612\tcv_agg's rmse: 0.367094\n",
      "[143]\tcv_agg's auc: 0.801717\tcv_agg's rmse: 0.367043\n",
      "[144]\tcv_agg's auc: 0.801771\tcv_agg's rmse: 0.367008\n",
      "[145]\tcv_agg's auc: 0.801803\tcv_agg's rmse: 0.366991\n",
      "[146]\tcv_agg's auc: 0.801889\tcv_agg's rmse: 0.366947\n",
      "[147]\tcv_agg's auc: 0.802005\tcv_agg's rmse: 0.366917\n",
      "[148]\tcv_agg's auc: 0.802328\tcv_agg's rmse: 0.366817\n",
      "[149]\tcv_agg's auc: 0.802525\tcv_agg's rmse: 0.36675\n",
      "[150]\tcv_agg's auc: 0.802634\tcv_agg's rmse: 0.366708\n",
      "[151]\tcv_agg's auc: 0.802717\tcv_agg's rmse: 0.366681\n",
      "[152]\tcv_agg's auc: 0.802796\tcv_agg's rmse: 0.366646\n",
      "[153]\tcv_agg's auc: 0.802924\tcv_agg's rmse: 0.366607\n",
      "[154]\tcv_agg's auc: 0.803044\tcv_agg's rmse: 0.366562\n",
      "[155]\tcv_agg's auc: 0.803051\tcv_agg's rmse: 0.366546\n",
      "[156]\tcv_agg's auc: 0.803145\tcv_agg's rmse: 0.366496\n",
      "[157]\tcv_agg's auc: 0.803228\tcv_agg's rmse: 0.366457\n",
      "[158]\tcv_agg's auc: 0.803319\tcv_agg's rmse: 0.366415\n",
      "[159]\tcv_agg's auc: 0.803387\tcv_agg's rmse: 0.366397\n",
      "[160]\tcv_agg's auc: 0.803493\tcv_agg's rmse: 0.366354\n",
      "[161]\tcv_agg's auc: 0.803513\tcv_agg's rmse: 0.366341\n",
      "[162]\tcv_agg's auc: 0.80356\tcv_agg's rmse: 0.366317\n",
      "[163]\tcv_agg's auc: 0.803674\tcv_agg's rmse: 0.366276\n",
      "[164]\tcv_agg's auc: 0.803894\tcv_agg's rmse: 0.366207\n",
      "[165]\tcv_agg's auc: 0.803944\tcv_agg's rmse: 0.366172\n",
      "[166]\tcv_agg's auc: 0.803972\tcv_agg's rmse: 0.366146\n",
      "[167]\tcv_agg's auc: 0.804037\tcv_agg's rmse: 0.366124\n",
      "[168]\tcv_agg's auc: 0.804087\tcv_agg's rmse: 0.3661\n",
      "[169]\tcv_agg's auc: 0.804179\tcv_agg's rmse: 0.36606\n",
      "[170]\tcv_agg's auc: 0.804234\tcv_agg's rmse: 0.366039\n",
      "[171]\tcv_agg's auc: 0.804386\tcv_agg's rmse: 0.365986\n",
      "[172]\tcv_agg's auc: 0.804468\tcv_agg's rmse: 0.365949\n",
      "[173]\tcv_agg's auc: 0.804656\tcv_agg's rmse: 0.365894\n",
      "[174]\tcv_agg's auc: 0.804814\tcv_agg's rmse: 0.365829\n",
      "[175]\tcv_agg's auc: 0.804877\tcv_agg's rmse: 0.365797\n",
      "[176]\tcv_agg's auc: 0.804953\tcv_agg's rmse: 0.365769\n",
      "[177]\tcv_agg's auc: 0.805221\tcv_agg's rmse: 0.36567\n",
      "[178]\tcv_agg's auc: 0.805367\tcv_agg's rmse: 0.365613\n",
      "[179]\tcv_agg's auc: 0.805386\tcv_agg's rmse: 0.365593\n",
      "[180]\tcv_agg's auc: 0.805466\tcv_agg's rmse: 0.36556\n",
      "[181]\tcv_agg's auc: 0.805521\tcv_agg's rmse: 0.365521\n",
      "[182]\tcv_agg's auc: 0.805571\tcv_agg's rmse: 0.365494\n",
      "[183]\tcv_agg's auc: 0.805692\tcv_agg's rmse: 0.365429\n",
      "[184]\tcv_agg's auc: 0.805756\tcv_agg's rmse: 0.365401\n",
      "[185]\tcv_agg's auc: 0.805851\tcv_agg's rmse: 0.365357\n",
      "[186]\tcv_agg's auc: 0.805887\tcv_agg's rmse: 0.365332\n",
      "[187]\tcv_agg's auc: 0.806095\tcv_agg's rmse: 0.365273\n",
      "[188]\tcv_agg's auc: 0.806354\tcv_agg's rmse: 0.365189\n",
      "[189]\tcv_agg's auc: 0.806401\tcv_agg's rmse: 0.365165\n",
      "[190]\tcv_agg's auc: 0.80645\tcv_agg's rmse: 0.365147\n",
      "[191]\tcv_agg's auc: 0.80651\tcv_agg's rmse: 0.365123\n",
      "[192]\tcv_agg's auc: 0.806501\tcv_agg's rmse: 0.365111\n",
      "[193]\tcv_agg's auc: 0.806502\tcv_agg's rmse: 0.365105\n",
      "[194]\tcv_agg's auc: 0.806559\tcv_agg's rmse: 0.365072\n",
      "[195]\tcv_agg's auc: 0.806663\tcv_agg's rmse: 0.365028\n",
      "[196]\tcv_agg's auc: 0.806724\tcv_agg's rmse: 0.364995\n",
      "[197]\tcv_agg's auc: 0.806854\tcv_agg's rmse: 0.364952\n",
      "[198]\tcv_agg's auc: 0.806888\tcv_agg's rmse: 0.364938\n",
      "[199]\tcv_agg's auc: 0.807022\tcv_agg's rmse: 0.364894\n",
      "[200]\tcv_agg's auc: 0.80707\tcv_agg's rmse: 0.364862\n",
      "[201]\tcv_agg's auc: 0.807148\tcv_agg's rmse: 0.364812\n",
      "[202]\tcv_agg's auc: 0.807342\tcv_agg's rmse: 0.364758\n",
      "[203]\tcv_agg's auc: 0.807361\tcv_agg's rmse: 0.36474\n",
      "[204]\tcv_agg's auc: 0.8074\tcv_agg's rmse: 0.364718\n",
      "[205]\tcv_agg's auc: 0.807459\tcv_agg's rmse: 0.364687\n",
      "[206]\tcv_agg's auc: 0.807476\tcv_agg's rmse: 0.36468\n",
      "[207]\tcv_agg's auc: 0.807624\tcv_agg's rmse: 0.364627\n",
      "[208]\tcv_agg's auc: 0.807794\tcv_agg's rmse: 0.364574\n",
      "[209]\tcv_agg's auc: 0.807831\tcv_agg's rmse: 0.364552\n",
      "[210]\tcv_agg's auc: 0.807896\tcv_agg's rmse: 0.36452\n",
      "[211]\tcv_agg's auc: 0.807955\tcv_agg's rmse: 0.364498\n",
      "[212]\tcv_agg's auc: 0.808007\tcv_agg's rmse: 0.364479\n",
      "[213]\tcv_agg's auc: 0.808072\tcv_agg's rmse: 0.364449\n",
      "[214]\tcv_agg's auc: 0.808164\tcv_agg's rmse: 0.364411\n",
      "[215]\tcv_agg's auc: 0.808247\tcv_agg's rmse: 0.36438\n",
      "[216]\tcv_agg's auc: 0.808306\tcv_agg's rmse: 0.364355\n",
      "[217]\tcv_agg's auc: 0.808333\tcv_agg's rmse: 0.364342\n",
      "[218]\tcv_agg's auc: 0.808319\tcv_agg's rmse: 0.36434\n",
      "[219]\tcv_agg's auc: 0.808417\tcv_agg's rmse: 0.364282\n",
      "[220]\tcv_agg's auc: 0.808439\tcv_agg's rmse: 0.36427\n",
      "[221]\tcv_agg's auc: 0.80848\tcv_agg's rmse: 0.364245\n",
      "[222]\tcv_agg's auc: 0.808553\tcv_agg's rmse: 0.36422\n",
      "[223]\tcv_agg's auc: 0.808649\tcv_agg's rmse: 0.364168\n",
      "[224]\tcv_agg's auc: 0.808698\tcv_agg's rmse: 0.36415\n",
      "[225]\tcv_agg's auc: 0.808813\tcv_agg's rmse: 0.364102\n",
      "[226]\tcv_agg's auc: 0.808831\tcv_agg's rmse: 0.364088\n",
      "[227]\tcv_agg's auc: 0.808934\tcv_agg's rmse: 0.364053\n",
      "[228]\tcv_agg's auc: 0.809017\tcv_agg's rmse: 0.364021\n",
      "[229]\tcv_agg's auc: 0.809086\tcv_agg's rmse: 0.363994\n",
      "[230]\tcv_agg's auc: 0.809114\tcv_agg's rmse: 0.363978\n",
      "[231]\tcv_agg's auc: 0.809183\tcv_agg's rmse: 0.363953\n",
      "[232]\tcv_agg's auc: 0.809195\tcv_agg's rmse: 0.363931\n",
      "[233]\tcv_agg's auc: 0.809227\tcv_agg's rmse: 0.363914\n",
      "[234]\tcv_agg's auc: 0.809272\tcv_agg's rmse: 0.363889\n",
      "[235]\tcv_agg's auc: 0.80938\tcv_agg's rmse: 0.363848\n",
      "[236]\tcv_agg's auc: 0.809487\tcv_agg's rmse: 0.363806\n",
      "[237]\tcv_agg's auc: 0.80949\tcv_agg's rmse: 0.363797\n",
      "[238]\tcv_agg's auc: 0.809555\tcv_agg's rmse: 0.363761\n",
      "[239]\tcv_agg's auc: 0.809594\tcv_agg's rmse: 0.363744\n",
      "[240]\tcv_agg's auc: 0.809595\tcv_agg's rmse: 0.363733\n",
      "[241]\tcv_agg's auc: 0.809631\tcv_agg's rmse: 0.363717\n",
      "[242]\tcv_agg's auc: 0.809694\tcv_agg's rmse: 0.363677\n",
      "[243]\tcv_agg's auc: 0.809739\tcv_agg's rmse: 0.363655\n",
      "[244]\tcv_agg's auc: 0.809772\tcv_agg's rmse: 0.363642\n",
      "[245]\tcv_agg's auc: 0.809814\tcv_agg's rmse: 0.36362\n",
      "[246]\tcv_agg's auc: 0.809902\tcv_agg's rmse: 0.363591\n",
      "[247]\tcv_agg's auc: 0.809923\tcv_agg's rmse: 0.363576\n",
      "[248]\tcv_agg's auc: 0.809979\tcv_agg's rmse: 0.363553\n",
      "[249]\tcv_agg's auc: 0.810002\tcv_agg's rmse: 0.363543\n",
      "[250]\tcv_agg's auc: 0.810007\tcv_agg's rmse: 0.363536\n",
      "[251]\tcv_agg's auc: 0.810044\tcv_agg's rmse: 0.363525\n",
      "[252]\tcv_agg's auc: 0.810049\tcv_agg's rmse: 0.363517\n",
      "[253]\tcv_agg's auc: 0.810077\tcv_agg's rmse: 0.363499\n",
      "[254]\tcv_agg's auc: 0.810144\tcv_agg's rmse: 0.363467\n",
      "[255]\tcv_agg's auc: 0.810201\tcv_agg's rmse: 0.363445\n",
      "[256]\tcv_agg's auc: 0.810369\tcv_agg's rmse: 0.363379\n",
      "[257]\tcv_agg's auc: 0.810402\tcv_agg's rmse: 0.363362\n",
      "[258]\tcv_agg's auc: 0.810411\tcv_agg's rmse: 0.363348\n",
      "[259]\tcv_agg's auc: 0.81056\tcv_agg's rmse: 0.363286\n",
      "[260]\tcv_agg's auc: 0.810681\tcv_agg's rmse: 0.363239\n",
      "[261]\tcv_agg's auc: 0.810794\tcv_agg's rmse: 0.36319\n",
      "[262]\tcv_agg's auc: 0.810908\tcv_agg's rmse: 0.363147\n",
      "[263]\tcv_agg's auc: 0.810959\tcv_agg's rmse: 0.363122\n",
      "[264]\tcv_agg's auc: 0.81105\tcv_agg's rmse: 0.363091\n",
      "[265]\tcv_agg's auc: 0.811152\tcv_agg's rmse: 0.363047\n",
      "[266]\tcv_agg's auc: 0.8112\tcv_agg's rmse: 0.363022\n",
      "[267]\tcv_agg's auc: 0.811218\tcv_agg's rmse: 0.363005\n",
      "[268]\tcv_agg's auc: 0.811263\tcv_agg's rmse: 0.362983\n",
      "[269]\tcv_agg's auc: 0.811328\tcv_agg's rmse: 0.36296\n",
      "[270]\tcv_agg's auc: 0.811507\tcv_agg's rmse: 0.362879\n",
      "[271]\tcv_agg's auc: 0.8117\tcv_agg's rmse: 0.3628\n",
      "[272]\tcv_agg's auc: 0.811727\tcv_agg's rmse: 0.36279\n",
      "[273]\tcv_agg's auc: 0.811817\tcv_agg's rmse: 0.362745\n",
      "[274]\tcv_agg's auc: 0.811864\tcv_agg's rmse: 0.362724\n",
      "[275]\tcv_agg's auc: 0.811917\tcv_agg's rmse: 0.362703\n",
      "[276]\tcv_agg's auc: 0.811983\tcv_agg's rmse: 0.362674\n",
      "[277]\tcv_agg's auc: 0.812034\tcv_agg's rmse: 0.362655\n",
      "[278]\tcv_agg's auc: 0.812093\tcv_agg's rmse: 0.362628\n",
      "[279]\tcv_agg's auc: 0.812105\tcv_agg's rmse: 0.362615\n",
      "[280]\tcv_agg's auc: 0.81216\tcv_agg's rmse: 0.362589\n",
      "[281]\tcv_agg's auc: 0.812217\tcv_agg's rmse: 0.362568\n",
      "[282]\tcv_agg's auc: 0.812198\tcv_agg's rmse: 0.362567\n",
      "[283]\tcv_agg's auc: 0.812226\tcv_agg's rmse: 0.362553\n",
      "[284]\tcv_agg's auc: 0.812212\tcv_agg's rmse: 0.362554\n",
      "[285]\tcv_agg's auc: 0.812246\tcv_agg's rmse: 0.362527\n",
      "[286]\tcv_agg's auc: 0.812456\tcv_agg's rmse: 0.362436\n",
      "[287]\tcv_agg's auc: 0.812466\tcv_agg's rmse: 0.362433\n",
      "[288]\tcv_agg's auc: 0.8125\tcv_agg's rmse: 0.362408\n",
      "[289]\tcv_agg's auc: 0.812552\tcv_agg's rmse: 0.362383\n",
      "[290]\tcv_agg's auc: 0.812547\tcv_agg's rmse: 0.362382\n",
      "[291]\tcv_agg's auc: 0.812576\tcv_agg's rmse: 0.362367\n",
      "[292]\tcv_agg's auc: 0.812619\tcv_agg's rmse: 0.362356\n",
      "[293]\tcv_agg's auc: 0.812621\tcv_agg's rmse: 0.362354\n",
      "[294]\tcv_agg's auc: 0.812631\tcv_agg's rmse: 0.362344\n",
      "[295]\tcv_agg's auc: 0.812725\tcv_agg's rmse: 0.362311\n",
      "[296]\tcv_agg's auc: 0.812754\tcv_agg's rmse: 0.362296\n",
      "[297]\tcv_agg's auc: 0.812819\tcv_agg's rmse: 0.362268\n",
      "[298]\tcv_agg's auc: 0.812864\tcv_agg's rmse: 0.362249\n",
      "[299]\tcv_agg's auc: 0.812883\tcv_agg's rmse: 0.362231\n",
      "[300]\tcv_agg's auc: 0.812937\tcv_agg's rmse: 0.362215\n",
      "[301]\tcv_agg's auc: 0.813003\tcv_agg's rmse: 0.362191\n",
      "[302]\tcv_agg's auc: 0.813037\tcv_agg's rmse: 0.362169\n",
      "[303]\tcv_agg's auc: 0.813104\tcv_agg's rmse: 0.362135\n",
      "[304]\tcv_agg's auc: 0.813176\tcv_agg's rmse: 0.362103\n",
      "[305]\tcv_agg's auc: 0.813194\tcv_agg's rmse: 0.362089\n",
      "[306]\tcv_agg's auc: 0.813182\tcv_agg's rmse: 0.362089\n",
      "[307]\tcv_agg's auc: 0.813225\tcv_agg's rmse: 0.362065\n",
      "[308]\tcv_agg's auc: 0.813226\tcv_agg's rmse: 0.362062\n",
      "[309]\tcv_agg's auc: 0.813252\tcv_agg's rmse: 0.362048\n",
      "[310]\tcv_agg's auc: 0.813272\tcv_agg's rmse: 0.362032\n",
      "[311]\tcv_agg's auc: 0.81328\tcv_agg's rmse: 0.36203\n",
      "[312]\tcv_agg's auc: 0.813277\tcv_agg's rmse: 0.362024\n",
      "[313]\tcv_agg's auc: 0.813336\tcv_agg's rmse: 0.361982\n",
      "[314]\tcv_agg's auc: 0.813349\tcv_agg's rmse: 0.361971\n",
      "[315]\tcv_agg's auc: 0.8134\tcv_agg's rmse: 0.361947\n",
      "[316]\tcv_agg's auc: 0.813401\tcv_agg's rmse: 0.361946\n",
      "[317]\tcv_agg's auc: 0.813413\tcv_agg's rmse: 0.361937\n",
      "[318]\tcv_agg's auc: 0.813404\tcv_agg's rmse: 0.361934\n",
      "[319]\tcv_agg's auc: 0.813487\tcv_agg's rmse: 0.361912\n",
      "[320]\tcv_agg's auc: 0.813569\tcv_agg's rmse: 0.361865\n",
      "[321]\tcv_agg's auc: 0.813651\tcv_agg's rmse: 0.361831\n",
      "[322]\tcv_agg's auc: 0.81369\tcv_agg's rmse: 0.361817\n",
      "[323]\tcv_agg's auc: 0.813776\tcv_agg's rmse: 0.361783\n",
      "[324]\tcv_agg's auc: 0.81379\tcv_agg's rmse: 0.361774\n",
      "[325]\tcv_agg's auc: 0.813827\tcv_agg's rmse: 0.361753\n",
      "[326]\tcv_agg's auc: 0.813864\tcv_agg's rmse: 0.361734\n",
      "[327]\tcv_agg's auc: 0.813916\tcv_agg's rmse: 0.361711\n",
      "[328]\tcv_agg's auc: 0.813933\tcv_agg's rmse: 0.361705\n",
      "[329]\tcv_agg's auc: 0.814016\tcv_agg's rmse: 0.361679\n",
      "[330]\tcv_agg's auc: 0.814062\tcv_agg's rmse: 0.361654\n",
      "[331]\tcv_agg's auc: 0.81419\tcv_agg's rmse: 0.361597\n",
      "[332]\tcv_agg's auc: 0.814254\tcv_agg's rmse: 0.361562\n",
      "[333]\tcv_agg's auc: 0.814307\tcv_agg's rmse: 0.36154\n",
      "[334]\tcv_agg's auc: 0.814346\tcv_agg's rmse: 0.361525\n",
      "[335]\tcv_agg's auc: 0.814363\tcv_agg's rmse: 0.361511\n",
      "[336]\tcv_agg's auc: 0.814458\tcv_agg's rmse: 0.361476\n",
      "[337]\tcv_agg's auc: 0.814511\tcv_agg's rmse: 0.361459\n",
      "[338]\tcv_agg's auc: 0.814574\tcv_agg's rmse: 0.361448\n",
      "[339]\tcv_agg's auc: 0.814575\tcv_agg's rmse: 0.361444\n",
      "[340]\tcv_agg's auc: 0.81462\tcv_agg's rmse: 0.361418\n",
      "[341]\tcv_agg's auc: 0.814667\tcv_agg's rmse: 0.361396\n",
      "[342]\tcv_agg's auc: 0.814695\tcv_agg's rmse: 0.361377\n",
      "[343]\tcv_agg's auc: 0.814722\tcv_agg's rmse: 0.361354\n",
      "[344]\tcv_agg's auc: 0.814753\tcv_agg's rmse: 0.361334\n",
      "[345]\tcv_agg's auc: 0.814753\tcv_agg's rmse: 0.361332\n",
      "[346]\tcv_agg's auc: 0.814782\tcv_agg's rmse: 0.361316\n",
      "[347]\tcv_agg's auc: 0.814788\tcv_agg's rmse: 0.361299\n",
      "[348]\tcv_agg's auc: 0.814864\tcv_agg's rmse: 0.361262\n",
      "[349]\tcv_agg's auc: 0.814855\tcv_agg's rmse: 0.361261\n",
      "[350]\tcv_agg's auc: 0.814871\tcv_agg's rmse: 0.361247\n",
      "[351]\tcv_agg's auc: 0.814874\tcv_agg's rmse: 0.361246\n",
      "[352]\tcv_agg's auc: 0.814864\tcv_agg's rmse: 0.361241\n",
      "[353]\tcv_agg's auc: 0.814872\tcv_agg's rmse: 0.361224\n",
      "[354]\tcv_agg's auc: 0.814904\tcv_agg's rmse: 0.361215\n",
      "[355]\tcv_agg's auc: 0.814963\tcv_agg's rmse: 0.361191\n",
      "[356]\tcv_agg's auc: 0.814972\tcv_agg's rmse: 0.361181\n",
      "[357]\tcv_agg's auc: 0.815001\tcv_agg's rmse: 0.361169\n",
      "[358]\tcv_agg's auc: 0.815019\tcv_agg's rmse: 0.361157\n",
      "[359]\tcv_agg's auc: 0.815123\tcv_agg's rmse: 0.361118\n",
      "[360]\tcv_agg's auc: 0.815128\tcv_agg's rmse: 0.361115\n",
      "[361]\tcv_agg's auc: 0.815149\tcv_agg's rmse: 0.361108\n",
      "[362]\tcv_agg's auc: 0.815248\tcv_agg's rmse: 0.361068\n",
      "[363]\tcv_agg's auc: 0.815244\tcv_agg's rmse: 0.361069\n",
      "[364]\tcv_agg's auc: 0.815267\tcv_agg's rmse: 0.361052\n",
      "[365]\tcv_agg's auc: 0.815304\tcv_agg's rmse: 0.361034\n",
      "[366]\tcv_agg's auc: 0.815454\tcv_agg's rmse: 0.360985\n",
      "[367]\tcv_agg's auc: 0.815473\tcv_agg's rmse: 0.360969\n",
      "[368]\tcv_agg's auc: 0.815546\tcv_agg's rmse: 0.360942\n",
      "[369]\tcv_agg's auc: 0.815564\tcv_agg's rmse: 0.360933\n",
      "[370]\tcv_agg's auc: 0.815685\tcv_agg's rmse: 0.360877\n",
      "[371]\tcv_agg's auc: 0.815784\tcv_agg's rmse: 0.360829\n",
      "[372]\tcv_agg's auc: 0.815792\tcv_agg's rmse: 0.360815\n",
      "[373]\tcv_agg's auc: 0.815803\tcv_agg's rmse: 0.360812\n",
      "[374]\tcv_agg's auc: 0.815818\tcv_agg's rmse: 0.360806\n",
      "[375]\tcv_agg's auc: 0.81585\tcv_agg's rmse: 0.360797\n",
      "[376]\tcv_agg's auc: 0.815836\tcv_agg's rmse: 0.360788\n",
      "[377]\tcv_agg's auc: 0.815829\tcv_agg's rmse: 0.360785\n",
      "[378]\tcv_agg's auc: 0.815832\tcv_agg's rmse: 0.360783\n",
      "[379]\tcv_agg's auc: 0.815874\tcv_agg's rmse: 0.360756\n",
      "[380]\tcv_agg's auc: 0.815883\tcv_agg's rmse: 0.360751\n",
      "[381]\tcv_agg's auc: 0.815889\tcv_agg's rmse: 0.360739\n",
      "[382]\tcv_agg's auc: 0.815925\tcv_agg's rmse: 0.360723\n",
      "[383]\tcv_agg's auc: 0.815939\tcv_agg's rmse: 0.360715\n",
      "[384]\tcv_agg's auc: 0.815928\tcv_agg's rmse: 0.360717\n",
      "[385]\tcv_agg's auc: 0.815967\tcv_agg's rmse: 0.360701\n",
      "[386]\tcv_agg's auc: 0.815981\tcv_agg's rmse: 0.360682\n",
      "[387]\tcv_agg's auc: 0.816005\tcv_agg's rmse: 0.360666\n",
      "[388]\tcv_agg's auc: 0.81608\tcv_agg's rmse: 0.360641\n",
      "[389]\tcv_agg's auc: 0.816134\tcv_agg's rmse: 0.360624\n",
      "[390]\tcv_agg's auc: 0.816135\tcv_agg's rmse: 0.36062\n",
      "[391]\tcv_agg's auc: 0.816195\tcv_agg's rmse: 0.360592\n",
      "[392]\tcv_agg's auc: 0.816223\tcv_agg's rmse: 0.360577\n",
      "[393]\tcv_agg's auc: 0.816245\tcv_agg's rmse: 0.360567\n",
      "[394]\tcv_agg's auc: 0.816286\tcv_agg's rmse: 0.360552\n",
      "[395]\tcv_agg's auc: 0.816286\tcv_agg's rmse: 0.360555\n",
      "[396]\tcv_agg's auc: 0.816274\tcv_agg's rmse: 0.360554\n",
      "[397]\tcv_agg's auc: 0.816314\tcv_agg's rmse: 0.360544\n",
      "[398]\tcv_agg's auc: 0.816342\tcv_agg's rmse: 0.360531\n",
      "[399]\tcv_agg's auc: 0.816535\tcv_agg's rmse: 0.360473\n",
      "[400]\tcv_agg's auc: 0.816628\tcv_agg's rmse: 0.360438\n",
      "[401]\tcv_agg's auc: 0.816668\tcv_agg's rmse: 0.360425\n",
      "[402]\tcv_agg's auc: 0.816714\tcv_agg's rmse: 0.360401\n",
      "[403]\tcv_agg's auc: 0.816804\tcv_agg's rmse: 0.360373\n",
      "[404]\tcv_agg's auc: 0.816805\tcv_agg's rmse: 0.360369\n",
      "[405]\tcv_agg's auc: 0.816831\tcv_agg's rmse: 0.360354\n",
      "[406]\tcv_agg's auc: 0.816825\tcv_agg's rmse: 0.360357\n",
      "[407]\tcv_agg's auc: 0.816894\tcv_agg's rmse: 0.360333\n",
      "[408]\tcv_agg's auc: 0.816934\tcv_agg's rmse: 0.36032\n",
      "[409]\tcv_agg's auc: 0.816969\tcv_agg's rmse: 0.360303\n",
      "[410]\tcv_agg's auc: 0.81697\tcv_agg's rmse: 0.360302\n",
      "[411]\tcv_agg's auc: 0.816979\tcv_agg's rmse: 0.360291\n",
      "[412]\tcv_agg's auc: 0.81702\tcv_agg's rmse: 0.360273\n",
      "[413]\tcv_agg's auc: 0.817132\tcv_agg's rmse: 0.360218\n",
      "[414]\tcv_agg's auc: 0.81715\tcv_agg's rmse: 0.360205\n",
      "[415]\tcv_agg's auc: 0.817146\tcv_agg's rmse: 0.360202\n",
      "[416]\tcv_agg's auc: 0.817198\tcv_agg's rmse: 0.360183\n",
      "[417]\tcv_agg's auc: 0.817229\tcv_agg's rmse: 0.360174\n",
      "[418]\tcv_agg's auc: 0.817311\tcv_agg's rmse: 0.36015\n",
      "[419]\tcv_agg's auc: 0.817323\tcv_agg's rmse: 0.360145\n",
      "[420]\tcv_agg's auc: 0.81734\tcv_agg's rmse: 0.360138\n",
      "[421]\tcv_agg's auc: 0.817342\tcv_agg's rmse: 0.360134\n",
      "[422]\tcv_agg's auc: 0.817359\tcv_agg's rmse: 0.360128\n",
      "[423]\tcv_agg's auc: 0.817354\tcv_agg's rmse: 0.360124\n",
      "[424]\tcv_agg's auc: 0.817426\tcv_agg's rmse: 0.360101\n",
      "[425]\tcv_agg's auc: 0.817449\tcv_agg's rmse: 0.360088\n",
      "[426]\tcv_agg's auc: 0.817485\tcv_agg's rmse: 0.360069\n",
      "[427]\tcv_agg's auc: 0.817578\tcv_agg's rmse: 0.360032\n",
      "[428]\tcv_agg's auc: 0.817641\tcv_agg's rmse: 0.360011\n",
      "[429]\tcv_agg's auc: 0.817657\tcv_agg's rmse: 0.360006\n",
      "[430]\tcv_agg's auc: 0.817677\tcv_agg's rmse: 0.359997\n",
      "[431]\tcv_agg's auc: 0.817689\tcv_agg's rmse: 0.35999\n",
      "[432]\tcv_agg's auc: 0.817699\tcv_agg's rmse: 0.35998\n",
      "[433]\tcv_agg's auc: 0.817718\tcv_agg's rmse: 0.359971\n",
      "[434]\tcv_agg's auc: 0.817752\tcv_agg's rmse: 0.359955\n",
      "[435]\tcv_agg's auc: 0.817784\tcv_agg's rmse: 0.359942\n",
      "[436]\tcv_agg's auc: 0.817821\tcv_agg's rmse: 0.35993\n",
      "[437]\tcv_agg's auc: 0.817863\tcv_agg's rmse: 0.359909\n",
      "[438]\tcv_agg's auc: 0.817862\tcv_agg's rmse: 0.359902\n",
      "[439]\tcv_agg's auc: 0.817861\tcv_agg's rmse: 0.359895\n",
      "[440]\tcv_agg's auc: 0.817953\tcv_agg's rmse: 0.359866\n",
      "[441]\tcv_agg's auc: 0.817957\tcv_agg's rmse: 0.359856\n",
      "[442]\tcv_agg's auc: 0.817953\tcv_agg's rmse: 0.359848\n",
      "[443]\tcv_agg's auc: 0.817956\tcv_agg's rmse: 0.359846\n",
      "[444]\tcv_agg's auc: 0.818023\tcv_agg's rmse: 0.359821\n",
      "[445]\tcv_agg's auc: 0.818046\tcv_agg's rmse: 0.359814\n",
      "[446]\tcv_agg's auc: 0.818068\tcv_agg's rmse: 0.359807\n",
      "[447]\tcv_agg's auc: 0.818104\tcv_agg's rmse: 0.359796\n",
      "[448]\tcv_agg's auc: 0.818136\tcv_agg's rmse: 0.359774\n",
      "[449]\tcv_agg's auc: 0.818177\tcv_agg's rmse: 0.359757\n",
      "[450]\tcv_agg's auc: 0.818175\tcv_agg's rmse: 0.359752\n",
      "[451]\tcv_agg's auc: 0.818192\tcv_agg's rmse: 0.359739\n",
      "[452]\tcv_agg's auc: 0.818225\tcv_agg's rmse: 0.359727\n",
      "[453]\tcv_agg's auc: 0.81832\tcv_agg's rmse: 0.359694\n",
      "[454]\tcv_agg's auc: 0.818359\tcv_agg's rmse: 0.359675\n",
      "[455]\tcv_agg's auc: 0.818402\tcv_agg's rmse: 0.359656\n",
      "[456]\tcv_agg's auc: 0.818405\tcv_agg's rmse: 0.359651\n",
      "[457]\tcv_agg's auc: 0.818413\tcv_agg's rmse: 0.359649\n",
      "[458]\tcv_agg's auc: 0.818433\tcv_agg's rmse: 0.359638\n",
      "[459]\tcv_agg's auc: 0.818489\tcv_agg's rmse: 0.359616\n",
      "[460]\tcv_agg's auc: 0.818488\tcv_agg's rmse: 0.359617\n",
      "[461]\tcv_agg's auc: 0.818535\tcv_agg's rmse: 0.3596\n",
      "[462]\tcv_agg's auc: 0.81854\tcv_agg's rmse: 0.3596\n",
      "[463]\tcv_agg's auc: 0.818541\tcv_agg's rmse: 0.359592\n",
      "[464]\tcv_agg's auc: 0.818589\tcv_agg's rmse: 0.359575\n",
      "[465]\tcv_agg's auc: 0.818611\tcv_agg's rmse: 0.359566\n",
      "[466]\tcv_agg's auc: 0.818713\tcv_agg's rmse: 0.359525\n",
      "[467]\tcv_agg's auc: 0.818717\tcv_agg's rmse: 0.35952\n",
      "[468]\tcv_agg's auc: 0.818704\tcv_agg's rmse: 0.359523\n",
      "[469]\tcv_agg's auc: 0.81876\tcv_agg's rmse: 0.359504\n",
      "[470]\tcv_agg's auc: 0.818781\tcv_agg's rmse: 0.359492\n",
      "[471]\tcv_agg's auc: 0.818829\tcv_agg's rmse: 0.359472\n",
      "[472]\tcv_agg's auc: 0.818816\tcv_agg's rmse: 0.35948\n",
      "[473]\tcv_agg's auc: 0.818857\tcv_agg's rmse: 0.359463\n",
      "[474]\tcv_agg's auc: 0.818944\tcv_agg's rmse: 0.359433\n",
      "[475]\tcv_agg's auc: 0.818974\tcv_agg's rmse: 0.35941\n",
      "[476]\tcv_agg's auc: 0.819065\tcv_agg's rmse: 0.35937\n",
      "[477]\tcv_agg's auc: 0.819077\tcv_agg's rmse: 0.359365\n",
      "[478]\tcv_agg's auc: 0.819138\tcv_agg's rmse: 0.359342\n",
      "[479]\tcv_agg's auc: 0.819186\tcv_agg's rmse: 0.359318\n",
      "[480]\tcv_agg's auc: 0.81919\tcv_agg's rmse: 0.359317\n",
      "[481]\tcv_agg's auc: 0.819221\tcv_agg's rmse: 0.359299\n",
      "[482]\tcv_agg's auc: 0.81924\tcv_agg's rmse: 0.359293\n",
      "[483]\tcv_agg's auc: 0.819227\tcv_agg's rmse: 0.359294\n",
      "[484]\tcv_agg's auc: 0.819291\tcv_agg's rmse: 0.359267\n",
      "[485]\tcv_agg's auc: 0.819319\tcv_agg's rmse: 0.359259\n",
      "[486]\tcv_agg's auc: 0.819325\tcv_agg's rmse: 0.35925\n",
      "[487]\tcv_agg's auc: 0.819342\tcv_agg's rmse: 0.359238\n",
      "[488]\tcv_agg's auc: 0.819329\tcv_agg's rmse: 0.359239\n",
      "[489]\tcv_agg's auc: 0.819362\tcv_agg's rmse: 0.359222\n",
      "[490]\tcv_agg's auc: 0.819388\tcv_agg's rmse: 0.359211\n",
      "[491]\tcv_agg's auc: 0.819483\tcv_agg's rmse: 0.359168\n",
      "[492]\tcv_agg's auc: 0.819522\tcv_agg's rmse: 0.359149\n",
      "[493]\tcv_agg's auc: 0.819559\tcv_agg's rmse: 0.359141\n",
      "[494]\tcv_agg's auc: 0.819562\tcv_agg's rmse: 0.359141\n",
      "[495]\tcv_agg's auc: 0.819586\tcv_agg's rmse: 0.35913\n",
      "[496]\tcv_agg's auc: 0.819598\tcv_agg's rmse: 0.359126\n",
      "[497]\tcv_agg's auc: 0.819601\tcv_agg's rmse: 0.359118\n",
      "[498]\tcv_agg's auc: 0.819608\tcv_agg's rmse: 0.359116\n",
      "[499]\tcv_agg's auc: 0.819603\tcv_agg's rmse: 0.359113\n",
      "[500]\tcv_agg's auc: 0.819622\tcv_agg's rmse: 0.359098\n",
      "[501]\tcv_agg's auc: 0.81961\tcv_agg's rmse: 0.359096\n",
      "[502]\tcv_agg's auc: 0.819615\tcv_agg's rmse: 0.35909\n",
      "[503]\tcv_agg's auc: 0.819649\tcv_agg's rmse: 0.35908\n",
      "[504]\tcv_agg's auc: 0.819657\tcv_agg's rmse: 0.359077\n",
      "[505]\tcv_agg's auc: 0.819708\tcv_agg's rmse: 0.359059\n",
      "[506]\tcv_agg's auc: 0.819759\tcv_agg's rmse: 0.359044\n",
      "[507]\tcv_agg's auc: 0.819821\tcv_agg's rmse: 0.359003\n",
      "[508]\tcv_agg's auc: 0.819825\tcv_agg's rmse: 0.358996\n",
      "[509]\tcv_agg's auc: 0.819843\tcv_agg's rmse: 0.358983\n",
      "[510]\tcv_agg's auc: 0.819867\tcv_agg's rmse: 0.358971\n",
      "[511]\tcv_agg's auc: 0.819956\tcv_agg's rmse: 0.358936\n",
      "[512]\tcv_agg's auc: 0.819951\tcv_agg's rmse: 0.358934\n",
      "[513]\tcv_agg's auc: 0.819997\tcv_agg's rmse: 0.358916\n",
      "[514]\tcv_agg's auc: 0.820009\tcv_agg's rmse: 0.358908\n",
      "[515]\tcv_agg's auc: 0.820096\tcv_agg's rmse: 0.358876\n",
      "[516]\tcv_agg's auc: 0.820106\tcv_agg's rmse: 0.358872\n",
      "[517]\tcv_agg's auc: 0.820122\tcv_agg's rmse: 0.358862\n",
      "[518]\tcv_agg's auc: 0.820212\tcv_agg's rmse: 0.358832\n",
      "[519]\tcv_agg's auc: 0.82023\tcv_agg's rmse: 0.358829\n",
      "[520]\tcv_agg's auc: 0.820233\tcv_agg's rmse: 0.358825\n",
      "[521]\tcv_agg's auc: 0.820277\tcv_agg's rmse: 0.358806\n",
      "[522]\tcv_agg's auc: 0.820295\tcv_agg's rmse: 0.358796\n",
      "[523]\tcv_agg's auc: 0.820391\tcv_agg's rmse: 0.358767\n",
      "[524]\tcv_agg's auc: 0.820469\tcv_agg's rmse: 0.358744\n",
      "[525]\tcv_agg's auc: 0.820534\tcv_agg's rmse: 0.358719\n",
      "[526]\tcv_agg's auc: 0.820541\tcv_agg's rmse: 0.358711\n",
      "[527]\tcv_agg's auc: 0.820565\tcv_agg's rmse: 0.3587\n",
      "[528]\tcv_agg's auc: 0.820595\tcv_agg's rmse: 0.358693\n",
      "[529]\tcv_agg's auc: 0.820652\tcv_agg's rmse: 0.358667\n",
      "[530]\tcv_agg's auc: 0.820719\tcv_agg's rmse: 0.358633\n",
      "[531]\tcv_agg's auc: 0.820724\tcv_agg's rmse: 0.358629\n",
      "[532]\tcv_agg's auc: 0.820756\tcv_agg's rmse: 0.358614\n",
      "[533]\tcv_agg's auc: 0.820765\tcv_agg's rmse: 0.35861\n",
      "[534]\tcv_agg's auc: 0.820857\tcv_agg's rmse: 0.358573\n",
      "[535]\tcv_agg's auc: 0.82088\tcv_agg's rmse: 0.358563\n",
      "[536]\tcv_agg's auc: 0.820901\tcv_agg's rmse: 0.358559\n",
      "[537]\tcv_agg's auc: 0.820931\tcv_agg's rmse: 0.358541\n",
      "[538]\tcv_agg's auc: 0.820952\tcv_agg's rmse: 0.358535\n",
      "[539]\tcv_agg's auc: 0.820978\tcv_agg's rmse: 0.358527\n",
      "[540]\tcv_agg's auc: 0.82101\tcv_agg's rmse: 0.358511\n",
      "[541]\tcv_agg's auc: 0.820994\tcv_agg's rmse: 0.358515\n",
      "[542]\tcv_agg's auc: 0.821014\tcv_agg's rmse: 0.358509\n",
      "[543]\tcv_agg's auc: 0.821015\tcv_agg's rmse: 0.358507\n",
      "[544]\tcv_agg's auc: 0.821028\tcv_agg's rmse: 0.358502\n",
      "[545]\tcv_agg's auc: 0.821034\tcv_agg's rmse: 0.358495\n",
      "[546]\tcv_agg's auc: 0.821015\tcv_agg's rmse: 0.358498\n",
      "[547]\tcv_agg's auc: 0.821031\tcv_agg's rmse: 0.35849\n",
      "[548]\tcv_agg's auc: 0.821099\tcv_agg's rmse: 0.358457\n",
      "[549]\tcv_agg's auc: 0.821116\tcv_agg's rmse: 0.35845\n",
      "[550]\tcv_agg's auc: 0.821118\tcv_agg's rmse: 0.358448\n",
      "[551]\tcv_agg's auc: 0.821133\tcv_agg's rmse: 0.358443\n",
      "[552]\tcv_agg's auc: 0.82115\tcv_agg's rmse: 0.358437\n",
      "[553]\tcv_agg's auc: 0.821176\tcv_agg's rmse: 0.358427\n",
      "[554]\tcv_agg's auc: 0.821205\tcv_agg's rmse: 0.358418\n",
      "[555]\tcv_agg's auc: 0.821183\tcv_agg's rmse: 0.35842\n",
      "[556]\tcv_agg's auc: 0.821203\tcv_agg's rmse: 0.358409\n",
      "[557]\tcv_agg's auc: 0.821216\tcv_agg's rmse: 0.358404\n",
      "[558]\tcv_agg's auc: 0.821212\tcv_agg's rmse: 0.358399\n",
      "[559]\tcv_agg's auc: 0.821223\tcv_agg's rmse: 0.358393\n",
      "[560]\tcv_agg's auc: 0.821324\tcv_agg's rmse: 0.358354\n",
      "[561]\tcv_agg's auc: 0.821377\tcv_agg's rmse: 0.35833\n",
      "[562]\tcv_agg's auc: 0.821367\tcv_agg's rmse: 0.358336\n",
      "[563]\tcv_agg's auc: 0.821378\tcv_agg's rmse: 0.358334\n",
      "[564]\tcv_agg's auc: 0.821369\tcv_agg's rmse: 0.358339\n",
      "[565]\tcv_agg's auc: 0.821361\tcv_agg's rmse: 0.358339\n",
      "[566]\tcv_agg's auc: 0.821408\tcv_agg's rmse: 0.358324\n",
      "[567]\tcv_agg's auc: 0.821428\tcv_agg's rmse: 0.358319\n",
      "[568]\tcv_agg's auc: 0.82146\tcv_agg's rmse: 0.35831\n",
      "[569]\tcv_agg's auc: 0.821536\tcv_agg's rmse: 0.35828\n",
      "[570]\tcv_agg's auc: 0.821537\tcv_agg's rmse: 0.358274\n",
      "[571]\tcv_agg's auc: 0.821611\tcv_agg's rmse: 0.35824\n",
      "[572]\tcv_agg's auc: 0.821643\tcv_agg's rmse: 0.358223\n",
      "[573]\tcv_agg's auc: 0.821653\tcv_agg's rmse: 0.358218\n",
      "[574]\tcv_agg's auc: 0.821722\tcv_agg's rmse: 0.358183\n",
      "[575]\tcv_agg's auc: 0.821733\tcv_agg's rmse: 0.358174\n",
      "[576]\tcv_agg's auc: 0.821769\tcv_agg's rmse: 0.358159\n",
      "[577]\tcv_agg's auc: 0.821775\tcv_agg's rmse: 0.358155\n",
      "[578]\tcv_agg's auc: 0.82179\tcv_agg's rmse: 0.358148\n",
      "[579]\tcv_agg's auc: 0.821817\tcv_agg's rmse: 0.358137\n",
      "[580]\tcv_agg's auc: 0.821852\tcv_agg's rmse: 0.358118\n",
      "[581]\tcv_agg's auc: 0.821934\tcv_agg's rmse: 0.358086\n",
      "[582]\tcv_agg's auc: 0.821945\tcv_agg's rmse: 0.358076\n",
      "[583]\tcv_agg's auc: 0.821958\tcv_agg's rmse: 0.358066\n",
      "[584]\tcv_agg's auc: 0.821952\tcv_agg's rmse: 0.358068\n",
      "[585]\tcv_agg's auc: 0.821957\tcv_agg's rmse: 0.358065\n",
      "[586]\tcv_agg's auc: 0.821947\tcv_agg's rmse: 0.358067\n",
      "[587]\tcv_agg's auc: 0.821935\tcv_agg's rmse: 0.35807\n",
      "[588]\tcv_agg's auc: 0.821987\tcv_agg's rmse: 0.358053\n",
      "[589]\tcv_agg's auc: 0.822014\tcv_agg's rmse: 0.358045\n",
      "[590]\tcv_agg's auc: 0.822009\tcv_agg's rmse: 0.358044\n",
      "[591]\tcv_agg's auc: 0.821983\tcv_agg's rmse: 0.358049\n",
      "[592]\tcv_agg's auc: 0.82203\tcv_agg's rmse: 0.358026\n",
      "[593]\tcv_agg's auc: 0.822055\tcv_agg's rmse: 0.358019\n",
      "[594]\tcv_agg's auc: 0.822068\tcv_agg's rmse: 0.35801\n",
      "[595]\tcv_agg's auc: 0.822066\tcv_agg's rmse: 0.358004\n",
      "[596]\tcv_agg's auc: 0.822111\tcv_agg's rmse: 0.357984\n",
      "[597]\tcv_agg's auc: 0.822109\tcv_agg's rmse: 0.357984\n",
      "[598]\tcv_agg's auc: 0.822139\tcv_agg's rmse: 0.357972\n",
      "[599]\tcv_agg's auc: 0.822127\tcv_agg's rmse: 0.357975\n",
      "[600]\tcv_agg's auc: 0.822155\tcv_agg's rmse: 0.357963\n",
      "[601]\tcv_agg's auc: 0.822139\tcv_agg's rmse: 0.357967\n",
      "[602]\tcv_agg's auc: 0.822123\tcv_agg's rmse: 0.357968\n",
      "[603]\tcv_agg's auc: 0.822205\tcv_agg's rmse: 0.357927\n",
      "[604]\tcv_agg's auc: 0.822215\tcv_agg's rmse: 0.357922\n",
      "[605]\tcv_agg's auc: 0.822265\tcv_agg's rmse: 0.357907\n",
      "[606]\tcv_agg's auc: 0.822249\tcv_agg's rmse: 0.357907\n",
      "[607]\tcv_agg's auc: 0.822321\tcv_agg's rmse: 0.357888\n",
      "[608]\tcv_agg's auc: 0.822296\tcv_agg's rmse: 0.35789\n",
      "[609]\tcv_agg's auc: 0.822382\tcv_agg's rmse: 0.357852\n",
      "[610]\tcv_agg's auc: 0.822381\tcv_agg's rmse: 0.357845\n",
      "[611]\tcv_agg's auc: 0.82246\tcv_agg's rmse: 0.357811\n",
      "[612]\tcv_agg's auc: 0.8225\tcv_agg's rmse: 0.357791\n",
      "[613]\tcv_agg's auc: 0.822493\tcv_agg's rmse: 0.357791\n",
      "[614]\tcv_agg's auc: 0.822519\tcv_agg's rmse: 0.357776\n",
      "[615]\tcv_agg's auc: 0.822569\tcv_agg's rmse: 0.357756\n",
      "[616]\tcv_agg's auc: 0.822634\tcv_agg's rmse: 0.357728\n",
      "[617]\tcv_agg's auc: 0.822633\tcv_agg's rmse: 0.357728\n",
      "[618]\tcv_agg's auc: 0.822621\tcv_agg's rmse: 0.357731\n",
      "[619]\tcv_agg's auc: 0.822626\tcv_agg's rmse: 0.357727\n",
      "[620]\tcv_agg's auc: 0.822691\tcv_agg's rmse: 0.357705\n",
      "[621]\tcv_agg's auc: 0.822719\tcv_agg's rmse: 0.357694\n",
      "[622]\tcv_agg's auc: 0.822695\tcv_agg's rmse: 0.357696\n",
      "[623]\tcv_agg's auc: 0.822719\tcv_agg's rmse: 0.357683\n",
      "[624]\tcv_agg's auc: 0.822753\tcv_agg's rmse: 0.357673\n",
      "[625]\tcv_agg's auc: 0.822767\tcv_agg's rmse: 0.357666\n",
      "[626]\tcv_agg's auc: 0.82278\tcv_agg's rmse: 0.357658\n",
      "[627]\tcv_agg's auc: 0.82278\tcv_agg's rmse: 0.357655\n",
      "[628]\tcv_agg's auc: 0.822848\tcv_agg's rmse: 0.357628\n",
      "[629]\tcv_agg's auc: 0.822829\tcv_agg's rmse: 0.357631\n",
      "[630]\tcv_agg's auc: 0.822884\tcv_agg's rmse: 0.357612\n",
      "[631]\tcv_agg's auc: 0.8229\tcv_agg's rmse: 0.357599\n",
      "[632]\tcv_agg's auc: 0.8229\tcv_agg's rmse: 0.357599\n",
      "[633]\tcv_agg's auc: 0.822921\tcv_agg's rmse: 0.357589\n",
      "[634]\tcv_agg's auc: 0.823032\tcv_agg's rmse: 0.357544\n",
      "[635]\tcv_agg's auc: 0.823034\tcv_agg's rmse: 0.357542\n",
      "[636]\tcv_agg's auc: 0.823007\tcv_agg's rmse: 0.357546\n",
      "[637]\tcv_agg's auc: 0.823083\tcv_agg's rmse: 0.357518\n",
      "[638]\tcv_agg's auc: 0.823106\tcv_agg's rmse: 0.357511\n",
      "[639]\tcv_agg's auc: 0.823123\tcv_agg's rmse: 0.357506\n",
      "[640]\tcv_agg's auc: 0.823137\tcv_agg's rmse: 0.357497\n",
      "[641]\tcv_agg's auc: 0.823108\tcv_agg's rmse: 0.357503\n",
      "[642]\tcv_agg's auc: 0.823151\tcv_agg's rmse: 0.357487\n",
      "[643]\tcv_agg's auc: 0.823173\tcv_agg's rmse: 0.357481\n",
      "[644]\tcv_agg's auc: 0.823206\tcv_agg's rmse: 0.357465\n",
      "[645]\tcv_agg's auc: 0.823185\tcv_agg's rmse: 0.357472\n",
      "[646]\tcv_agg's auc: 0.823191\tcv_agg's rmse: 0.357466\n",
      "[647]\tcv_agg's auc: 0.823202\tcv_agg's rmse: 0.357464\n",
      "[648]\tcv_agg's auc: 0.823201\tcv_agg's rmse: 0.357464\n",
      "[649]\tcv_agg's auc: 0.823202\tcv_agg's rmse: 0.35746\n",
      "[650]\tcv_agg's auc: 0.82322\tcv_agg's rmse: 0.357456\n",
      "[651]\tcv_agg's auc: 0.823326\tcv_agg's rmse: 0.357417\n",
      "[652]\tcv_agg's auc: 0.823304\tcv_agg's rmse: 0.357422\n",
      "[653]\tcv_agg's auc: 0.823305\tcv_agg's rmse: 0.357421\n",
      "[654]\tcv_agg's auc: 0.823328\tcv_agg's rmse: 0.35741\n",
      "[655]\tcv_agg's auc: 0.823308\tcv_agg's rmse: 0.357417\n",
      "[656]\tcv_agg's auc: 0.82334\tcv_agg's rmse: 0.357404\n",
      "[657]\tcv_agg's auc: 0.823335\tcv_agg's rmse: 0.357396\n",
      "[658]\tcv_agg's auc: 0.823341\tcv_agg's rmse: 0.357391\n",
      "[659]\tcv_agg's auc: 0.823372\tcv_agg's rmse: 0.357376\n",
      "[660]\tcv_agg's auc: 0.823355\tcv_agg's rmse: 0.35738\n",
      "[661]\tcv_agg's auc: 0.823365\tcv_agg's rmse: 0.357373\n",
      "[662]\tcv_agg's auc: 0.823386\tcv_agg's rmse: 0.357365\n",
      "[663]\tcv_agg's auc: 0.823402\tcv_agg's rmse: 0.35736\n",
      "[664]\tcv_agg's auc: 0.823414\tcv_agg's rmse: 0.357355\n",
      "[665]\tcv_agg's auc: 0.823419\tcv_agg's rmse: 0.357353\n",
      "[666]\tcv_agg's auc: 0.823404\tcv_agg's rmse: 0.357357\n",
      "[667]\tcv_agg's auc: 0.823458\tcv_agg's rmse: 0.35733\n",
      "[668]\tcv_agg's auc: 0.823462\tcv_agg's rmse: 0.357329\n",
      "[669]\tcv_agg's auc: 0.823501\tcv_agg's rmse: 0.357315\n",
      "[670]\tcv_agg's auc: 0.82351\tcv_agg's rmse: 0.357308\n",
      "[671]\tcv_agg's auc: 0.823565\tcv_agg's rmse: 0.35729\n",
      "[672]\tcv_agg's auc: 0.82357\tcv_agg's rmse: 0.357289\n",
      "[673]\tcv_agg's auc: 0.823601\tcv_agg's rmse: 0.357278\n",
      "[674]\tcv_agg's auc: 0.823583\tcv_agg's rmse: 0.357284\n",
      "[675]\tcv_agg's auc: 0.823662\tcv_agg's rmse: 0.357255\n",
      "[676]\tcv_agg's auc: 0.823649\tcv_agg's rmse: 0.357257\n",
      "[677]\tcv_agg's auc: 0.823662\tcv_agg's rmse: 0.357253\n",
      "[678]\tcv_agg's auc: 0.823662\tcv_agg's rmse: 0.357254\n",
      "[679]\tcv_agg's auc: 0.823702\tcv_agg's rmse: 0.357238\n",
      "[680]\tcv_agg's auc: 0.823729\tcv_agg's rmse: 0.357228\n",
      "[681]\tcv_agg's auc: 0.823753\tcv_agg's rmse: 0.357219\n",
      "[682]\tcv_agg's auc: 0.82376\tcv_agg's rmse: 0.357213\n",
      "[683]\tcv_agg's auc: 0.823748\tcv_agg's rmse: 0.357213\n",
      "[684]\tcv_agg's auc: 0.823763\tcv_agg's rmse: 0.357205\n",
      "[685]\tcv_agg's auc: 0.823795\tcv_agg's rmse: 0.357188\n",
      "[686]\tcv_agg's auc: 0.823839\tcv_agg's rmse: 0.357174\n",
      "[687]\tcv_agg's auc: 0.823847\tcv_agg's rmse: 0.357171\n",
      "[688]\tcv_agg's auc: 0.823951\tcv_agg's rmse: 0.357129\n",
      "[689]\tcv_agg's auc: 0.823964\tcv_agg's rmse: 0.357122\n",
      "[690]\tcv_agg's auc: 0.823955\tcv_agg's rmse: 0.357121\n",
      "[691]\tcv_agg's auc: 0.824007\tcv_agg's rmse: 0.357096\n",
      "[692]\tcv_agg's auc: 0.824005\tcv_agg's rmse: 0.357098\n",
      "[693]\tcv_agg's auc: 0.82401\tcv_agg's rmse: 0.357088\n",
      "[694]\tcv_agg's auc: 0.82404\tcv_agg's rmse: 0.357075\n",
      "[695]\tcv_agg's auc: 0.824024\tcv_agg's rmse: 0.357081\n",
      "[696]\tcv_agg's auc: 0.824027\tcv_agg's rmse: 0.357081\n",
      "[697]\tcv_agg's auc: 0.824036\tcv_agg's rmse: 0.357076\n",
      "[698]\tcv_agg's auc: 0.824035\tcv_agg's rmse: 0.357068\n",
      "[699]\tcv_agg's auc: 0.824052\tcv_agg's rmse: 0.357062\n",
      "[700]\tcv_agg's auc: 0.824046\tcv_agg's rmse: 0.357061\n",
      "[701]\tcv_agg's auc: 0.824069\tcv_agg's rmse: 0.357052\n",
      "[702]\tcv_agg's auc: 0.82406\tcv_agg's rmse: 0.357051\n",
      "[703]\tcv_agg's auc: 0.824128\tcv_agg's rmse: 0.357019\n",
      "[704]\tcv_agg's auc: 0.824146\tcv_agg's rmse: 0.357011\n",
      "[705]\tcv_agg's auc: 0.82413\tcv_agg's rmse: 0.357012\n",
      "[706]\tcv_agg's auc: 0.82414\tcv_agg's rmse: 0.357005\n",
      "[707]\tcv_agg's auc: 0.824147\tcv_agg's rmse: 0.357004\n",
      "[708]\tcv_agg's auc: 0.824158\tcv_agg's rmse: 0.357001\n",
      "[709]\tcv_agg's auc: 0.824159\tcv_agg's rmse: 0.356998\n",
      "[710]\tcv_agg's auc: 0.824148\tcv_agg's rmse: 0.356998\n",
      "[711]\tcv_agg's auc: 0.82415\tcv_agg's rmse: 0.356995\n",
      "[712]\tcv_agg's auc: 0.824181\tcv_agg's rmse: 0.356985\n",
      "[713]\tcv_agg's auc: 0.82423\tcv_agg's rmse: 0.356962\n",
      "[714]\tcv_agg's auc: 0.824237\tcv_agg's rmse: 0.356957\n",
      "[715]\tcv_agg's auc: 0.824234\tcv_agg's rmse: 0.356953\n",
      "[716]\tcv_agg's auc: 0.824223\tcv_agg's rmse: 0.356955\n",
      "[717]\tcv_agg's auc: 0.82426\tcv_agg's rmse: 0.356945\n",
      "[718]\tcv_agg's auc: 0.824296\tcv_agg's rmse: 0.356935\n",
      "[719]\tcv_agg's auc: 0.82432\tcv_agg's rmse: 0.356923\n",
      "[720]\tcv_agg's auc: 0.824334\tcv_agg's rmse: 0.356913\n",
      "[721]\tcv_agg's auc: 0.824379\tcv_agg's rmse: 0.356895\n",
      "[722]\tcv_agg's auc: 0.824367\tcv_agg's rmse: 0.356894\n",
      "[723]\tcv_agg's auc: 0.824403\tcv_agg's rmse: 0.356881\n",
      "[724]\tcv_agg's auc: 0.824412\tcv_agg's rmse: 0.356873\n",
      "[725]\tcv_agg's auc: 0.824431\tcv_agg's rmse: 0.356868\n",
      "[726]\tcv_agg's auc: 0.824455\tcv_agg's rmse: 0.35686\n",
      "[727]\tcv_agg's auc: 0.824467\tcv_agg's rmse: 0.35685\n",
      "[728]\tcv_agg's auc: 0.82447\tcv_agg's rmse: 0.356844\n",
      "[729]\tcv_agg's auc: 0.82451\tcv_agg's rmse: 0.356824\n",
      "[730]\tcv_agg's auc: 0.824492\tcv_agg's rmse: 0.356828\n",
      "[731]\tcv_agg's auc: 0.82449\tcv_agg's rmse: 0.356829\n",
      "[732]\tcv_agg's auc: 0.824506\tcv_agg's rmse: 0.356821\n",
      "[733]\tcv_agg's auc: 0.824517\tcv_agg's rmse: 0.356821\n",
      "[734]\tcv_agg's auc: 0.824522\tcv_agg's rmse: 0.356818\n",
      "[735]\tcv_agg's auc: 0.824548\tcv_agg's rmse: 0.356808\n",
      "[736]\tcv_agg's auc: 0.824549\tcv_agg's rmse: 0.356807\n",
      "[737]\tcv_agg's auc: 0.824544\tcv_agg's rmse: 0.356804\n",
      "[738]\tcv_agg's auc: 0.824565\tcv_agg's rmse: 0.356797\n",
      "[739]\tcv_agg's auc: 0.824564\tcv_agg's rmse: 0.356797\n",
      "[740]\tcv_agg's auc: 0.824583\tcv_agg's rmse: 0.356787\n",
      "[741]\tcv_agg's auc: 0.824582\tcv_agg's rmse: 0.356787\n",
      "[742]\tcv_agg's auc: 0.824585\tcv_agg's rmse: 0.356784\n",
      "[743]\tcv_agg's auc: 0.824638\tcv_agg's rmse: 0.356764\n",
      "[744]\tcv_agg's auc: 0.824652\tcv_agg's rmse: 0.356752\n",
      "[745]\tcv_agg's auc: 0.824656\tcv_agg's rmse: 0.356747\n",
      "[746]\tcv_agg's auc: 0.824669\tcv_agg's rmse: 0.356744\n",
      "[747]\tcv_agg's auc: 0.824678\tcv_agg's rmse: 0.356737\n",
      "[748]\tcv_agg's auc: 0.824706\tcv_agg's rmse: 0.356723\n",
      "[749]\tcv_agg's auc: 0.824721\tcv_agg's rmse: 0.356717\n",
      "[750]\tcv_agg's auc: 0.824721\tcv_agg's rmse: 0.356716\n",
      "[751]\tcv_agg's auc: 0.824747\tcv_agg's rmse: 0.356704\n",
      "[752]\tcv_agg's auc: 0.824766\tcv_agg's rmse: 0.356695\n",
      "[753]\tcv_agg's auc: 0.824767\tcv_agg's rmse: 0.356694\n",
      "[754]\tcv_agg's auc: 0.824758\tcv_agg's rmse: 0.356696\n",
      "[755]\tcv_agg's auc: 0.824752\tcv_agg's rmse: 0.356697\n",
      "[756]\tcv_agg's auc: 0.824769\tcv_agg's rmse: 0.356692\n",
      "[757]\tcv_agg's auc: 0.824778\tcv_agg's rmse: 0.356689\n",
      "[758]\tcv_agg's auc: 0.824799\tcv_agg's rmse: 0.356676\n",
      "[759]\tcv_agg's auc: 0.824835\tcv_agg's rmse: 0.356662\n",
      "[760]\tcv_agg's auc: 0.824835\tcv_agg's rmse: 0.356666\n",
      "[761]\tcv_agg's auc: 0.824836\tcv_agg's rmse: 0.356662\n",
      "[762]\tcv_agg's auc: 0.824825\tcv_agg's rmse: 0.356667\n",
      "[763]\tcv_agg's auc: 0.824834\tcv_agg's rmse: 0.356658\n",
      "[764]\tcv_agg's auc: 0.82483\tcv_agg's rmse: 0.356658\n",
      "[765]\tcv_agg's auc: 0.82485\tcv_agg's rmse: 0.356646\n",
      "[766]\tcv_agg's auc: 0.824891\tcv_agg's rmse: 0.356625\n",
      "[767]\tcv_agg's auc: 0.824901\tcv_agg's rmse: 0.356617\n",
      "[768]\tcv_agg's auc: 0.824893\tcv_agg's rmse: 0.356618\n",
      "[769]\tcv_agg's auc: 0.824904\tcv_agg's rmse: 0.356613\n",
      "[770]\tcv_agg's auc: 0.824873\tcv_agg's rmse: 0.356617\n",
      "[771]\tcv_agg's auc: 0.824853\tcv_agg's rmse: 0.356618\n",
      "[772]\tcv_agg's auc: 0.82486\tcv_agg's rmse: 0.356609\n",
      "[773]\tcv_agg's auc: 0.824845\tcv_agg's rmse: 0.356614\n",
      "[774]\tcv_agg's auc: 0.824864\tcv_agg's rmse: 0.356614\n",
      "[775]\tcv_agg's auc: 0.824865\tcv_agg's rmse: 0.356608\n",
      "[776]\tcv_agg's auc: 0.824872\tcv_agg's rmse: 0.356603\n",
      "[777]\tcv_agg's auc: 0.824873\tcv_agg's rmse: 0.356601\n",
      "[778]\tcv_agg's auc: 0.82486\tcv_agg's rmse: 0.356601\n",
      "[779]\tcv_agg's auc: 0.824875\tcv_agg's rmse: 0.356593\n",
      "[780]\tcv_agg's auc: 0.824885\tcv_agg's rmse: 0.356587\n",
      "[781]\tcv_agg's auc: 0.824905\tcv_agg's rmse: 0.356578\n",
      "[782]\tcv_agg's auc: 0.824916\tcv_agg's rmse: 0.356568\n",
      "[783]\tcv_agg's auc: 0.824949\tcv_agg's rmse: 0.356554\n",
      "[784]\tcv_agg's auc: 0.824959\tcv_agg's rmse: 0.356548\n",
      "[785]\tcv_agg's auc: 0.824949\tcv_agg's rmse: 0.356552\n",
      "[786]\tcv_agg's auc: 0.82497\tcv_agg's rmse: 0.356541\n",
      "[787]\tcv_agg's auc: 0.824985\tcv_agg's rmse: 0.356536\n",
      "[788]\tcv_agg's auc: 0.824991\tcv_agg's rmse: 0.356532\n",
      "[789]\tcv_agg's auc: 0.825008\tcv_agg's rmse: 0.356524\n",
      "[790]\tcv_agg's auc: 0.825025\tcv_agg's rmse: 0.35651\n",
      "[791]\tcv_agg's auc: 0.825077\tcv_agg's rmse: 0.356488\n",
      "[792]\tcv_agg's auc: 0.825096\tcv_agg's rmse: 0.356477\n",
      "[793]\tcv_agg's auc: 0.825113\tcv_agg's rmse: 0.356468\n",
      "[794]\tcv_agg's auc: 0.825129\tcv_agg's rmse: 0.356462\n",
      "[795]\tcv_agg's auc: 0.825147\tcv_agg's rmse: 0.356448\n",
      "[796]\tcv_agg's auc: 0.825174\tcv_agg's rmse: 0.356442\n",
      "[797]\tcv_agg's auc: 0.825157\tcv_agg's rmse: 0.356444\n",
      "[798]\tcv_agg's auc: 0.825234\tcv_agg's rmse: 0.356417\n",
      "[799]\tcv_agg's auc: 0.825236\tcv_agg's rmse: 0.356412\n",
      "[800]\tcv_agg's auc: 0.825232\tcv_agg's rmse: 0.356414\n",
      "[801]\tcv_agg's auc: 0.825216\tcv_agg's rmse: 0.356419\n",
      "[802]\tcv_agg's auc: 0.825227\tcv_agg's rmse: 0.356411\n",
      "[803]\tcv_agg's auc: 0.82524\tcv_agg's rmse: 0.35641\n",
      "[804]\tcv_agg's auc: 0.825276\tcv_agg's rmse: 0.356393\n",
      "[805]\tcv_agg's auc: 0.825288\tcv_agg's rmse: 0.356386\n",
      "[806]\tcv_agg's auc: 0.825289\tcv_agg's rmse: 0.356381\n",
      "[807]\tcv_agg's auc: 0.82533\tcv_agg's rmse: 0.35637\n",
      "[808]\tcv_agg's auc: 0.825333\tcv_agg's rmse: 0.356369\n",
      "[809]\tcv_agg's auc: 0.825353\tcv_agg's rmse: 0.356361\n",
      "[810]\tcv_agg's auc: 0.825338\tcv_agg's rmse: 0.35636\n",
      "[811]\tcv_agg's auc: 0.825339\tcv_agg's rmse: 0.356358\n",
      "[812]\tcv_agg's auc: 0.825333\tcv_agg's rmse: 0.356356\n",
      "[813]\tcv_agg's auc: 0.825327\tcv_agg's rmse: 0.356356\n",
      "[814]\tcv_agg's auc: 0.825318\tcv_agg's rmse: 0.356353\n",
      "[815]\tcv_agg's auc: 0.825338\tcv_agg's rmse: 0.35634\n",
      "[816]\tcv_agg's auc: 0.825415\tcv_agg's rmse: 0.356306\n",
      "[817]\tcv_agg's auc: 0.825415\tcv_agg's rmse: 0.356304\n",
      "[818]\tcv_agg's auc: 0.825425\tcv_agg's rmse: 0.356303\n",
      "[819]\tcv_agg's auc: 0.825447\tcv_agg's rmse: 0.3563\n",
      "[820]\tcv_agg's auc: 0.825464\tcv_agg's rmse: 0.356296\n",
      "[821]\tcv_agg's auc: 0.825487\tcv_agg's rmse: 0.356286\n",
      "[822]\tcv_agg's auc: 0.825502\tcv_agg's rmse: 0.356279\n",
      "[823]\tcv_agg's auc: 0.825522\tcv_agg's rmse: 0.356274\n",
      "[824]\tcv_agg's auc: 0.825636\tcv_agg's rmse: 0.356222\n",
      "[825]\tcv_agg's auc: 0.825634\tcv_agg's rmse: 0.356224\n",
      "[826]\tcv_agg's auc: 0.825635\tcv_agg's rmse: 0.35622\n",
      "[827]\tcv_agg's auc: 0.825635\tcv_agg's rmse: 0.356222\n",
      "[828]\tcv_agg's auc: 0.825627\tcv_agg's rmse: 0.356224\n",
      "[829]\tcv_agg's auc: 0.825625\tcv_agg's rmse: 0.356226\n",
      "[830]\tcv_agg's auc: 0.825622\tcv_agg's rmse: 0.356219\n",
      "[831]\tcv_agg's auc: 0.825626\tcv_agg's rmse: 0.356218\n",
      "[832]\tcv_agg's auc: 0.825624\tcv_agg's rmse: 0.356219\n",
      "[833]\tcv_agg's auc: 0.82566\tcv_agg's rmse: 0.356198\n",
      "[834]\tcv_agg's auc: 0.825659\tcv_agg's rmse: 0.356197\n",
      "[835]\tcv_agg's auc: 0.825668\tcv_agg's rmse: 0.356189\n",
      "[836]\tcv_agg's auc: 0.825724\tcv_agg's rmse: 0.356161\n",
      "[837]\tcv_agg's auc: 0.825726\tcv_agg's rmse: 0.356156\n",
      "[838]\tcv_agg's auc: 0.825729\tcv_agg's rmse: 0.356153\n",
      "[839]\tcv_agg's auc: 0.825766\tcv_agg's rmse: 0.356141\n",
      "[840]\tcv_agg's auc: 0.825798\tcv_agg's rmse: 0.35613\n",
      "[841]\tcv_agg's auc: 0.825793\tcv_agg's rmse: 0.356132\n",
      "[842]\tcv_agg's auc: 0.825805\tcv_agg's rmse: 0.356122\n",
      "[843]\tcv_agg's auc: 0.825789\tcv_agg's rmse: 0.356129\n",
      "[844]\tcv_agg's auc: 0.825794\tcv_agg's rmse: 0.356126\n",
      "[845]\tcv_agg's auc: 0.825808\tcv_agg's rmse: 0.356123\n",
      "[846]\tcv_agg's auc: 0.825801\tcv_agg's rmse: 0.356125\n",
      "[847]\tcv_agg's auc: 0.825801\tcv_agg's rmse: 0.356125\n",
      "[848]\tcv_agg's auc: 0.825836\tcv_agg's rmse: 0.356109\n",
      "[849]\tcv_agg's auc: 0.825843\tcv_agg's rmse: 0.356108\n",
      "[850]\tcv_agg's auc: 0.825846\tcv_agg's rmse: 0.356102\n",
      "[851]\tcv_agg's auc: 0.825873\tcv_agg's rmse: 0.356094\n",
      "[852]\tcv_agg's auc: 0.825888\tcv_agg's rmse: 0.356088\n",
      "[853]\tcv_agg's auc: 0.825901\tcv_agg's rmse: 0.356086\n",
      "[854]\tcv_agg's auc: 0.825929\tcv_agg's rmse: 0.356076\n",
      "[855]\tcv_agg's auc: 0.825949\tcv_agg's rmse: 0.356071\n",
      "[856]\tcv_agg's auc: 0.825964\tcv_agg's rmse: 0.356067\n",
      "[857]\tcv_agg's auc: 0.825981\tcv_agg's rmse: 0.356058\n",
      "[858]\tcv_agg's auc: 0.825983\tcv_agg's rmse: 0.356059\n",
      "[859]\tcv_agg's auc: 0.826013\tcv_agg's rmse: 0.356045\n",
      "[860]\tcv_agg's auc: 0.826038\tcv_agg's rmse: 0.356039\n",
      "[861]\tcv_agg's auc: 0.82607\tcv_agg's rmse: 0.356027\n",
      "[862]\tcv_agg's auc: 0.826086\tcv_agg's rmse: 0.356018\n",
      "[863]\tcv_agg's auc: 0.82609\tcv_agg's rmse: 0.356017\n",
      "[864]\tcv_agg's auc: 0.826083\tcv_agg's rmse: 0.356018\n",
      "[865]\tcv_agg's auc: 0.826093\tcv_agg's rmse: 0.356012\n",
      "[866]\tcv_agg's auc: 0.826114\tcv_agg's rmse: 0.356002\n",
      "[867]\tcv_agg's auc: 0.826104\tcv_agg's rmse: 0.356003\n",
      "[868]\tcv_agg's auc: 0.826128\tcv_agg's rmse: 0.355993\n",
      "[869]\tcv_agg's auc: 0.826115\tcv_agg's rmse: 0.355998\n",
      "[870]\tcv_agg's auc: 0.826144\tcv_agg's rmse: 0.355985\n",
      "[871]\tcv_agg's auc: 0.826149\tcv_agg's rmse: 0.355977\n",
      "[872]\tcv_agg's auc: 0.826176\tcv_agg's rmse: 0.355969\n",
      "[873]\tcv_agg's auc: 0.826182\tcv_agg's rmse: 0.355971\n",
      "[874]\tcv_agg's auc: 0.826178\tcv_agg's rmse: 0.35597\n",
      "[875]\tcv_agg's auc: 0.826174\tcv_agg's rmse: 0.355972\n",
      "[876]\tcv_agg's auc: 0.82616\tcv_agg's rmse: 0.355977\n",
      "[877]\tcv_agg's auc: 0.826164\tcv_agg's rmse: 0.355975\n",
      "[878]\tcv_agg's auc: 0.826217\tcv_agg's rmse: 0.355956\n",
      "[879]\tcv_agg's auc: 0.826236\tcv_agg's rmse: 0.355951\n",
      "[880]\tcv_agg's auc: 0.826232\tcv_agg's rmse: 0.355951\n",
      "[881]\tcv_agg's auc: 0.826245\tcv_agg's rmse: 0.355945\n",
      "[882]\tcv_agg's auc: 0.826245\tcv_agg's rmse: 0.355938\n",
      "[883]\tcv_agg's auc: 0.826251\tcv_agg's rmse: 0.355939\n",
      "[884]\tcv_agg's auc: 0.826251\tcv_agg's rmse: 0.355939\n",
      "[885]\tcv_agg's auc: 0.826378\tcv_agg's rmse: 0.35589\n",
      "[886]\tcv_agg's auc: 0.826361\tcv_agg's rmse: 0.355894\n",
      "[887]\tcv_agg's auc: 0.826337\tcv_agg's rmse: 0.3559\n",
      "[888]\tcv_agg's auc: 0.82634\tcv_agg's rmse: 0.355896\n",
      "[889]\tcv_agg's auc: 0.826351\tcv_agg's rmse: 0.355895\n",
      "[890]\tcv_agg's auc: 0.82637\tcv_agg's rmse: 0.355891\n",
      "[891]\tcv_agg's auc: 0.82637\tcv_agg's rmse: 0.355887\n",
      "[892]\tcv_agg's auc: 0.826368\tcv_agg's rmse: 0.355889\n",
      "[893]\tcv_agg's auc: 0.826371\tcv_agg's rmse: 0.355884\n",
      "[894]\tcv_agg's auc: 0.826363\tcv_agg's rmse: 0.355887\n",
      "[895]\tcv_agg's auc: 0.826406\tcv_agg's rmse: 0.355875\n",
      "[896]\tcv_agg's auc: 0.82641\tcv_agg's rmse: 0.35587\n",
      "[897]\tcv_agg's auc: 0.826423\tcv_agg's rmse: 0.35587\n",
      "[898]\tcv_agg's auc: 0.826414\tcv_agg's rmse: 0.355873\n",
      "[899]\tcv_agg's auc: 0.826393\tcv_agg's rmse: 0.355875\n",
      "[900]\tcv_agg's auc: 0.826383\tcv_agg's rmse: 0.355875\n",
      "[901]\tcv_agg's auc: 0.826381\tcv_agg's rmse: 0.355875\n",
      "[902]\tcv_agg's auc: 0.82638\tcv_agg's rmse: 0.355873\n",
      "[903]\tcv_agg's auc: 0.826377\tcv_agg's rmse: 0.355871\n",
      "[904]\tcv_agg's auc: 0.826375\tcv_agg's rmse: 0.355873\n",
      "[905]\tcv_agg's auc: 0.826409\tcv_agg's rmse: 0.355856\n",
      "[906]\tcv_agg's auc: 0.826448\tcv_agg's rmse: 0.355835\n",
      "[907]\tcv_agg's auc: 0.826525\tcv_agg's rmse: 0.355805\n",
      "[908]\tcv_agg's auc: 0.826528\tcv_agg's rmse: 0.355804\n",
      "[909]\tcv_agg's auc: 0.826525\tcv_agg's rmse: 0.355803\n",
      "[910]\tcv_agg's auc: 0.826556\tcv_agg's rmse: 0.355787\n",
      "[911]\tcv_agg's auc: 0.826551\tcv_agg's rmse: 0.355782\n",
      "[912]\tcv_agg's auc: 0.826546\tcv_agg's rmse: 0.355786\n",
      "[913]\tcv_agg's auc: 0.826534\tcv_agg's rmse: 0.355785\n",
      "[914]\tcv_agg's auc: 0.826527\tcv_agg's rmse: 0.355788\n",
      "[915]\tcv_agg's auc: 0.826506\tcv_agg's rmse: 0.355791\n",
      "[916]\tcv_agg's auc: 0.826586\tcv_agg's rmse: 0.35575\n",
      "[917]\tcv_agg's auc: 0.826596\tcv_agg's rmse: 0.355747\n",
      "[918]\tcv_agg's auc: 0.826585\tcv_agg's rmse: 0.355751\n",
      "[919]\tcv_agg's auc: 0.826587\tcv_agg's rmse: 0.355755\n",
      "[920]\tcv_agg's auc: 0.826594\tcv_agg's rmse: 0.355751\n",
      "[921]\tcv_agg's auc: 0.82661\tcv_agg's rmse: 0.355741\n",
      "[922]\tcv_agg's auc: 0.826607\tcv_agg's rmse: 0.355738\n",
      "[923]\tcv_agg's auc: 0.82661\tcv_agg's rmse: 0.355737\n",
      "[924]\tcv_agg's auc: 0.826604\tcv_agg's rmse: 0.35574\n",
      "[925]\tcv_agg's auc: 0.826635\tcv_agg's rmse: 0.355732\n",
      "[926]\tcv_agg's auc: 0.826633\tcv_agg's rmse: 0.355729\n",
      "[927]\tcv_agg's auc: 0.826612\tcv_agg's rmse: 0.355735\n",
      "[928]\tcv_agg's auc: 0.826636\tcv_agg's rmse: 0.35572\n",
      "[929]\tcv_agg's auc: 0.82663\tcv_agg's rmse: 0.355724\n",
      "[930]\tcv_agg's auc: 0.826643\tcv_agg's rmse: 0.355713\n",
      "[931]\tcv_agg's auc: 0.826724\tcv_agg's rmse: 0.355679\n",
      "[932]\tcv_agg's auc: 0.826732\tcv_agg's rmse: 0.355674\n",
      "[933]\tcv_agg's auc: 0.826731\tcv_agg's rmse: 0.355675\n",
      "[934]\tcv_agg's auc: 0.826773\tcv_agg's rmse: 0.355664\n",
      "[935]\tcv_agg's auc: 0.826786\tcv_agg's rmse: 0.355659\n",
      "[936]\tcv_agg's auc: 0.826787\tcv_agg's rmse: 0.355656\n",
      "[937]\tcv_agg's auc: 0.826804\tcv_agg's rmse: 0.355645\n",
      "[938]\tcv_agg's auc: 0.826791\tcv_agg's rmse: 0.355649\n",
      "[939]\tcv_agg's auc: 0.826837\tcv_agg's rmse: 0.355629\n",
      "[940]\tcv_agg's auc: 0.826861\tcv_agg's rmse: 0.355621\n",
      "[941]\tcv_agg's auc: 0.82685\tcv_agg's rmse: 0.355623\n",
      "[942]\tcv_agg's auc: 0.826846\tcv_agg's rmse: 0.355621\n",
      "[943]\tcv_agg's auc: 0.826848\tcv_agg's rmse: 0.355619\n",
      "[944]\tcv_agg's auc: 0.826837\tcv_agg's rmse: 0.355624\n",
      "[945]\tcv_agg's auc: 0.826828\tcv_agg's rmse: 0.35563\n",
      "[946]\tcv_agg's auc: 0.826827\tcv_agg's rmse: 0.355626\n",
      "[947]\tcv_agg's auc: 0.826817\tcv_agg's rmse: 0.355629\n",
      "[948]\tcv_agg's auc: 0.826822\tcv_agg's rmse: 0.355624\n",
      "[949]\tcv_agg's auc: 0.826847\tcv_agg's rmse: 0.355619\n",
      "[950]\tcv_agg's auc: 0.826834\tcv_agg's rmse: 0.355622\n",
      "[951]\tcv_agg's auc: 0.826853\tcv_agg's rmse: 0.355614\n",
      "[952]\tcv_agg's auc: 0.826854\tcv_agg's rmse: 0.355609\n",
      "[953]\tcv_agg's auc: 0.826815\tcv_agg's rmse: 0.355618\n",
      "[954]\tcv_agg's auc: 0.826805\tcv_agg's rmse: 0.355617\n",
      "[955]\tcv_agg's auc: 0.826788\tcv_agg's rmse: 0.355623\n",
      "[956]\tcv_agg's auc: 0.826811\tcv_agg's rmse: 0.355611\n",
      "[957]\tcv_agg's auc: 0.826819\tcv_agg's rmse: 0.35561\n",
      "[958]\tcv_agg's auc: 0.826809\tcv_agg's rmse: 0.355609\n",
      "[959]\tcv_agg's auc: 0.826832\tcv_agg's rmse: 0.355598\n",
      "[960]\tcv_agg's auc: 0.826809\tcv_agg's rmse: 0.355606\n"
     ]
    }
   ],
   "source": [
    "# CV for lgbm\n",
    "# Using log vals to simulate eval metric\n",
    "cv_result = lgb.cv(\n",
    "lgb_params, \n",
    "lgb_train, \n",
    "nfold=10,\n",
    "num_boost_round=2000, \n",
    "early_stopping_rounds=40,\n",
    "verbose_eval=20, \n",
    "show_stdv=False,\n",
    "seed=2001\n",
    ")\n",
    "\n",
    "print('Num rds from 10-fold CV, lrate =.01 == %i.' % len(cv_result['auc-mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set number of boost rds w/ CV result\n",
    "cv_result[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
    "if cv_result:\n",
    "    rds = len(cv_result)\n",
    "    print('num rds from cv = %i' % rds)\n",
    "else:\n",
    "    rds = 940\n",
    "    print('rds manually set to %i' % rds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tcv_agg's auc: 0.761433\tcv_agg's rmse: 0.389428\n",
      "[40]\tcv_agg's auc: 0.775898\tcv_agg's rmse: 0.379144\n",
      "[60]\tcv_agg's auc: 0.785548\tcv_agg's rmse: 0.374442\n",
      "[80]\tcv_agg's auc: 0.791878\tcv_agg's rmse: 0.371547\n",
      "[100]\tcv_agg's auc: 0.795907\tcv_agg's rmse: 0.369703\n",
      "[120]\tcv_agg's auc: 0.798611\tcv_agg's rmse: 0.368399\n",
      "[140]\tcv_agg's auc: 0.80139\tcv_agg's rmse: 0.36722\n",
      "[160]\tcv_agg's auc: 0.803493\tcv_agg's rmse: 0.366354\n",
      "[180]\tcv_agg's auc: 0.805466\tcv_agg's rmse: 0.36556\n",
      "[200]\tcv_agg's auc: 0.80707\tcv_agg's rmse: 0.364862\n",
      "[220]\tcv_agg's auc: 0.808439\tcv_agg's rmse: 0.36427\n",
      "[240]\tcv_agg's auc: 0.809595\tcv_agg's rmse: 0.363733\n",
      "[260]\tcv_agg's auc: 0.810681\tcv_agg's rmse: 0.363239\n",
      "[280]\tcv_agg's auc: 0.81216\tcv_agg's rmse: 0.362589\n",
      "[300]\tcv_agg's auc: 0.812937\tcv_agg's rmse: 0.362215\n",
      "[320]\tcv_agg's auc: 0.813569\tcv_agg's rmse: 0.361865\n",
      "[340]\tcv_agg's auc: 0.81462\tcv_agg's rmse: 0.361418\n",
      "[360]\tcv_agg's auc: 0.815128\tcv_agg's rmse: 0.361115\n",
      "[380]\tcv_agg's auc: 0.815883\tcv_agg's rmse: 0.360751\n",
      "[400]\tcv_agg's auc: 0.816628\tcv_agg's rmse: 0.360438\n",
      "[420]\tcv_agg's auc: 0.81734\tcv_agg's rmse: 0.360138\n",
      "[440]\tcv_agg's auc: 0.817953\tcv_agg's rmse: 0.359866\n",
      "[460]\tcv_agg's auc: 0.818488\tcv_agg's rmse: 0.359617\n",
      "[480]\tcv_agg's auc: 0.81919\tcv_agg's rmse: 0.359317\n",
      "[500]\tcv_agg's auc: 0.819622\tcv_agg's rmse: 0.359098\n",
      "[520]\tcv_agg's auc: 0.820233\tcv_agg's rmse: 0.358825\n",
      "[540]\tcv_agg's auc: 0.82101\tcv_agg's rmse: 0.358511\n",
      "[560]\tcv_agg's auc: 0.821324\tcv_agg's rmse: 0.358354\n",
      "[580]\tcv_agg's auc: 0.821852\tcv_agg's rmse: 0.358118\n",
      "[600]\tcv_agg's auc: 0.822155\tcv_agg's rmse: 0.357963\n",
      "[620]\tcv_agg's auc: 0.822691\tcv_agg's rmse: 0.357705\n",
      "[640]\tcv_agg's auc: 0.823137\tcv_agg's rmse: 0.357497\n",
      "[660]\tcv_agg's auc: 0.823355\tcv_agg's rmse: 0.35738\n",
      "[680]\tcv_agg's auc: 0.823729\tcv_agg's rmse: 0.357228\n",
      "[700]\tcv_agg's auc: 0.824046\tcv_agg's rmse: 0.357061\n",
      "[720]\tcv_agg's auc: 0.824334\tcv_agg's rmse: 0.356913\n",
      "[740]\tcv_agg's auc: 0.824583\tcv_agg's rmse: 0.356787\n",
      "[760]\tcv_agg's auc: 0.824835\tcv_agg's rmse: 0.356666\n",
      "[780]\tcv_agg's auc: 0.824885\tcv_agg's rmse: 0.356587\n",
      "[800]\tcv_agg's auc: 0.825232\tcv_agg's rmse: 0.356414\n",
      "[820]\tcv_agg's auc: 0.825464\tcv_agg's rmse: 0.356296\n",
      "[840]\tcv_agg's auc: 0.825798\tcv_agg's rmse: 0.35613\n",
      "[860]\tcv_agg's auc: 0.826038\tcv_agg's rmse: 0.356039\n",
      "[880]\tcv_agg's auc: 0.826232\tcv_agg's rmse: 0.355951\n",
      "[900]\tcv_agg's auc: 0.826383\tcv_agg's rmse: 0.355875\n",
      "[920]\tcv_agg's auc: 0.826594\tcv_agg's rmse: 0.355751\n",
      "[940]\tcv_agg's auc: 0.826861\tcv_agg's rmse: 0.355621\n",
      "[960]\tcv_agg's auc: 0.826809\tcv_agg's rmse: 0.355606\n"
     ]
    }
   ],
   "source": [
    "# See if we can get away with 63 bins (much faster than 255).\n",
    "lgb_params_2 = {\n",
    "    'learning_rate': 0.1,\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': ['rmse', 'auc'],\n",
    "    'num_leaves': 31,\n",
    "    'max_depth':-1,\n",
    "    'max_bin':63,\n",
    "    'min_split_gain':0,\n",
    "    'min_child_weight':5,\n",
    "    'min_child_samples':30,\n",
    "    'subsample':.8,\n",
    "    'subsample_freq':1,\n",
    "    'colsample_bytree':.8,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':0,\n",
    "    'seed':0,\n",
    "    'nthread':-1,\n",
    "    'verbose': 20\n",
    "}\n",
    "\n",
    "cv_result = lgb.cv(\n",
    "lgb_params_2, \n",
    "lgb_train, \n",
    "nfold=10,\n",
    "num_boost_round=2000, \n",
    "early_stopping_rounds=20,\n",
    "verbose_eval=50, \n",
    "show_stdv=False,\n",
    "seed=2001\n",
    ")\n",
    "\n",
    "print('Num rds from 10-fold CV, lrate =.01 == %i.' % len(cv_result['auc-mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check AUC vs. iter\n",
    "plt.plot(cv_result['auc-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get perf of starter on test set\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=960,\n",
    "    boosting_type='gbdt',\n",
    "    max_depth=-1,\n",
    "    max_bin=63,\n",
    "    min_split_gain=0,\n",
    "    min_child_weight=5,\n",
    "    min_child_samples=30,\n",
    "    subsample=.8,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=.8,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=0,\n",
    "    seed=0,\n",
    "    nthread=-1,\n",
    "    silent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm.fit(X_train_enc, Y_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score:\n",
      "0.819382956997\n",
      "\n",
      "LogLoss:\n",
      "0.41099269183\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.827     0.961     0.889     25951\n",
      "       True      0.736     0.351     0.476      8023\n",
      "\n",
      "avg / total      0.806     0.817     0.792     33974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_classif_on_test(gbm, X_test_enc, Y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune LightGBM with random search - far more efficient than grid search.\n",
    "\n",
    "param_dists = {\n",
    "    'num_leaves': scipy.stats.randint(15, 63),\n",
    "    'min_split_gain': scipy.stats.expon(scale=.5),\n",
    "    'min_child_weight': scipy.stats.uniform(2, 20),  # from 2 to 2+20\n",
    "    'min_child_samples': scipy.stats.randint(15, 100),\n",
    "    'subsample': scipy.stats.uniform(.6, .35),\n",
    "    # subsample_freq=1,\n",
    "    'colsample_bytree': scipy.stats.uniform(.6, .35),\n",
    "    # reg_alpha=0,\n",
    "    'reg_lambda': scipy.stats.expon(scale=.5),    \n",
    "}\n",
    "\n",
    "\n",
    "def test_params_random(model, param_value_dict, xs, ys, n_iter=100, n_jobs=1 verbose=False):\n",
    "    \"\"\"\n",
    "    Run a grid search for a model using given param values and print results.\n",
    "    CV is stratified K-Fold in this instance.\n",
    "    (SKL is smart enough to know to prefer stratified CV in classification problems.)\n",
    "    Note that for LightGBM we'd be better off using n_jobs = n_threads on processor\n",
    "      and setting GBM model's threads to 1! This is because LightGBM parallelizes\n",
    "      by feature, and our dataset doesn't have very many cols.\n",
    "    However, for simplicity, I've just left LightGBM at max threads and not parallelized\n",
    "      the random search process.\n",
    "    \n",
    "    :param model: SciKit-Learn model, i.e. one that has \n",
    "      .fit() and .predict() methods\n",
    "    :param param_value_dict: dict, with keys of param names \n",
    "      and values of [values] to try for given param\n",
    "    :return: None, prints results to stdout\n",
    "    \"\"\"\n",
    "    # First, convert ROC_AUC from SKL metric to scorer API.\n",
    "    roc_scorer = make_scorer(roc_auc_score)\n",
    "    # Init RandomizedSearchCV instance.\n",
    "    rsearch = RandomizedSearchCV(\n",
    "        gbm, \n",
    "        param_dists, \n",
    "        n_iter=n_iter, \n",
    "        scoring=roc_scorer, \n",
    "        fit_params=None, \n",
    "        n_jobs=1,\n",
    "        pre_dispatch='n_jobs', \n",
    "        random_state=0\n",
    "    )\n",
    "    rsearch.fit(xs, ys)\n",
    "    if verbose:\n",
    "        print('Raw grid search scores:')\n",
    "        rsearch.grid_scores_ \n",
    "    print('Best params for this search:')\n",
    "    print(rsearch.best_params_)\n",
    "    print('Best AUC for these params:')\n",
    "    print(rsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for this search:\n",
      "{'colsample_bytree': 0.63379236343961032, 'min_child_samples': 43, 'min_child_weight': 4.6296559858225521, 'min_split_gain': 0.028483268112510837, 'num_leaves': 60, 'reg_lambda': 0.3346514335803939, 'subsample': 0.76048009328282773}\n",
      "Best AUC for these params:\n",
      "0.656695475342\n"
     ]
    }
   ],
   "source": [
    "test_params_random(gbm, param_dists, X_train_enc, Y_train_enc, n_iter=200, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tcv_agg's auc: 0.770988\tcv_agg's rmse: 0.387766\n",
      "[40]\tcv_agg's auc: 0.785937\tcv_agg's rmse: 0.375396\n",
      "[60]\tcv_agg's auc: 0.793637\tcv_agg's rmse: 0.370853\n",
      "[80]\tcv_agg's auc: 0.798058\tcv_agg's rmse: 0.368747\n",
      "[100]\tcv_agg's auc: 0.801501\tcv_agg's rmse: 0.367319\n",
      "[120]\tcv_agg's auc: 0.802906\tcv_agg's rmse: 0.366443\n",
      "[140]\tcv_agg's auc: 0.804429\tcv_agg's rmse: 0.36571\n",
      "[160]\tcv_agg's auc: 0.805322\tcv_agg's rmse: 0.365298\n",
      "[180]\tcv_agg's auc: 0.806251\tcv_agg's rmse: 0.364898\n",
      "[200]\tcv_agg's auc: 0.807183\tcv_agg's rmse: 0.3645\n",
      "[220]\tcv_agg's auc: 0.807568\tcv_agg's rmse: 0.364272\n",
      "[240]\tcv_agg's auc: 0.808284\tcv_agg's rmse: 0.36395\n",
      "[260]\tcv_agg's auc: 0.809206\tcv_agg's rmse: 0.363571\n",
      "[280]\tcv_agg's auc: 0.809977\tcv_agg's rmse: 0.363245\n",
      "[300]\tcv_agg's auc: 0.810512\tcv_agg's rmse: 0.36298\n",
      "[320]\tcv_agg's auc: 0.811113\tcv_agg's rmse: 0.362682\n",
      "[340]\tcv_agg's auc: 0.811729\tcv_agg's rmse: 0.3624\n",
      "[360]\tcv_agg's auc: 0.811852\tcv_agg's rmse: 0.362323\n",
      "[380]\tcv_agg's auc: 0.812225\tcv_agg's rmse: 0.362127\n",
      "[400]\tcv_agg's auc: 0.812532\tcv_agg's rmse: 0.361905\n",
      "[420]\tcv_agg's auc: 0.812881\tcv_agg's rmse: 0.36173\n",
      "[440]\tcv_agg's auc: 0.813456\tcv_agg's rmse: 0.361463\n",
      "[460]\tcv_agg's auc: 0.813712\tcv_agg's rmse: 0.36133\n",
      "[480]\tcv_agg's auc: 0.813767\tcv_agg's rmse: 0.36128\n",
      "[500]\tcv_agg's auc: 0.814278\tcv_agg's rmse: 0.361092\n",
      "[520]\tcv_agg's auc: 0.814397\tcv_agg's rmse: 0.36101\n",
      "[540]\tcv_agg's auc: 0.814612\tcv_agg's rmse: 0.360894\n",
      "[560]\tcv_agg's auc: 0.814664\tcv_agg's rmse: 0.360821\n",
      "[580]\tcv_agg's auc: 0.815021\tcv_agg's rmse: 0.360656\n",
      "[600]\tcv_agg's auc: 0.815085\tcv_agg's rmse: 0.360606\n"
     ]
    }
   ],
   "source": [
    "# Try out params from above.\n",
    "# Were our ranges reasonable, or will we do worse than defaults?\n",
    "lgb_params_2 = {\n",
    "    'learning_rate': 0.1,\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': ['rmse', 'auc'],\n",
    "    'num_leaves': 60,\n",
    "    'max_depth':-1,\n",
    "    'max_bin':63,\n",
    "    'min_split_gain':0.28,\n",
    "    'min_child_weight':4.6,\n",
    "    'min_child_samples':43,\n",
    "    'subsample':.76,\n",
    "    'subsample_freq':1,\n",
    "    'colsample_bytree':.63,\n",
    "    'reg_alpha':0,\n",
    "    'reg_lambda':0.33,\n",
    "    'seed':0,\n",
    "    'nthread':-1,\n",
    "    'verbose': 20\n",
    "}\n",
    "\n",
    "cv_result = lgb.cv(\n",
    "lgb_params_2, \n",
    "lgb_train, \n",
    "nfold=10,\n",
    "num_boost_round=2000, \n",
    "early_stopping_rounds=20,\n",
    "verbose_eval=20, \n",
    "show_stdv=False,\n",
    "seed=2001\n",
    ")\n",
    "\n",
    "print('Num rds from 10-fold CV, lrate =.01 == %i.' % len(cv_result['auc-mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check AUC vs. iter\n",
    "plt.plot(cv_result['auc-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for this search:\n",
      "{'colsample_bytree': 0.72330908142018657, 'min_child_samples': 69, 'min_child_weight': 21.976940131357331, 'min_split_gain': 0.080935043349130792, 'num_leaves': 62, 'reg_lambda': 0.019141456103673956, 'subsample': 0.77791040700261171}\n",
      "Best AUC for these params:\n",
      "0.651684753249\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw grid search scores:\n",
      "Best params for this search:\n",
      "{'colsample_bytree': 0.73011936988780124, 'min_child_samples': 34, 'min_child_weight': 6.7309770347573288, 'subsample': 0.75998025533188096}\n",
      "Best AUC for these params:\n",
      "0.651641914448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw grid search scores:\n",
      "Best params for this search:\n",
      "{'min_child_samples': 24, 'num_leaves': 113}\n",
      "Best AUC for these params:\n",
      "0.673458934264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw grid search scores:\n",
      "Best params for this search:\n",
      "{'min_child_samples': 41, 'min_child_weight': 6.2363042875116852, 'num_leaves': 123}\n",
      "Best AUC for these params:\n",
      "0.674730830726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw grid search scores:\n",
      "Best params for this search:\n",
      "{'max_depth': 11, 'min_data_in_bin': 3, 'reg_lambda': 0.1}\n",
      "Best AUC for these params:\n",
      "0.672165523869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw grid search scores:\n",
      "Best params for this search:\n",
      "{'max_depth': 17, 'min_data_in_bin': 1, 'reg_lambda': 0.1}\n",
      "Best AUC for these params:\n",
      "0.674993629946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Tune LightGBM with random search - far more efficient than grid search.\n",
    "\n",
    "# Record of random search runs, 30-100 iters each:\n",
    "\n",
    "# Best params for this search:\n",
    "# {'colsample_bytree': 0.72330908142018657, 'min_child_samples': 69, 'min_child_weight': 21.976940131357331, 'min_split_gain': 0.080935043349130792, 'num_leaves': 62, 'reg_lambda': 0.019141456103673956, 'subsample': 0.77791040700261171}\n",
    "# Best AUC for these params:\n",
    "# 0.651684753249\n",
    "\n",
    "# ---\n",
    "\n",
    "# Raw grid search scores:\n",
    "# Best params for this search:\n",
    "# {'colsample_bytree': 0.73011936988780124, 'min_child_samples': 34, 'min_child_weight': 6.7309770347573288, 'subsample': 0.75998025533188096}\n",
    "# Best AUC for these params:\n",
    "# 0.651641914448\n",
    "\n",
    "# ---\n",
    "\n",
    "# Raw grid search scores:\n",
    "# Best params for this search:\n",
    "# {'min_child_samples': 24, 'num_leaves': 113}\n",
    "# Best AUC for these params:\n",
    "# 0.673458934264\n",
    "\n",
    "# Raw grid search scores:\n",
    "# Best params for this search:\n",
    "# {'min_child_samples': 41, 'min_child_weight': 6.2363042875116852, 'num_leaves': 123}\n",
    "# Best AUC for these params:\n",
    "# 0.674730830726\n",
    "\n",
    "# ---\n",
    "\n",
    "# Use entire sample now.\n",
    "# Try finding if ANY L2 reg is desirable.\n",
    "# Also tune min_data_in_bin (how many obs must be in each of 63 bins per feature)\n",
    "# Also tune max_depth (though may just be -1 = unlimited, and we're already limited by num_leaves)\n",
    "\n",
    "# Raw grid search scores:\n",
    "# Best params for this search:\n",
    "# {'max_depth': 11, 'min_data_in_bin': 3, 'reg_lambda': 0.1}\n",
    "# Best AUC for these params:\n",
    "# 0.672165523869\n",
    "\n",
    "# ---\n",
    "\n",
    "# Clearly want lower number of min_data_in_bin, high max_depth, and little-to-no l2 reg.\n",
    "# Makes sense - have lots of data -> hard to overfit\n",
    "# Fine-tune\n",
    "\n",
    "param_dists = {\n",
    "    # 'num_leaves': scipy.stats.randint(100, 127),\n",
    "    # 'min_child_weight': scipy.stats.uniform(2, 40),  # from 2 to 2+10\n",
    "    # 'min_child_samples': scipy.stats.randint(10, 80),\n",
    "    #'subsample': scipy.stats.uniform(.6, .25),\n",
    "    # subsample_freq=1,\n",
    "    # 'colsample_bytree': scipy.stats.uniform(.6, .25),\n",
    "    # reg_alpha=0,\n",
    "    'reg_lambda': [0.0, 0.10, 0.15, 0.20, .25, .30, .40],  # try using very small amt of L2 reg\n",
    "    'min_data_in_bin': scipy.stats.randint(1, 4), \n",
    "    'max_depth': [12, 13, 14, 15, 16, 17, 18, -1]\n",
    "}\n",
    "\n",
    "test_params_random(gbm, param_dists, X_train_enc, Y_train_enc, n_iter=40, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "# first run:\n",
    "# Raw grid search scores:\n",
    "# Best params for this search:\n",
    "# {'max_depth': 15, 'min_data_in_bin': 1, 'reg_lambda': 0.2}\n",
    "# Best AUC for these params:\n",
    "# 0.674529497263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: CLEAN UP above random search :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's rmse: 0.402503\tcv_agg's auc: 0.78138\n",
      "[200]\tcv_agg's rmse: 0.380142\tcv_agg's auc: 0.789801\n",
      "[300]\tcv_agg's rmse: 0.372706\tcv_agg's auc: 0.797537\n",
      "[400]\tcv_agg's rmse: 0.368524\tcv_agg's auc: 0.80358\n",
      "[500]\tcv_agg's rmse: 0.365974\tcv_agg's auc: 0.80826\n",
      "[600]\tcv_agg's rmse: 0.364149\tcv_agg's auc: 0.811478\n",
      "[700]\tcv_agg's rmse: 0.362806\tcv_agg's auc: 0.814102\n",
      "[800]\tcv_agg's rmse: 0.361837\tcv_agg's auc: 0.815976\n",
      "[900]\tcv_agg's rmse: 0.360906\tcv_agg's auc: 0.817905\n",
      "[1000]\tcv_agg's rmse: 0.36018\tcv_agg's auc: 0.819351\n",
      "[1100]\tcv_agg's rmse: 0.359573\tcv_agg's auc: 0.820524\n",
      "[1200]\tcv_agg's rmse: 0.359019\tcv_agg's auc: 0.821602\n",
      "[1300]\tcv_agg's rmse: 0.358566\tcv_agg's auc: 0.822431\n",
      "[1400]\tcv_agg's rmse: 0.357991\tcv_agg's auc: 0.823639\n",
      "[1500]\tcv_agg's rmse: 0.357601\tcv_agg's auc: 0.824364\n",
      "[1600]\tcv_agg's rmse: 0.357254\tcv_agg's auc: 0.824962\n",
      "[1700]\tcv_agg's rmse: 0.356817\tcv_agg's auc: 0.825834\n",
      "[1800]\tcv_agg's rmse: 0.356524\tcv_agg's auc: 0.82634\n",
      "[1900]\tcv_agg's rmse: 0.356223\tcv_agg's auc: 0.826907\n",
      "[2000]\tcv_agg's rmse: 0.355986\tcv_agg's auc: 0.827308\n",
      "[2100]\tcv_agg's rmse: 0.355693\tcv_agg's auc: 0.827866\n",
      "[2200]\tcv_agg's rmse: 0.355461\tcv_agg's auc: 0.828257\n",
      "[2300]\tcv_agg's rmse: 0.355234\tcv_agg's auc: 0.828636\n",
      "[2400]\tcv_agg's rmse: 0.355097\tcv_agg's auc: 0.828812\n",
      "[2500]\tcv_agg's rmse: 0.354891\tcv_agg's auc: 0.829164\n",
      "[2600]\tcv_agg's rmse: 0.354721\tcv_agg's auc: 0.829447\n",
      "[2700]\tcv_agg's rmse: 0.354519\tcv_agg's auc: 0.829806\n",
      "Num rds from 10-fold CV, lrate =.01 == 2778.\n"
     ]
    }
   ],
   "source": [
    "# Now that params are tuned, let's see how well we can do on CV \n",
    "# using a lower lrate of .01 (from .1).\n",
    "# New seed so we aren't overfitting on the particular slices from param optimization.\n",
    "\n",
    "params3 = {\n",
    "    'objective': 'binary',\n",
    "    'num_leaves': 127,\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': 17,\n",
    "    'max_bin': 63,\n",
    "    'min_split_gain': 0,\n",
    "    'min_child_weight': 6.5,\n",
    "    'min_child_samples': 40,\n",
    "    'subsample': .76,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': .73,\n",
    "    'bin_construct_sample_cnt': 999999, # use entire dataset\n",
    "    'min_data_in_bin': 1,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 0.1,\n",
    "    'seed': 0,\n",
    "    'nthread': -1,\n",
    "}\n",
    "\n",
    "cv_result = lgb.cv(\n",
    "params3, \n",
    "lgb_train, \n",
    "nfold=10,\n",
    "# stratified=True,\n",
    "num_boost_round=5000, \n",
    "early_stopping_rounds=20,\n",
    "metrics=['auc','rmse'],\n",
    "verbose_eval=100, \n",
    "show_stdv=False,\n",
    "seed=2002\n",
    ")\n",
    "\n",
    "print('Num rds from 10-fold CV, lrate =.01 == %i.' % len(cv_result['auc-mean']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's rmse: 0.402457\tcv_agg's auc: 0.781738\n",
      "[200]\tcv_agg's rmse: 0.380053\tcv_agg's auc: 0.79018\n",
      "[300]\tcv_agg's rmse: 0.372578\tcv_agg's auc: 0.798026\n",
      "[400]\tcv_agg's rmse: 0.368402\tcv_agg's auc: 0.803908\n",
      "[500]\tcv_agg's rmse: 0.365867\tcv_agg's auc: 0.808466\n",
      "[600]\tcv_agg's rmse: 0.36403\tcv_agg's auc: 0.811831\n",
      "[700]\tcv_agg's rmse: 0.362653\tcv_agg's auc: 0.814554\n",
      "[800]\tcv_agg's rmse: 0.361719\tcv_agg's auc: 0.816314\n",
      "[900]\tcv_agg's rmse: 0.360773\tcv_agg's auc: 0.818253\n",
      "[1000]\tcv_agg's rmse: 0.360013\tcv_agg's auc: 0.819767\n",
      "[1100]\tcv_agg's rmse: 0.359428\tcv_agg's auc: 0.820871\n",
      "[1200]\tcv_agg's rmse: 0.358867\tcv_agg's auc: 0.821946\n",
      "[1300]\tcv_agg's rmse: 0.358408\tcv_agg's auc: 0.822776\n",
      "[1400]\tcv_agg's rmse: 0.357832\tcv_agg's auc: 0.823953\n",
      "[1500]\tcv_agg's rmse: 0.357441\tcv_agg's auc: 0.824663\n",
      "[1600]\tcv_agg's rmse: 0.357081\tcv_agg's auc: 0.825315\n",
      "[1700]\tcv_agg's rmse: 0.356663\tcv_agg's auc: 0.826133\n",
      "[1800]\tcv_agg's rmse: 0.356355\tcv_agg's auc: 0.826681\n",
      "[1900]\tcv_agg's rmse: 0.356038\tcv_agg's auc: 0.827234\n",
      "[2000]\tcv_agg's rmse: 0.355757\tcv_agg's auc: 0.827749\n",
      "[2100]\tcv_agg's rmse: 0.355473\tcv_agg's auc: 0.828281\n",
      "{'rmse-mean': [0.49802825097685866, 0.49624278603207123, 0.4944831774101292, 0.49269142440304819, 0.49084047741255388, 0.48910070129757333, 0.487292125662867, 0.4856059376603265, 0.48386268787083131, 0.48228347115526587, 0.48098543776433916, 0.47947297017422885, 0.47787978828883021, 0.47654240184902913, 0.47507615051318741, 0.47346810304873743, 0.47192445071896583, 0.4705270913831372, 0.46907405802935315, 0.46762891685928487, 0.46618674620692274, 0.46474600000545169, 0.46329156924217668, 0.46189875953256132, 0.46057433044776452, 0.45933188464072705, 0.45808430789703103, 0.45679591863686292, 0.45543188784565947, 0.45415816838220352, 0.45302597001101946, 0.45189384027419388, 0.45078999668670683, 0.44956798481837251, 0.4483835752112405, 0.44721702837647248, 0.44607354710389613, 0.44495400289574966, 0.44385942732638711, 0.44274888184337646, 0.44170564300379678, 0.44067540201493288, 0.43963677495104109, 0.43860877224072647, 0.43760635357534267, 0.43660040762370605, 0.43563782455752093, 0.43465797840617071, 0.43372492171492161, 0.43277865537278604, 0.43193888134642167, 0.43104095952423266, 0.43014361550875496, 0.42935762179358627, 0.42845623739119337, 0.42762252204886142, 0.42687389732566705, 0.42607363679650845, 0.42547915972844824, 0.42486775633053658, 0.42407163906829892, 0.42338891173186138, 0.42261933085254738, 0.42186456972977587, 0.42120394772257103, 0.42065359771061778, 0.41995440021992725, 0.41932885607308651, 0.41870699285875579, 0.41809535588726082, 0.41742487961927904, 0.4167433249334036, 0.41608398635551563, 0.41544612982668705, 0.41482241578338153, 0.41427075776055161, 0.41360936892949862, 0.41308654874561557, 0.41249955768315594, 0.411910699119655, 0.41134632278341332, 0.41083831132161996, 0.41035460087618525, 0.40980942667748871, 0.40932433682698949, 0.40886964845868884, 0.40830538896081991, 0.40785408206581258, 0.40734476033348088, 0.40691382346479699, 0.406406869720911, 0.40591472908445869, 0.40549262157016619, 0.40500577630187673, 0.4045184197694125, 0.4041914822939357, 0.40374038983530047, 0.40328168344438992, 0.40283228162921969, 0.40245734269140787, 0.40202190289868217, 0.40162322866129541, 0.40122259005716776, 0.40083330978768306, 0.40041215718271123, 0.39999554011651289, 0.39961345989645453, 0.39920345569310356, 0.39881826118537145, 0.39849135130336083, 0.39823958527330644, 0.39784786786698512, 0.39749831732504831, 0.39713615254700718, 0.39678872009820271, 0.3963934245036318, 0.39605705953872661, 0.39571889118196585, 0.39539727473944908, 0.39506409796872438, 0.3947363616463534, 0.39442710466404785, 0.3941139509535988, 0.39383462157141447, 0.39353509430395156, 0.39315681115045914, 0.39289588994742408, 0.39260851623442461, 0.3923257991191223, 0.39205334521313023, 0.39175771539672805, 0.3915072849977923, 0.39116689920297387, 0.39090083060145675, 0.39064986870874802, 0.3904025344018871, 0.39014963801465141, 0.38993029109156913, 0.38971381726594573, 0.38946885761655853, 0.38923234559524955, 0.38900282526331231, 0.38886228251911842, 0.38866334589671159, 0.38852466159949134, 0.38829519107523713, 0.38806593252594074, 0.38782611145587048, 0.38763671900788571, 0.38741349537772346, 0.38720903204831458, 0.38700493014371518, 0.386833462406036, 0.38664305342512456, 0.38652752817677227, 0.38633270320871987, 0.38613819790981585, 0.38595414359753433, 0.3857908334594663, 0.38562733639843316, 0.38543764398970021, 0.38533662900440185, 0.38516992285105944, 0.38498360884948962, 0.38480541559229225, 0.3846475640533174, 0.38449783324692499, 0.38433711137262278, 0.38418164389957082, 0.38409008595983291, 0.38395346967425975, 0.3837864103270433, 0.38364291932027933, 0.38347784479619162, 0.38334077564503455, 0.38320627604909291, 0.38307360314011241, 0.38290003116711874, 0.38273896032649585, 0.38266505620283492, 0.38248838898787724, 0.38232867122946385, 0.38220099338293306, 0.38205774569691608, 0.3819056961755134, 0.38176437490058124, 0.38164429684870849, 0.38149986540299774, 0.38137191290146788, 0.38124232888862924, 0.38111938261190853, 0.38097100247506599, 0.38083040927596373, 0.38071375714524386, 0.38058582824985471, 0.38046312372724145, 0.38033391382579512, 0.38020348370068513, 0.38010691022632892, 0.38005271903458704, 0.38000305479071578, 0.37989737671721985, 0.37978227408143417, 0.37967873995672907, 0.37957171276581742, 0.37952354461047777, 0.37941301114471704, 0.37929648772627184, 0.37918252245555689, 0.37908092633657064, 0.37897218004831684, 0.37886853851417202, 0.37878146627782155, 0.37866779183187155, 0.37856107602267153, 0.37851710648870501, 0.37840773676187489, 0.37830953240608584, 0.37821379943567968, 0.37817420417538161, 0.37807272667163533, 0.37797874908557227, 0.37788497074148764, 0.37779368380378553, 0.37769822782913054, 0.37766042391638838, 0.37757227061736043, 0.37747754273277712, 0.37738503024244474, 0.37726504364560226, 0.3771756606743914, 0.37707291522482628, 0.37697688109823713, 0.3768901784419374, 0.37680445944369745, 0.37673396483813015, 0.37665451089793434, 0.37657387005201626, 0.37649721682364529, 0.37646563245718984, 0.37634762827496365, 0.37631823759246846, 0.37624007587436004, 0.37616627070135628, 0.37607954819151235, 0.37600327443823317, 0.37592796414841301, 0.37585452546459897, 0.37582537896393337, 0.37568689186056059, 0.37565887493264516, 0.37558769338994236, 0.37550146157178366, 0.37543731792020746, 0.37536785395843297, 0.37529758374276878, 0.37516668431348288, 0.37503735373327485, 0.37496789741768249, 0.37490691545230809, 0.37483493973025772, 0.37476815407136854, 0.37470452253606046, 0.37463790306168099, 0.37455432134103239, 0.3744825636436615, 0.37445884061230073, 0.37439381266158428, 0.37433398770725806, 0.37426317765989708, 0.37420025356680969, 0.37414005306948955, 0.37408094871566266, 0.3740283785063947, 0.37397516229105465, 0.37395227502844064, 0.37386533300363067, 0.37380264827739823, 0.37372060518927269, 0.37366120803552999, 0.3736414033634089, 0.3735560459820928, 0.37350511355455235, 0.3734418984995499, 0.37333627401586411, 0.37327204884458642, 0.37320113208081762, 0.37314207557436563, 0.37308400445292245, 0.37306350959603585, 0.373006320482062, 0.37298785700908488, 0.372921481553248, 0.37286950668910135, 0.37280699069551709, 0.37278796867607594, 0.37273122180268786, 0.37268077725255389, 0.3726331081981204, 0.37257800937151997, 0.37252332938149169, 0.37247492307160501, 0.3724225674428322, 0.37237336794865417, 0.37232756571791431, 0.37225351640247417, 0.3721987330615073, 0.37214669268924205, 0.37209921171614802, 0.37205395020048587, 0.37200634625144269, 0.37194080255048562, 0.37190106200289935, 0.37186068593724408, 0.37181254790350088, 0.37176162599643348, 0.37171958972212793, 0.37167019578103888, 0.37162174815253335, 0.37154349508180634, 0.37149571515857688, 0.37146162647116265, 0.37144559324843568, 0.37137029707005925, 0.37132389041479985, 0.37130774960840085, 0.37126998120075749, 0.37120877438589372, 0.37116291713946858, 0.37112632523896755, 0.3710740056359676, 0.37103249100535696, 0.37097645999593326, 0.37091204223213159, 0.37089804401482473, 0.37085296786538535, 0.37081392487724596, 0.37077477901205275, 0.37073122368032518, 0.37069470067708399, 0.37065595400105705, 0.37061746290493219, 0.37058121952896272, 0.37054418972494918, 0.37047096435544324, 0.37042670707925957, 0.37041280472719318, 0.37037157379362207, 0.37036083895863264, 0.37028581619439865, 0.37025111561762014, 0.37021903727703787, 0.37015209897207618, 0.37008800248381679, 0.3700456868413628, 0.37000733237962774, 0.3699767323078183, 0.36993595268683455, 0.36992327485413024, 0.3699095107408073, 0.36989717793730331, 0.36986127980414168, 0.36982609751300993, 0.36979210092457138, 0.36972215538300329, 0.36967749415279644, 0.36964367780889035, 0.3695960117129482, 0.369564517587749, 0.36952711004944427, 0.3694960391811728, 0.36946357583645972, 0.36942337588925578, 0.36936859169250491, 0.36931061649306868, 0.36927986296430643, 0.36922758272680245, 0.36916806049102668, 0.36914068241420417, 0.36910691977685051, 0.36906434094405371, 0.36902947164175115, 0.36899820711642878, 0.36893593554597298, 0.36890689662069837, 0.36884639291744847, 0.36881233477449893, 0.36878549641493868, 0.36875851413626465, 0.36870103023349909, 0.36866020396756005, 0.36862962060492316, 0.36859711383596677, 0.36856882565123311, 0.36853210499446093, 0.36850030926140853, 0.36847677929552647, 0.36844877874546189, 0.36844008565377862, 0.36840171349655088, 0.36836579694823313, 0.3683411226229234, 0.36830420619675336, 0.36825159391837753, 0.36822565299927446, 0.36817086682236305, 0.36814351376716059, 0.36811751635095552, 0.36808209991293711, 0.36805540439582696, 0.36802898619720742, 0.36799308797030017, 0.36794733641568672, 0.36792629384978393, 0.36789658506191153, 0.36786862139868115, 0.36784503123562751, 0.36782371287927995, 0.36777957986237453, 0.36775168600018249, 0.36770859582275561, 0.36768408827213916, 0.36765769925659741, 0.36763462493746879, 0.36761274135186489, 0.36759216899513891, 0.36756927451784904, 0.36754306574159196, 0.36753348607742381, 0.36750528709722735, 0.36748223850821937, 0.36745564624349047, 0.36742887587832218, 0.3673970653626869, 0.36736743788324111, 0.36734008360922876, 0.36731564615392659, 0.36728169642769865, 0.36725672364380163, 0.36722712220368853, 0.36720380836037014, 0.36716297685415772, 0.36714168966042171, 0.36710711573307281, 0.36708761413871072, 0.36706211039430714, 0.36703854683577608, 0.36701718247779674, 0.36699429226981028, 0.36695633632696323, 0.36694637174344458, 0.36692426708645931, 0.36689867706768364, 0.36687150339923491, 0.36684608127222684, 0.3668206211611153, 0.36678682258108236, 0.36676522510477777, 0.36674532427841122, 0.36671803394319291, 0.36669467296484148, 0.36665885019126254, 0.36664005641933672, 0.36661159950839212, 0.36659039686950634, 0.36658341271875283, 0.36657470976493067, 0.3665495930690551, 0.36652222985105543, 0.36648702557693225, 0.3664552032931484, 0.36643633464790909, 0.36641301635219659, 0.3663865001940918, 0.36635208608386616, 0.36632986350901026, 0.36630795333251492, 0.36628225732928199, 0.36627447078263736, 0.36626861940046057, 0.36624713019309907, 0.36622801849651315, 0.36621384183803429, 0.36619163839076158, 0.36618386522659874, 0.36616015444655053, 0.36613833508352445, 0.36612090816711013, 0.36609704743885763, 0.36607422037285314, 0.36604707014832871, 0.36602953935862004, 0.36600606097825167, 0.3659887197031283, 0.36596551894856477, 0.36594609134942535, 0.3659373728900509, 0.36591614763370511, 0.36588599992014509, 0.36586731934462591, 0.36584435155309242, 0.36581221935529595, 0.36578813302015639, 0.36576934714135328, 0.36575235806199591, 0.36573203644245633, 0.36569941633490655, 0.36569119421524943, 0.36566401981741026, 0.36564466249716315, 0.36561548587870951, 0.36559381965571575, 0.3655732304719298, 0.36555179019630168, 0.36553206366800001, 0.3655166131212973, 0.36549565628236269, 0.36547782295779824, 0.36545051656005306, 0.36542143991709108, 0.36539897051754427, 0.36538305946068755, 0.36536425239714504, 0.36534521422733451, 0.3653388210231075, 0.36531226044692727, 0.36529397922299595, 0.36527820758093149, 0.36525557891974858, 0.36522718270966376, 0.36520222153500664, 0.36517767545789803, 0.3651564785124341, 0.36512579002061157, 0.36510995343629971, 0.36509023605981517, 0.36507279331322862, 0.36504319466056467, 0.36502729940969642, 0.36500940681848582, 0.36499195950564284, 0.36497571894563885, 0.36495520566439665, 0.36494144369013998, 0.36493028056058496, 0.36490471756960913, 0.36489149994323916, 0.3648847885887902, 0.36486733191069065, 0.36486094884533282, 0.36485524065246039, 0.36484127812742767, 0.36482681020304786, 0.36481103286751276, 0.36479335024210152, 0.36478101400445562, 0.36476749445682288, 0.36475193520091431, 0.36473388481281377, 0.36469391841230514, 0.3646802591110388, 0.36466811242397612, 0.36465337110262841, 0.36463009325759438, 0.36461492699423148, 0.36460085791478447, 0.36457965312165397, 0.36456251403433571, 0.36454306914118123, 0.36452851832892175, 0.36450070592141964, 0.36446323341478415, 0.36444448459960188, 0.3644296169979645, 0.3644134037556529, 0.36439646334045661, 0.36437837216404445, 0.36435554756009442, 0.36433943144284592, 0.36432357314244734, 0.36430455276152618, 0.36428450195177087, 0.36426792304520028, 0.3642561167618526, 0.36424363706894375, 0.36422777792015681, 0.3642214291397321, 0.36420117148866893, 0.36418214453852221, 0.36416641440247965, 0.36415305094663064, 0.36414095242938616, 0.36412598848301159, 0.36411430257350169, 0.36409475086590037, 0.36407370085350221, 0.36406691580913508, 0.3640515840298173, 0.36404152226560182, 0.36402972493056374, 0.36402237637035578, 0.36400962540916409, 0.36399481198346806, 0.36397848290444101, 0.36397167245918105, 0.36395731828678202, 0.36394794770152894, 0.36393546238621477, 0.36391444196644573, 0.36389687716086144, 0.36386855847153549, 0.36384805268553705, 0.36383432814687994, 0.36381829903413399, 0.36380699631259211, 0.36378601583449083, 0.36377453464562393, 0.36376064360368082, 0.36373719613897526, 0.36371283268883092, 0.36369578252414797, 0.36367875162814084, 0.36366062872506977, 0.36363704368412508, 0.36362696050566046, 0.36360623264140884, 0.36359339823178322, 0.3635675077346664, 0.36355694190456073, 0.36354461422514595, 0.36353406549808354, 0.36352454062293216, 0.36351288566908579, 0.36349162506491062, 0.36348074673260966, 0.36346923970290651, 0.36343692985431408, 0.36342502587948045, 0.36340544215327131, 0.36339413603899035, 0.36338121134165002, 0.36336480778342273, 0.36335328108174086, 0.36333496943942822, 0.36332609653216802, 0.36331646253404432, 0.36330392186249916, 0.36328754053441686, 0.36327344801530875, 0.36326252634489953, 0.36325553099984809, 0.36324681001681358, 0.36323586506164368, 0.36321366934564331, 0.3632023908434211, 0.36319213368523867, 0.36317916319039156, 0.36316400350648709, 0.36314728120379114, 0.36313907650374844, 0.36312703231369253, 0.36311402376294488, 0.36310558093543482, 0.36309548787929946, 0.36307588322602247, 0.36306488637529399, 0.36304707711828338, 0.36303394563363822, 0.36302277166380137, 0.36300933346678443, 0.36299260979372833, 0.36298259769377339, 0.36297311846688829, 0.36296063032862602, 0.36294653910799096, 0.36293524089509605, 0.36291959319627592, 0.36290879663616915, 0.36288775291286812, 0.36287726293536726, 0.36286576902824408, 0.36285649170495393, 0.36284697034356128, 0.36283972319948654, 0.36281137138065095, 0.36279970075919354, 0.3627876239683091, 0.36277414073060521, 0.36276107013703185, 0.36275187026782835, 0.36274796009807536, 0.36274030933889712, 0.36273018368536153, 0.36272078779328737, 0.3627143315139304, 0.36270500780079101, 0.36269311581021496, 0.36268435352113504, 0.36266330314548423, 0.36265286908065597, 0.36264242301418614, 0.36263103467117203, 0.3626269696738243, 0.3626164610739494, 0.36260936710361402, 0.36260385930306022, 0.36259473279879084, 0.36257982395662325, 0.36254725235580326, 0.36253801850495226, 0.36253337362838028, 0.36252515781381467, 0.36252166401627173, 0.36251450316463851, 0.36251152731866615, 0.36250137579423408, 0.36249230434905944, 0.36248602106515815, 0.36247709669001277, 0.36247039406553122, 0.36245585848187223, 0.36244604478429471, 0.36242843985695533, 0.36242369245730421, 0.36241713717415275, 0.36240647993297453, 0.36239619520020011, 0.36238599369618035, 0.36237795933757844, 0.36236614597094463, 0.36235519747546474, 0.3623385223268486, 0.36233197006791018, 0.36232015410137319, 0.36231183248365817, 0.36230341170673075, 0.36229606699701888, 0.36228500222878479, 0.36227858242660671, 0.36226636171607285, 0.36225011018959707, 0.36223973135364906, 0.3622258740663345, 0.36221695004700438, 0.36221101173997527, 0.36220214488754221, 0.36219678848104037, 0.36218834484161372, 0.36218052155472497, 0.36215953630759151, 0.36212931870152121, 0.36211789863326882, 0.36210953716468797, 0.36210654012765586, 0.36208968241189482, 0.36208139256024741, 0.36207431441524335, 0.36206373910265038, 0.36206047960420401, 0.36205113459843674, 0.36204294698858519, 0.36203568753850168, 0.3620281460855922, 0.36201895170175269, 0.36201294329193623, 0.36200379764960733, 0.36199119875346458, 0.3619844308909882, 0.36197640643149193, 0.36196814606861583, 0.36196375723935093, 0.36195268577923351, 0.36194517270919441, 0.36193973000898555, 0.36193424602907198, 0.36192492897449302, 0.36191768388214052, 0.36191211845327331, 0.36189999086949504, 0.3618932943234841, 0.36188599384100545, 0.36187778515381563, 0.361868309658464, 0.36185992482962676, 0.36185359142358231, 0.36184671878449454, 0.36184004369105244, 0.36183405295914378, 0.36182437700766867, 0.36180603314891319, 0.3618026712986836, 0.36179421417100838, 0.36178416702092736, 0.36177426244257133, 0.36176580078245607, 0.36176148883858095, 0.36175140919246451, 0.36173472222351677, 0.36172701884065434, 0.36171893211033929, 0.36170926673689852, 0.36169091066117731, 0.36168382472973937, 0.36167131983293377, 0.36165964948011942, 0.36164689252932913, 0.36163596625190059, 0.36162910955219946, 0.36160379751416505, 0.36159530781608329, 0.36158823377693583, 0.36158330223957458, 0.36157447758825279, 0.36157050633603677, 0.36156033044838176, 0.3615451010860512, 0.36153979830996963, 0.36153223246347593, 0.36152537544000718, 0.36150858613716308, 0.36149814569542188, 0.36148996595585575, 0.36147551072791922, 0.36144556637212005, 0.3614291494695881, 0.36141965969718287, 0.36141356729982543, 0.36140988822783848, 0.36140342204180509, 0.36139537787995768, 0.36138889793545687, 0.36138169429946249, 0.36137762900708192, 0.36136730310653614, 0.3613582399125943, 0.36134716030875264, 0.3613391872987809, 0.36133296471142334, 0.36131666928769535, 0.36131169641041944, 0.36130306258507089, 0.3612984514181391, 0.36129518025947005, 0.36129320049508845, 0.3612883823525721, 0.36128462447371856, 0.36128023663158082, 0.36127401810594828, 0.36125486373442284, 0.36124808146012638, 0.36123288418645771, 0.36122598829869312, 0.36121724101793939, 0.36120064733021501, 0.36119505907726246, 0.36118645080293144, 0.36117908789675485, 0.36117248731356516, 0.36115207555918827, 0.36114490262379401, 0.36113294223578429, 0.36112198920123983, 0.36111762550805121, 0.36111105825293499, 0.36110612286675031, 0.36108549403588303, 0.36107806166149975, 0.36107021648048859, 0.36106075546233896, 0.36105395431780207, 0.36104650063164351, 0.36103229226122951, 0.3610238351153458, 0.3610165055642377, 0.3610079030454339, 0.36100095195587462, 0.36099519121416651, 0.36098319408533852, 0.36097620464367725, 0.36096748954515612, 0.36095958110592952, 0.36095561845318402, 0.36094694109077896, 0.36093821440937623, 0.36091638853027186, 0.3608965222601267, 0.36089208006056317, 0.3608845616190749, 0.36087852025517286, 0.36085093342984847, 0.36084436593553942, 0.36083912622472331, 0.36083578042171582, 0.36083023092366256, 0.36080379501003196, 0.36079790171345627, 0.36078515858075699, 0.36077966980190862, 0.36077462696004403, 0.36077299276769492, 0.3607667810025168, 0.36075891960690504, 0.36075052257300927, 0.36074420770096155, 0.36073282399656492, 0.36072181282137405, 0.36069648758373646, 0.36067616828627475, 0.36067080242291266, 0.36066509755290066, 0.36064772058526073, 0.36064118605276774, 0.36063518646567028, 0.36063038460265989, 0.36062369139255146, 0.36060377434024204, 0.36059683908232992, 0.36059269764748231, 0.3605883014429398, 0.36057676123377552, 0.36056950797290332, 0.36056424259502368, 0.36056212273148525, 0.36055971991874891, 0.36055059966326064, 0.36054580686245452, 0.36053881792174192, 0.36053333177426949, 0.36052879480561389, 0.36051822151384327, 0.36049693588924664, 0.36048956143143301, 0.3604846396456764, 0.36047939017161379, 0.36047385634652784, 0.36046347030055098, 0.36046210335774165, 0.36045437850923934, 0.36043151528540751, 0.36042924551368893, 0.36042460576416546, 0.36041975977860857, 0.36041419979173428, 0.3604074261233276, 0.36040695149830554, 0.36039380590726772, 0.36038786589716371, 0.3603754529835248, 0.36037000475725339, 0.3603601125282484, 0.36034291776037508, 0.36033764747953173, 0.36033141054088536, 0.3603253279530535, 0.36031989829644118, 0.36031420472881986, 0.36030545010560705, 0.36029929126686955, 0.3602934299513364, 0.36028578690008806, 0.36027836761549975, 0.36027329010347675, 0.36026919053303452, 0.36026474812321596, 0.36025794706681102, 0.36024523973744671, 0.3602413303529447, 0.36023205613777221, 0.36021999532157167, 0.36021377765718271, 0.3602090126448142, 0.36019232771864107, 0.36017550398288456, 0.36017356608250173, 0.36016608867300992, 0.36016433778702017, 0.36016121754269748, 0.3601557008287572, 0.36015291104329211, 0.36014807286325218, 0.36014268303752794, 0.36013548007372137, 0.36012885197983158, 0.36012494222450953, 0.36012141133717057, 0.36011610451050313, 0.36011231176419317, 0.36010766774153763, 0.36010148921817126, 0.36009600601754072, 0.36008950199211842, 0.3600856085894234, 0.36007585105095774, 0.3600652362628779, 0.36005328532211844, 0.36004278564791142, 0.3600358544981721, 0.36002490779534829, 0.36002013529777427, 0.36001341055835911, 0.36000936761560376, 0.36000664592256654, 0.35999999755837697, 0.35999107302482275, 0.35998599152708421, 0.35998395611607659, 0.35997831611748304, 0.35996575778812112, 0.35995849637148825, 0.3599526118821551, 0.35993764801054162, 0.35993543689191043, 0.35992973374988957, 0.35992024216460372, 0.35991516550980951, 0.35990332831301242, 0.35989796086796, 0.35989413563019124, 0.35988960852505636, 0.35988594336149909, 0.35988412631368655, 0.35988176162914842, 0.35987096563677634, 0.35986161555669388, 0.35985715046669109, 0.35985090931243791, 0.35984835333036297, 0.35984375759008302, 0.35983825957533466, 0.35983516310342967, 0.35983072691860502, 0.35982791896741551, 0.35982101317148019, 0.3598148943445893, 0.35980813947450108, 0.35980050227468452, 0.35979458879833881, 0.35979107662206333, 0.35978743330614282, 0.35978433329711557, 0.35978180668525261, 0.35977770733527376, 0.35977090399796741, 0.35976654872190356, 0.35975894532571667, 0.3597537023024785, 0.35974691667836683, 0.35974346041904348, 0.35973810546744095, 0.35973583695575462, 0.35973150212447197, 0.35971976608646306, 0.35971489064828199, 0.35970902507734087, 0.35970331852983956, 0.35970159347735342, 0.35969444052687705, 0.35969018622290572, 0.35968183851123903, 0.35967834485348932, 0.35967432373386615, 0.35966685420250988, 0.35966425794842466, 0.35965962857150496, 0.35965163767923264, 0.35964707832283815, 0.35964424337851503, 0.35963909774642333, 0.35963514720582934, 0.35963231533536788, 0.35962819092189979, 0.35961791357594353, 0.35961397804851108, 0.35958636237348462, 0.35958220333534774, 0.3595776517270966, 0.35957038322467783, 0.35955954467224166, 0.35954271900201185, 0.35953806442558911, 0.35953483639598527, 0.35952622300029324, 0.35951681747056757, 0.35951354260174073, 0.35950922922472589, 0.35950440596282168, 0.35949857228564064, 0.35947875179551736, 0.35947480228389905, 0.35947323351951355, 0.35946831146314751, 0.35945792361991863, 0.3594551371550514, 0.35945094707758235, 0.3594479919765079, 0.35943948991497765, 0.35943564303167508, 0.35943092308342323, 0.35942860769901858, 0.35942764702303898, 0.35942267229541752, 0.35941804377603226, 0.35941609701128802, 0.35941237636551016, 0.35940838628263999, 0.35940150104830526, 0.35939659757320536, 0.35938592466165192, 0.35937633900363253, 0.3593631310681516, 0.3593594258384134, 0.3593554214249447, 0.35935129019911261, 0.35934145537477447, 0.35933652246644054, 0.35932709416261216, 0.35932189437440953, 0.35931874978201706, 0.35931228914735158, 0.35930695873793794, 0.35930180253400762, 0.35928729505606272, 0.3592805556406401, 0.35925888891865754, 0.35924394584016101, 0.35924053869370576, 0.35923853398615024, 0.3592267919984688, 0.35922336901893176, 0.35921842745302457, 0.35921071659900777, 0.35920769990418061, 0.35919602056283234, 0.3591928623223416, 0.3591879691980237, 0.359186234780491, 0.3591845856968775, 0.35917885237973329, 0.35915987281206035, 0.35915440592117726, 0.3591526427851669, 0.35914896114825201, 0.35914726770059263, 0.35914042374103355, 0.35913423848675563, 0.3591314307830894, 0.35912003665251019, 0.35911549440146839, 0.35911195756491854, 0.35910410610115873, 0.35909987667124971, 0.35909256854495097, 0.35908244412554524, 0.35907741230031454, 0.35907587639047556, 0.35907355874469982, 0.35906520965849664, 0.3590632307310761, 0.35905806296707621, 0.35905130534964569, 0.35904733173623166, 0.35904546556071903, 0.35904530820343938, 0.35904295671050562, 0.35903698687510194, 0.35903098574652692, 0.3590279432976422, 0.35902252852982908, 0.35901468620284438, 0.35901087568081452, 0.35900880671025942, 0.35900316251540554, 0.35899861187441612, 0.35898996323858257, 0.35898609891293332, 0.3589813740535, 0.35897947364920002, 0.35897695642268362, 0.35897176731500852, 0.35896645049835779, 0.35896266763742268, 0.35895379456469784, 0.35894825201535385, 0.35894182980483347, 0.35893683079974481, 0.35893236857145516, 0.35892859524125076, 0.3589238634853556, 0.35891965323116259, 0.35891508509149866, 0.35891226399772846, 0.35890491081166948, 0.3588998948164851, 0.35889692821884162, 0.35888918100910072, 0.35888779709616275, 0.35888619299195251, 0.3588834269462714, 0.35887309312352694, 0.35886669758167139, 0.35886367988915591, 0.35886161276197587, 0.35885647686203959, 0.35885441158238168, 0.35885152111079033, 0.35884724359421205, 0.35884281213603908, 0.35884196935234813, 0.3588355222353754, 0.35883322312857596, 0.35882986655392302, 0.35882479675937679, 0.35882301050651177, 0.35881295309071831, 0.35881212417590308, 0.35880439445115908, 0.35880181931114441, 0.35879779343740992, 0.35879529844849922, 0.35879076817174888, 0.35878825443070428, 0.35878573598189345, 0.35878220535520811, 0.35877706351686556, 0.35877281718782872, 0.35877007694048935, 0.35876830062793685, 0.35876333073788891, 0.35875789816808368, 0.35875253962416004, 0.35874871839734473, 0.3587473459832814, 0.35873159479998468, 0.35872961026924932, 0.35872628448726174, 0.35872346456042642, 0.35872105896571804, 0.35871698629785942, 0.35871328215039677, 0.3587112905542757, 0.35870239879114951, 0.35869919056998373, 0.35869570061192563, 0.35869225865020038, 0.35868842046979749, 0.35868820619190156, 0.35868393603990062, 0.35868112918147893, 0.35867944544594338, 0.35867049566618919, 0.35866534004283135, 0.35866262757139966, 0.35863706635761661, 0.35863294189707184, 0.3586297857834192, 0.35861091137333967, 0.3586063795843753, 0.3586039353702476, 0.35860188144693206, 0.35859792218627728, 0.3585876541354609, 0.3585834166630974, 0.35857731315972413, 0.35857394919965113, 0.35857298100232232, 0.35856814767203604, 0.35856518885636174, 0.35856266815541232, 0.35855836465616092, 0.35853223654437205, 0.35852883249501188, 0.35852403430465835, 0.35852034875961902, 0.35851874862346517, 0.35851432339072076, 0.35851335325031841, 0.35851147254594018, 0.35850664278322514, 0.35850611991165882, 0.35850197600691941, 0.35849808413477235, 0.35849660125654065, 0.35849351372659533, 0.35847382559135665, 0.35847214461834803, 0.35847146311738604, 0.35847049806502834, 0.35846612064620714, 0.35846315926298006, 0.35844975226754944, 0.35844566368259062, 0.3584371641823359, 0.35843366908500329, 0.35842421901815458, 0.35842100033566038, 0.35841759716275473, 0.35841509978929503, 0.35841301403856918, 0.35840900053196262, 0.35840752432095735, 0.35838887344021125, 0.35838684597493692, 0.35838357539861965, 0.358376003953643, 0.35837594127572686, 0.3583612857926981, 0.35835883966638327, 0.35835647098198675, 0.35835292326468648, 0.35834863153708607, 0.35833401473481269, 0.35833005787943667, 0.35832925792050063, 0.35831464720791328, 0.35831095745754438, 0.35829542567411332, 0.35828857826958072, 0.35828749494812795, 0.358283196676696, 0.35827675617057186, 0.35827340809543051, 0.35826803154161457, 0.35826014900612951, 0.35825672902094402, 0.35825526228526694, 0.35824976565649874, 0.35824272538554131, 0.35823380333402877, 0.35821707123037405, 0.35821140163485743, 0.35821039865272403, 0.35820709763706765, 0.35819798462276231, 0.35819706605095003, 0.35819688958913842, 0.35819394355490564, 0.35818934051664214, 0.35818367057204209, 0.35818168843739356, 0.35817432554457651, 0.3581734367479763, 0.35815732795738198, 0.35815473724838753, 0.35815138329811791, 0.3581462340325906, 0.35814434392683009, 0.35813714762928422, 0.35813536479683256, 0.35813108698218932, 0.35812409504117954, 0.35810744695486513, 0.358102299027517, 0.35809906069262765, 0.35809732552414547, 0.35809499788524091, 0.35809028275942106, 0.3580883990045694, 0.35808698093395797, 0.35808614661013394, 0.3580784249270576, 0.35807224627324008, 0.3580711284773358, 0.35806019699622044, 0.35805068523216477, 0.35804846321544709, 0.35802324492912685, 0.35801797100657817, 0.35801580065837857, 0.35800833184713243, 0.35800394316763029, 0.35799992155184907, 0.3579830983199182, 0.35798189061218227, 0.35797905490921961, 0.35797370858317829, 0.35797045948336492, 0.35796673951691627, 0.35796570924277704, 0.35796268533104841, 0.35795337051254317, 0.35795057613490755, 0.35794029614939404, 0.35791763152417327, 0.35791390126927147, 0.3579129135786932, 0.35791062972659332, 0.3579042286991313, 0.35790206989454221, 0.35790151197233111, 0.35788671208814032, 0.35788471482488199, 0.35788282068657262, 0.35786873049937545, 0.35786690317826958, 0.35785262711061838, 0.35784996337207853, 0.35784160393147491, 0.35783766764797575, 0.35783480592006323, 0.35783225164598831, 0.3578243247888514, 0.35781916426806637, 0.35781889975235259, 0.35781682516430308, 0.35781397435916573, 0.35780649602101711, 0.35780227431361472, 0.35779249231560306, 0.3577892729106863, 0.35778128415327071, 0.35777881128571809, 0.35776968806016785, 0.35775974005294642, 0.35775795526182336, 0.35775655488989544, 0.35775466067978201, 0.35775264272026258, 0.35775132373117075, 0.35774905024885267, 0.35774671358346671, 0.35774581891022206, 0.35773535682545893, 0.35773185702585708, 0.35772708820334803, 0.35772657997782464, 0.35771957762352102, 0.35771865079538068, 0.35771573537821777, 0.35771214506812576, 0.35770558150323473, 0.35770445770717918, 0.35770014602937056, 0.35769987500807304, 0.35768503015585862, 0.35766745161337371, 0.35766600624694844, 0.35765925228930545, 0.35765234939179374, 0.35765148346760645, 0.3576478395468955, 0.35764362224837098, 0.35764204406160915, 0.35764225335427774, 0.35763989670827639, 0.3576356127936039, 0.35763290494507077, 0.35762538338398275, 0.35762232191019244, 0.35761225867069812, 0.35761016361337983, 0.35760790460659947, 0.3576057010216836, 0.3575888436351421, 0.35758071090609234, 0.35757548260339811, 0.35757403709488356, 0.35757233850810261, 0.35756575256023515, 0.3575637917192338, 0.3575618258329985, 0.35756137117036563, 0.35756081127885114, 0.3575441764054097, 0.35754109469738038, 0.357539350884576, 0.35753759033346788, 0.35753569530461693, 0.35753298900209007, 0.35753054485676339, 0.35752909240419728, 0.35752634715422005, 0.35752323519941964, 0.35752185328477798, 0.35752098256848697, 0.35752001021893448, 0.35751830300498477, 0.35751731308032142, 0.35751384398389113, 0.35751214728252223, 0.35751237340364606, 0.35751002951858085, 0.35750851721838006, 0.35750457158800092, 0.3575020375969149, 0.35749897574027678, 0.35749259381564569, 0.35749062083088928, 0.35748859007049438, 0.35748798767439005, 0.35747679237932528, 0.35746825972879548, 0.35746781847153947, 0.35745917298252172, 0.35745859992069173, 0.35745758043699077, 0.35745084641804903, 0.35744827946985636, 0.35744217380414034, 0.35744121851680427, 0.35744062386482722, 0.35743466042893512, 0.35742878774991016, 0.35742371587942934, 0.35742355601378445, 0.35741658827173572, 0.35740339850849168, 0.35740152272390124, 0.357400059795345, 0.35739764978996658, 0.35739649562699405, 0.35739057479679187, 0.3573898027193364, 0.35738592919351592, 0.35738520805850316, 0.35738062148648014, 0.35737599051642688, 0.35736807595666614, 0.3573680000381888, 0.35736031472816754, 0.35735797107122613, 0.35735757239688942, 0.35735650736310948, 0.3573533283203556, 0.35734766595509343, 0.3573384415203219, 0.35733575612395474, 0.3573344075783953, 0.35733197137757061, 0.35732875998378921, 0.35732672894029704, 0.3573180840477504, 0.35731633934392293, 0.35731396282877298, 0.35730771289300289, 0.35729704557442393, 0.35729669720834567, 0.35729101812068609, 0.35728663204476069, 0.35728425108358086, 0.3572803515607777, 0.3572730181341367, 0.35727323113390841, 0.35726810616732496, 0.3572655265489606, 0.3572633442271072, 0.35725647547592987, 0.35725326650428574, 0.35725418632246292, 0.35724656213079509, 0.35724002741668814, 0.35723757417127938, 0.35723625365775535, 0.35723518137433924, 0.3572338053255309, 0.35723238038776495, 0.35720956871860665, 0.35720580955244469, 0.35720221844968558, 0.35719677600792005, 0.35719589701722787, 0.35719355987683604, 0.35719213291500629, 0.35719272036852517, 0.35718293357147074, 0.35718122826414261, 0.35717577371214931, 0.35717656889583804, 0.35717612469905341, 0.35717481403869333, 0.35717417472563584, 0.35717293717150433, 0.35717249597987172, 0.35717217622920783, 0.35716950068456643, 0.35716363095160086, 0.35716353685239616, 0.3571614456747636, 0.35714666252361715, 0.35714335647367618, 0.35714122781118307, 0.35713669961244465, 0.35713672460049339, 0.35713542320204061, 0.35713400922097388, 0.35712831545884854, 0.35712763054739166, 0.35712754705711014, 0.35712572432136469, 0.35711654206370286, 0.35710875134873521, 0.35710689708039617, 0.35710533318131821, 0.35710453603090159, 0.35710133311141173, 0.35710028147749129, 0.3570905312061417, 0.35708813420587621, 0.35708445231489561, 0.35708288632532326, 0.35708097932879895, 0.35707779868408102, 0.35707148136474504, 0.35707011437267366, 0.35706505798450106, 0.35706347314995007, 0.35705866590282737, 0.3570566197505689, 0.35704972517781614, 0.35703387354895555, 0.35703288448163323, 0.35703074282881697, 0.35703007312047086, 0.35703046820758294, 0.35702540487290579, 0.35702461049338108, 0.35700972902359995, 0.3570082077972499, 0.35700701411931635, 0.35700599610591188, 0.35700258752177838, 0.35700171075057041, 0.3569939649351806, 0.35699173707188664, 0.35698741265736561, 0.35698629846341812, 0.356980343727202, 0.35697804895910767, 0.35697592858722971, 0.35697633275502905, 0.35697188364305449, 0.35696534697622945, 0.35696246580369412, 0.35696225432912754, 0.35695878770438305, 0.35695881096920934, 0.35695713913719196, 0.35694547952146105, 0.3569398588284306, 0.35693607045544956, 0.35692768506303785, 0.35692708045491006, 0.35692676871352552, 0.3569248196342461, 0.35690329213073457, 0.35690037096839367, 0.35690023941171184, 0.3568875352213105, 0.35688424611440339, 0.35687957079354321, 0.35687599346091126, 0.35685756185769851, 0.35685763124184466, 0.35685040028510218, 0.35684493325411493, 0.35684069138164948, 0.35683872002607858, 0.35682985688416596, 0.35681602145030616, 0.35681443876212959, 0.356801561492885, 0.35680038220096122, 0.35679946415891339, 0.35679722029735039, 0.35678564887758391, 0.35678466967685157, 0.35678481231370618, 0.35677455334812669, 0.35677363577496962, 0.35677092367522578, 0.35677038369623493, 0.35676597309969216, 0.35675800205670177, 0.35674715537511137, 0.35674038855073531, 0.35673415446272216, 0.35673387561076003, 0.35672986836563392, 0.356728996989636, 0.35672732486753767, 0.35672595098839865, 0.35671607427564511, 0.35671454706347794, 0.3567135251991706, 0.3567066364098026, 0.35670461853169688, 0.35670026544197897, 0.35670102686112781, 0.35669862200863406, 0.35669643625555364, 0.3566940401006965, 0.35669089251583197, 0.35668959038059728, 0.35668241864621864, 0.35668173071359127, 0.35667668154013188, 0.35666934473331546, 0.35666921640829019, 0.35666613894294591, 0.35666343678673573, 0.35666343478076579, 0.35666162788687289, 0.3566579308025174, 0.3566563311415003, 0.35665386030976293, 0.35665020838190192, 0.35664967098453787, 0.35664907575771326, 0.35664425700516023, 0.35663047095661804, 0.35662957665319783, 0.35661556755972168, 0.35661310027921522, 0.35661181933131364, 0.35660760279393844, 0.35660560044715278, 0.35659169208669422, 0.35658618837540401, 0.35658537505848353, 0.35658449554156257, 0.35658400011516278, 0.35656432193323961, 0.35656277601119168, 0.35656156207989448, 0.35655972815962589, 0.35655353430604653, 0.35655093080471356, 0.35654930297951887, 0.35654728064987046, 0.35654622283959531, 0.35654089540107181, 0.35653938687781489, 0.35653633668480633, 0.35653378436435051, 0.35652859738481457, 0.35652759129741629, 0.35652736150966058, 0.35652468948049804, 0.35652243702430514, 0.3565221143457446, 0.35652018284446069, 0.35652035725642495, 0.35651606538234976, 0.3565144423192434, 0.35651257556852589, 0.35651234721562386, 0.35651237803231656, 0.35650983250948742, 0.35650725456818905, 0.35650564513142763, 0.35650461202349365, 0.35649735900415269, 0.35649723747516221, 0.35649551347842506, 0.35649388408945493, 0.35649113359254087, 0.35649104970223405, 0.35648673556270033, 0.35648594852446497, 0.3564848941710218, 0.35648239113025404, 0.35648158115490236, 0.35648188448799345, 0.35647922535378174, 0.35647770775983639, 0.35647694285495835, 0.35647533313800228, 0.35647513837864581, 0.35647025916473946, 0.35646704717584138, 0.35644496544700977, 0.35644464532849934, 0.3564439500564377, 0.35643827840218756, 0.35643768226944283, 0.35642869536765215, 0.35642225941884448, 0.35642178681689063, 0.35641991265038225, 0.35642012546103657, 0.35641661834338795, 0.35641565910717682, 0.35641306535890283, 0.35641190956542579, 0.35640295512913667, 0.35640070669786456, 0.35639824958889615, 0.35638986222350916, 0.35638136634473394, 0.35637652286291177, 0.35637455440346438, 0.35637314438459333, 0.35637317655362388, 0.35637251058646147, 0.35637153016241307, 0.35637200395394508, 0.35637117125515527, 0.35636876952734131, 0.35636633347181268, 0.35635640689411946, 0.35635512953378939, 0.35634758064308619, 0.35633922839258225, 0.35633372933486362, 0.35633152556534475, 0.35632609277334232, 0.35632477757703124, 0.35631359528816786, 0.35631238058843101, 0.35631038254991565, 0.35630028555153531, 0.35629944653061379, 0.35628839406982915, 0.35628713315925042, 0.35628690965079468, 0.3562847883740447, 0.35628242579545699, 0.35628050268954725, 0.3562775151414091, 0.35627586223843449, 0.35627446674377528, 0.35627319411139807, 0.35627236177116378, 0.35627206175400378, 0.35627218745225187, 0.35626383513068027, 0.35626193175099952, 0.35626147677849385, 0.35626062683214837, 0.35625870751277994, 0.35625417548686505, 0.35624848473185289, 0.35624675042837139, 0.35624360182400883, 0.35624196175069694, 0.3562348882433003, 0.35622250494902891, 0.35622095118420077, 0.35622043887634391, 0.35620682006281762, 0.35620485703710736, 0.35620396737452226, 0.35620272184947505, 0.35619760904618009, 0.35619497819675344, 0.35619203877216093, 0.35619226287058692, 0.35619175747419385, 0.35618919916176728, 0.3561898454354937, 0.35618894708219734, 0.35618338814412442, 0.35618359683942885, 0.35617842742746353, 0.35617175295494369, 0.35617138596794717, 0.35616841122095011, 0.35616706064050507, 0.35615389006722842, 0.35615182800020179, 0.35615102264336318, 0.35614927159954901, 0.35614692358089606, 0.35614197731671726, 0.35614161775968894, 0.35613990310455634, 0.35613944589248508, 0.35613673254877787, 0.35613521773103007, 0.35612665956740486, 0.35611731192888885, 0.35611619698383523, 0.35610474672842918, 0.35609833085783171, 0.35609684912480294, 0.35609279941828909, 0.35609220484970833, 0.35609071751103344, 0.35608929209139195, 0.35608678210352823, 0.35608618880515136, 0.35608423237832704, 0.35608397392955482, 0.35608182620313017, 0.35608167102513444, 0.35608001468121586, 0.35607788194669288, 0.35607434737537286, 0.35607121669215952, 0.35606970878530941, 0.35606938419956957, 0.356054067099914, 0.35605260636862701, 0.35605195548116475, 0.35605086161374788, 0.35605221462441428, 0.35604408887943439, 0.35604245878156815, 0.35604091669245069, 0.3560387447975164, 0.35603806964877066, 0.35603629181090352, 0.35603503757148758, 0.35603182150124252, 0.35603045074076062, 0.35602931605025367, 0.35602798779768352, 0.35602727631486142, 0.3560197092914611, 0.35601830559721748, 0.3560168652367911, 0.35601268109923156, 0.35600818611134388, 0.35600468408230906, 0.35600247428408849, 0.35600218455444066, 0.35599878289394804, 0.35599294279054627, 0.35598521872053301, 0.35598375020514805, 0.35598217016333322, 0.3559809928245915, 0.355978243915679, 0.3559783764716567, 0.35597695382856853, 0.35597705990357686, 0.35597703471112052, 0.35597568160465248, 0.3559731938258261, 0.35597192995560739, 0.35597206887162891, 0.35596777185291145, 0.35596655368179958, 0.35596596624788751, 0.35595084583378755, 0.35595152257787804, 0.35595013661743952, 0.35595092459057776, 0.35595025401323888, 0.35594656527464907, 0.35594445377695583, 0.35594314175848746, 0.35594311375611265, 0.35594233106264528, 0.35594192351088549, 0.35593807487612306, 0.35593859999476907, 0.35593340724469635, 0.35593320948385254, 0.35593291801156973, 0.35593187388367392, 0.35592906005160163, 0.35592747363354083, 0.35592758247587586, 0.35592753436489694, 0.35592674003006541, 0.35592486741027907, 0.35592377830943689, 0.355923641266288, 0.35590824997154136, 0.35589384295542364, 0.35588549551206122, 0.35588186387890991, 0.35588249676557665, 0.35587340999694594, 0.35586725135946085, 0.35584709776209267, 0.35584626142560755, 0.35584492597240258, 0.35584216751240078, 0.35584103326403116, 0.3558410560859081, 0.35583997369575676, 0.35583881900503578, 0.35583718435579265, 0.35583568864252352, 0.35583476782318024, 0.35583409514533082, 0.35583195895956676, 0.35582846005898866, 0.35582448889511709, 0.35582352001787365, 0.35581630496535527, 0.3558151365128196, 0.35581385868422777, 0.35581299705151914, 0.35580661419198212, 0.35580507768964764, 0.35579903553650433, 0.35579601586352416, 0.3557912404454075, 0.35578535509604964, 0.35578525206399314, 0.35577996279845037, 0.35576839349963918, 0.35576620486454147, 0.35576662485802785, 0.35576500011746803, 0.3557641444366148, 0.35576228903708262, 0.35575700267356086, 0.3557535581176558, 0.35575241726995627, 0.35575228697764405, 0.35575110051052522, 0.35574689241387164, 0.3557473008228017, 0.35574109156842809, 0.35573872815025198, 0.35573373065796626, 0.3557340361605818, 0.35573241655389881, 0.35571029659448417, 0.35570766020786637, 0.35570832094907895, 0.35568860098210331, 0.35568915601995471, 0.35568655169407337, 0.35568610467184342, 0.35568623297329738, 0.35568259406630254, 0.35567524659248084, 0.35566972914118206, 0.35566452871218229, 0.35565311228000718, 0.35564131167333135, 0.35564165723800445, 0.35564243252293026, 0.35564233600906614, 0.35563072517847588, 0.35562710856101065, 0.35562586427703408, 0.35562666965486073, 0.35562607968815441, 0.35562575237536753, 0.35562598765167391, 0.35562574921580248, 0.35561885347348438, 0.35561741621825022, 0.35561695802182713, 0.35561077510341599, 0.35560806580886928, 0.35560310742765028, 0.35560055298515603, 0.35559909690463043, 0.35559775385595049, 0.35559568922770757, 0.35559629043944752, 0.35557752296461059, 0.35557604576803792, 0.35557562803691861, 0.35557412151200285, 0.3555711645395665, 0.35556964982324907, 0.35556877423238509, 0.35556863101826741, 0.35556962729603869, 0.35556792328123732, 0.3555657498717214, 0.35556525489855062, 0.3555628903560345, 0.35556102943315249, 0.35556039107202697, 0.35555884254967662, 0.35555384875894569, 0.35555267025184473, 0.35555236789637151, 0.35554218230703705, 0.35554081848316393, 0.35553847506618741, 0.35553233479073132, 0.35553163311412861, 0.35553212295573194, 0.35552849346925863, 0.35552878277206357, 0.35552880955625865, 0.3555273030916567, 0.35552649758067473, 0.35552412109524145, 0.3555250949033737, 0.35552383003711613, 0.35552468791357555, 0.35552029831337967, 0.35551862894345265, 0.35550564609259949, 0.35550046464763618, 0.35550086089989941, 0.35550151197999935, 0.35550159741303528, 0.35549509184352351, 0.35548735433368611, 0.35548198575623113, 0.35548138040662441, 0.35547994628964547, 0.35547903928918956, 0.35547679199247828, 0.35547620299242599, 0.35547518174545972, 0.3554741167694741, 0.3554728802083269, 0.35547310304317487, 0.35547198641974426, 0.35547200590208261, 0.35546053355079255, 0.35545939645037727, 0.35546067706995216, 0.35545931008776854, 0.35545772390319541, 0.35545055262237446, 0.35544852094894946, 0.35544767473364136, 0.35544857631044041, 0.35544679967284415, 0.35543425992564198, 0.35542816354516127, 0.35542116282808084, 0.35541924774322975, 0.35541984574270857, 0.35542215251110842, 0.35541561614771389, 0.35540792370221069, 0.35540645799954962, 0.35540655392735482, 0.35540475471868743, 0.35540543206592956, 0.35540457899741063, 0.35540379289320045, 0.35540277392929337, 0.35540039741310397, 0.3553960804759646, 0.35539474066466437, 0.3553934947310543, 0.35539342344083191, 0.35539245132411895, 0.3553903900892369, 0.35538097560080839, 0.35537902281685302, 0.35537716875946651, 0.35537340785012927, 0.35537205867660338, 0.35537001212780028, 0.35535267595738029, 0.35535171672493793, 0.35534958972158959, 0.35534633138622285, 0.35534132824609987, 0.35533283810899446, 0.35533232556312566, 0.35533248950714136, 0.35532759582984558, 0.355326973234803, 0.355328067915679, 0.35532813517771883, 0.35532833543468451, 0.35532589695057876, 0.35532411109631379, 0.35532284204030617, 0.35532051944283627, 0.35532008839294421, 0.35531962014443896, 0.35531875349533359, 0.35531987035191998, 0.35531423926443451], 'rmse-stdv': [1.4382519803177563e-05, 1.5944986449815816e-05, 1.6934447449321987e-05, 3.3538966126365944e-05, 3.8972172493251649e-05, 5.01651437528025e-05, 5.6210595286631314e-05, 6.1726144990266373e-05, 6.1269996828324311e-05, 6.2656953454165763e-05, 6.5865250591247426e-05, 6.8936563154016806e-05, 8.5286202722617416e-05, 8.9342780355853449e-05, 9.3628141469681027e-05, 0.00010057867053767439, 0.00010932207161183562, 0.00011179516171767385, 0.00012665394878902224, 0.00013872026960907665, 0.00014618562030854347, 0.00014924620359730935, 0.000158121509343585, 0.00016451839711794226, 0.00017316231482113212, 0.00017165548790998444, 0.00017974874248233516, 0.00019607388997681249, 0.00021091563650869956, 0.00020725458019886354, 0.00021126228494290086, 0.00021142749203924264, 0.00021374712620604079, 0.00022303061094113035, 0.00023217735318956066, 0.00024796223989271773, 0.00025525131029146484, 0.00026108499433428513, 0.00027402572070290394, 0.00029786957142530771, 0.00030580665086696573, 0.00032221166784273411, 0.0003345466731069549, 0.00034215843124468066, 0.0003541424596567835, 0.00035234508414737033, 0.00036353206300826258, 0.00038006620876377507, 0.00037819293304766969, 0.00038089599162332828, 0.00038155237231340434, 0.00038711084837382315, 0.00040487291992320079, 0.00041114793921654075, 0.00042958324274140395, 0.0004378005829397122, 0.00044238612082123753, 0.00044990547927726648, 0.00045566420599125942, 0.00046028941029886797, 0.00046453657490578534, 0.00046726036688724958, 0.00047281014062069737, 0.00048923387343997654, 0.00049178434549657589, 0.00049565548758466482, 0.00050079886272515303, 0.00050382771915406484, 0.00050687096224721754, 0.00051005324151640779, 0.0005235872771502814, 0.00053669291610493862, 0.00054535223166490539, 0.00055557241981191543, 0.00057547250089190266, 0.00057873202799122369, 0.00059184606839652828, 0.00059572441073448804, 0.00060169145020501174, 0.00060729344558654852, 0.00061176003992822678, 0.00061877833308765918, 0.00062516245709528216, 0.00062827527792530481, 0.00063389539454556385, 0.00063588899244939191, 0.00063851631956857277, 0.00064189100626675731, 0.00065380669888809436, 0.00065472675530461378, 0.00066616879845721145, 0.00067748382856080091, 0.00067801655238130133, 0.00069205800899344548, 0.00069929029567452293, 0.00070229227243057807, 0.00071524654539287767, 0.00072051219493982099, 0.00073954829249616902, 0.00074230969046508194, 0.00074659009971850797, 0.00075574645565411314, 0.00076618714255410707, 0.0007743033479467098, 0.00078474262294533632, 0.00080333500690721825, 0.00082091451818584271, 0.00083454254530692458, 0.00084712646115103567, 0.00084639969344252341, 0.00084949523403084312, 0.00086125795040339393, 0.00086290465600218687, 0.0008714885624879242, 0.00087024008163503021, 0.00088534672767286247, 0.00089114413398001379, 0.00089800196916379191, 0.00090154807733682266, 0.00090614271438823973, 0.00090652812442196847, 0.00090619035917519596, 0.00091135958509157614, 0.00091405414558748334, 0.00092252634393468163, 0.00093173971474713323, 0.00093519536417338729, 0.0009524495455092702, 0.00096116238211017184, 0.00096618816771888033, 0.00097535468875082981, 0.00098643646321633664, 0.0009949769205989036, 0.00099646092059181763, 0.0010013031797864917, 0.0010115462712425259, 0.0010271925243888951, 0.0010262278209581131, 0.0010272828028351571, 0.0010303605647838832, 0.0010356616390515649, 0.0010472847126207858, 0.0010496271203346435, 0.0010537345022682593, 0.0010580394945775718, 0.0010665056674559158, 0.0010706695705904852, 0.0010773430363293327, 0.0010767203156239814, 0.0010877743428065899, 0.001095553890632034, 0.0011041063244301064, 0.0011087999808180031, 0.0011159840186845053, 0.0011171008943128273, 0.0011219650430991863, 0.0011233690378211103, 0.0011225177319434112, 0.0011215465431931157, 0.0011272548687560311, 0.0011341664168669928, 0.0011350323753033292, 0.0011399304911094499, 0.0011422702301988801, 0.0011443807247353756, 0.0011482142110213294, 0.0011467273221240186, 0.0011500216018755357, 0.0011518393099840675, 0.0011537062343145007, 0.0011535585624731584, 0.0011618814223211269, 0.001163634977371053, 0.0011773284115347575, 0.0011822455410506773, 0.0011836748559577383, 0.0011881558387630744, 0.001193430560554705, 0.0012002551137660538, 0.0012037053316527864, 0.001212763259359822, 0.0012192548898505848, 0.0012202917326672046, 0.001227269822476779, 0.0012386573267173023, 0.0012393539774995202, 0.0012409989961228701, 0.001247231869761506, 0.001247865587851569, 0.0012508592450242435, 0.0012588241298787382, 0.0012612283264725342, 0.0012643359192906591, 0.001270722537920719, 0.0012799778362506239, 0.0012790331304100651, 0.0012835688915995886, 0.0012929293426150656, 0.0012986580592867102, 0.0013010686312033516, 0.0013031385894598385, 0.0013078091153248105, 0.001312232023491588, 0.0013156804045498839, 0.0013181702331749579, 0.0013211917832798546, 0.0013193349204801354, 0.0013266432840206446, 0.0013352104409673066, 0.0013343746264383898, 0.0013313738154753384, 0.0013324390213174267, 0.0013375842659458131, 0.001341236237748344, 0.0013421797920778315, 0.00134464042946409, 0.0013519024208688505, 0.0013511084021268212, 0.0013448595132670845, 0.001348832244996364, 0.0013490696031676656, 0.0013558893503615977, 0.0013717903933912051, 0.0013764806363610906, 0.0013813463257941175, 0.0013816621039283263, 0.0013827991591108175, 0.0013883479602739596, 0.0013899397379788685, 0.0014102403653618748, 0.0014106146765146877, 0.0014155294480768937, 0.0014229888451236148, 0.0014236995145918575, 0.0014250471469643236, 0.0014286856479923627, 0.0014330909055608177, 0.0014418767037442926, 0.0014416858238841389, 0.0014439146896674507, 0.0014534776584119941, 0.0014564089374125471, 0.0014662601836183391, 0.0014674273281674009, 0.0014702694294135904, 0.0014689583143981163, 0.0014677639252456435, 0.0014711977879126618, 0.0014729147276747138, 0.0014810469849520301, 0.0014818268591930246, 0.0014831689431349466, 0.0014802700061584306, 0.0014822800684091654, 0.0014847957237869691, 0.001486366135431204, 0.0014932621509517728, 0.0014981058485970926, 0.0014991129021208421, 0.0015025598536854015, 0.0015026941172814521, 0.0015039376775637789, 0.0015077328950191581, 0.0015106710744874441, 0.0015119267786011512, 0.0015106033779999119, 0.0015141317485489181, 0.0015202420385255697, 0.0015203444679783671, 0.0015217955827537813, 0.0015228255149868229, 0.0015243359801746801, 0.0015256317794730032, 0.0015269884314422829, 0.0015291713069726469, 0.0015299593888200736, 0.0015401524663743284, 0.0015477287153929796, 0.001559025955647806, 0.0015650581129679762, 0.0015686785049974466, 0.0015827378926515292, 0.0015828977663844058, 0.0015840941051227402, 0.001596339037504869, 0.0015918723364589606, 0.0015968371621951213, 0.0016019568597224949, 0.0016030726003869534, 0.0016043490180700215, 0.0016026651796479979, 0.0016055645926894479, 0.0016075800670855508, 0.0016125957276999798, 0.0016113819044142298, 0.0016091129758303781, 0.0016099300631543169, 0.0016150078479033405, 0.0016145037743307516, 0.0016120559454543078, 0.0016191129915636342, 0.0016164847260013522, 0.0016161777473023952, 0.0016170401688991064, 0.0016169732942317684, 0.0016244775213383109, 0.0016262303877511847, 0.0016242726069202313, 0.0016278700989715773, 0.0016336735118153934, 0.0016327487378428307, 0.0016292195501004219, 0.0016300924561521771, 0.0016348304627445387, 0.0016343358170956722, 0.0016364815780912244, 0.0016388881309659707, 0.0016377474599620723, 0.001639727738747877, 0.0016421803133508045, 0.0016423548031162657, 0.0016423625347903568, 0.0016423400647266463, 0.0016487797375431888, 0.0016512588170295996, 0.0016516693016931253, 0.001656489251999827, 0.0016575081058986345, 0.0016558121463241769, 0.0016554752114063158, 0.0016557285262064354, 0.0016614394972801656, 0.0016641031916625447, 0.001666466559477141, 0.0016681824614241929, 0.0016710105672639601, 0.0016732497219881328, 0.0016738854853850099, 0.001678092612077995, 0.0016793837813132771, 0.0016835082709315643, 0.0016843274133171589, 0.0016851485507250294, 0.001686129801097017, 0.001689661154609992, 0.0016904952376786263, 0.001691878776780563, 0.0016931325283398562, 0.0016940136389404995, 0.0016991721820091448, 0.001701234584961904, 0.001704829850252347, 0.0017067129197180669, 0.0017082415860073372, 0.0017091624605735981, 0.0017117979950207617, 0.0017104210133537673, 0.001718270485882731, 0.0017188166670389759, 0.0017212294644209111, 0.0017211835138971167, 0.0017204062472982763, 0.0017215205009507282, 0.0017257537818046998, 0.0017298161508547091, 0.0017307055329942567, 0.0017354138144413229, 0.0017399382840040745, 0.0017409084225088651, 0.0017398606054447934, 0.0017401437654719902, 0.0017408003496102093, 0.0017391050213595046, 0.0017334080837996938, 0.0017408095503423579, 0.0017427053664852952, 0.0017502142520367673, 0.0017582119894264841, 0.0017587601680015225, 0.0017587013105465029, 0.0017602810487898544, 0.0017581701621051935, 0.0017602232622550412, 0.0017645151908937595, 0.0017660158161658366, 0.0017741961893913678, 0.001776601636251162, 0.0017798589752884557, 0.0017808771188585774, 0.0017917325410415326, 0.0017923434656302662, 0.0017911107040769901, 0.0017892415873444446, 0.0017874219609213791, 0.0017883258659915516, 0.0017918178700786205, 0.0017940997879437215, 0.0017955898645828068, 0.0017966160890766946, 0.0018004069847358571, 0.0018003794592422917, 0.0018045395233659955, 0.00180572213912722, 0.0018089558563217276, 0.001810364605792227, 0.0018156724790018789, 0.0018163225246649757, 0.0018132938753614249, 0.0018119174930863707, 0.0018163841717464611, 0.0018177341396786667, 0.0018161423643915664, 0.0018147166332455615, 0.0018179240336973768, 0.0018191743754978344, 0.0018212614715293484, 0.0018217476692545739, 0.0018223785909701844, 0.0018268685749749252, 0.0018309407699539744, 0.0018331616020092593, 0.0018340519634277637, 0.0018342792655745065, 0.0018345501002406345, 0.0018331806739961738, 0.0018331861078434786, 0.0018358444777883181, 0.0018385837225785796, 0.0018384471912201484, 0.0018366278249646595, 0.0018329102446318791, 0.0018339616511134802, 0.0018399558469088088, 0.0018451734305413889, 0.0018427538123771119, 0.0018444058074236938, 0.0018440406852115352, 0.0018493077535105429, 0.0018524277238399312, 0.001852702658056244, 0.0018525251215907512, 0.0018558551893615529, 0.0018524509578909689, 0.001853122383049336, 0.0018521479772463352, 0.0018517905830135347, 0.0018505573341243214, 0.0018482901122879921, 0.0018506059940848509, 0.0018503255153869038, 0.0018498896124245514, 0.0018539244161924825, 0.0018548977584476594, 0.0018566411164389499, 0.0018547908796835283, 0.0018557849726374533, 0.0018594889233533502, 0.0018619760750172025, 0.0018622027479861336, 0.0018637597730606279, 0.0018648995542459149, 0.0018651129710094995, 0.0018683319144918464, 0.001867506124954469, 0.0018692136930494158, 0.0018693122185246731, 0.0018694272284849437, 0.0018723399226927271, 0.0018722649828730314, 0.0018759971061338828, 0.0018803341515068622, 0.0018825196590307326, 0.0018789773452843673, 0.0018844042529606727, 0.0018848861193277611, 0.0018875603387152056, 0.0018879064758644606, 0.0018883808152350186, 0.0018889570479925745, 0.0018891285209930038, 0.0018902480438265294, 0.0018853183727177904, 0.0018865015284598042, 0.0018894278294895681, 0.0018892091231205218, 0.0018905379415869437, 0.0018922508119069098, 0.0018914336448772738, 0.0018943895164825494, 0.0018936672442141291, 0.0018951820785892352, 0.0018975045788043226, 0.0018958276446834597, 0.0018956099935390871, 0.0018958356866249965, 0.001897101205604103, 0.0018955096169814001, 0.0018954776342389964, 0.0018977273885719297, 0.0018986861815924289, 0.0019014708385301658, 0.001905739062798755, 0.0019071288825738284, 0.0019103337464802376, 0.0019137334759379316, 0.0019160683013651634, 0.0019170358159184919, 0.0019180212614735845, 0.0019179937938552782, 0.0019202415974713559, 0.0019246247454512808, 0.0019263168808493327, 0.0019264381684480152, 0.0019228069930352786, 0.001924522530660214, 0.0019229824312794021, 0.0019251258771225501, 0.001925422289916386, 0.0019270287673825725, 0.0019311271206829148, 0.001935440419505697, 0.0019365919536598591, 0.0019407077405433615, 0.0019380643391615784, 0.0019386392152033699, 0.0019422730743455701, 0.0019454713835283092, 0.001945819106382895, 0.0019469391715259592, 0.0019451272346215904, 0.0019524463245709038, 0.0019537679126228717, 0.0019542642962337253, 0.0019570284431980208, 0.0019589552758025753, 0.0019584456535014518, 0.0019571444781766641, 0.0019559574402865372, 0.0019548782945227566, 0.001954860061899882, 0.001956937537183027, 0.0019550646399235281, 0.0019570985441250462, 0.0019551284426320321, 0.001957198561156533, 0.0019614673060868808, 0.0019636900152399774, 0.0019642720494373509, 0.0019636711204057385, 0.0019644065022901202, 0.0019649114656903311, 0.0019646736094322452, 0.0019630356803682788, 0.0019647512452672976, 0.0019681030978896514, 0.0019675964082998872, 0.0019694597568982674, 0.0019679922688945421, 0.0019682588227871601, 0.0019703792977597647, 0.001971781663468483, 0.001971101778305995, 0.0019730514580309794, 0.0019759625628710201, 0.0019781601819893382, 0.0019777259561888612, 0.0019813040197278128, 0.0019832621932814984, 0.0019802128580781574, 0.0019807457740586569, 0.0019823189448673804, 0.0019786841857128662, 0.0019823981316655164, 0.0019818821554810338, 0.0019794709178042472, 0.0019813028119871085, 0.0019822944325773345, 0.001983968760802739, 0.0019858957281105364, 0.0019863863093518229, 0.0019854226799971087, 0.0019890972058181269, 0.0019869537190070727, 0.0019884069320781354, 0.0019892113169616326, 0.0019902010940189454, 0.0019906097729265495, 0.0019877434768103244, 0.0019904155397219496, 0.001991256528774161, 0.0019919621384163577, 0.0019908043417666333, 0.0019883592068811939, 0.0019886697452683563, 0.0019939122675506748, 0.0019943927960987856, 0.001993856151246632, 0.0019966253870810969, 0.0019972177357635247, 0.0019989060617244211, 0.0019992624586540734, 0.0019984372121504934, 0.0020015990567685787, 0.0020063725141040646, 0.0020048126821718797, 0.0020066160176995707, 0.0020075150070289747, 0.0020094662116511759, 0.0020128069599021584, 0.0020136153000363073, 0.0020157178876893745, 0.0020152546885584691, 0.0020148559574249773, 0.0020157379608565177, 0.0020160180816526088, 0.0020188101662038478, 0.0020221288886002361, 0.0020219907186856943, 0.00202247084062871, 0.0020234542737932689, 0.0020263994778925414, 0.0020281468210482912, 0.002025222157889211, 0.0020272545971645184, 0.002030134420409876, 0.00203382047867919, 0.0020353933951149906, 0.0020383656836387206, 0.002039370041539795, 0.0020406416822023855, 0.0020427255090514941, 0.002044396876187043, 0.0020444112737745841, 0.0020434744116513089, 0.0020419201328000297, 0.0020441807308861967, 0.0020461636566933743, 0.0020465926273435398, 0.0020467251161198912, 0.0020468085091599167, 0.0020475273370197606, 0.0020457155860071513, 0.002042664952306488, 0.002044870931292935, 0.0020438729025165966, 0.0020429115711153743, 0.0020456769634378496, 0.0020456815600752173, 0.0020449146615408669, 0.0020450346624464439, 0.002045422677260285, 0.0020480710609685412, 0.0020498347441744642, 0.0020535476699950531, 0.0020550604444256959, 0.0020547326150429689, 0.0020579940742644566, 0.002059869822531704, 0.002059375496680017, 0.0020588418429847654, 0.0020582966021161827, 0.0020597197805265698, 0.0020612673001036029, 0.0020621496425284576, 0.0020611570510095786, 0.0020596177839616566, 0.0020615303933334997, 0.0020596806766644371, 0.002061629844123343, 0.0020652955570928167, 0.0020660937698568039, 0.0020657112968952033, 0.0020664435307583087, 0.0020665349368111691, 0.0020652746077493597, 0.0020644642171077929, 0.0020652697854928442, 0.0020658225784066803, 0.0020664435835499873, 0.0020676079436027072, 0.0020698715028046898, 0.0020705403146640722, 0.0020716105617931703, 0.0020706607119304879, 0.0020697380521260619, 0.0020708732993232198, 0.002071400399560054, 0.0020717711290776865, 0.0020716978754081765, 0.0020719634262761737, 0.0020720323369313345, 0.0020729701702549897, 0.0020767320041854978, 0.0020768040468639071, 0.0020752407445889824, 0.0020746246715888572, 0.0020769183571982841, 0.0020777277354706359, 0.002078184276267759, 0.0020780555417644142, 0.0020786473230948758, 0.0020801100626492049, 0.0020796889536866463, 0.0020810110658588286, 0.00208186719347353, 0.0020832687564610879, 0.0020846934803192283, 0.0020824292640075704, 0.0020816809493149169, 0.0020821877827302466, 0.0020827833272978294, 0.0020822172713612342, 0.0020818354929990892, 0.0020830196773409999, 0.0020836120085830682, 0.00208604078883459, 0.0020868230352958047, 0.002085190732465601, 0.0020842041212087914, 0.0020833583583484265, 0.0020850249032833452, 0.002083446314216056, 0.002085432613728253, 0.0020853347251983453, 0.0020857785483365432, 0.0020862399170316014, 0.0020893665286684363, 0.0020884195284927758, 0.0020900891050245375, 0.002088563426957353, 0.0020904334046236914, 0.0020840137073942933, 0.0020844081826061254, 0.00208527826224258, 0.0020858899928488227, 0.0020851985242318987, 0.0020847596089382564, 0.0020851017587565231, 0.0020854668480875947, 0.0020867253042481603, 0.0020825323365672044, 0.0020804247473177311, 0.00207902537336566, 0.0020805561859363899, 0.0020825629880843609, 0.0020828942191100145, 0.0020833562134288338, 0.0020859507903447298, 0.0020879516307194079, 0.0020873410887028753, 0.0020888577862997939, 0.0020880791080431838, 0.0020863004172947406, 0.0020869914985004912, 0.0020854088963577046, 0.0020848270437385814, 0.0020863465716696727, 0.0020863971448090461, 0.002085895879603129, 0.0020866866421823836, 0.0020858423249477138, 0.0020865806714052268, 0.0020880386735370395, 0.0020904556941458712, 0.0020898099744168092, 0.0020907304411547571, 0.0020880574196647747, 0.0020867566467447449, 0.0020872682195724, 0.002088235585310831, 0.0020889404511582371, 0.0020879300088917419, 0.0020894395912821242, 0.0020910048946384337, 0.0020901548165893824, 0.0020921304956472743, 0.002091948799135383, 0.0020926383781831202, 0.0020961400687121009, 0.0020974765017899985, 0.0020975621979197018, 0.0020983384738233655, 0.002099739431049821, 0.0020964495770013172, 0.0020963434034851568, 0.0020970954235598705, 0.0020997006128724333, 0.0021009215289007196, 0.0021009379862387888, 0.0021036295887891425, 0.0021035435681250915, 0.0021046777748139974, 0.0021044641157498536, 0.0021070376657663409, 0.0021066013477942184, 0.0021079987516344168, 0.0021077305730214956, 0.0021044324803571285, 0.0021052972364069122, 0.0021071098479494054, 0.0021054055166423715, 0.0021028942382239203, 0.0021031612569584607, 0.0021008394922413455, 0.0021015293739456921, 0.0021039839243192451, 0.0021027571705111501, 0.0021010550271472689, 0.0021049697066673543, 0.0021047736726514266, 0.0021059305567477761, 0.0021065820790100082, 0.0021091999578527216, 0.0021099542907573568, 0.0021123705302541151, 0.0021155015466911118, 0.002115749867847197, 0.0021157710948408806, 0.0021143297880627993, 0.0021143588450721665, 0.0021138115868719015, 0.0021141136555188349, 0.0021156540999081766, 0.0021172655230630166, 0.0021168244586819863, 0.0021172144197584434, 0.0021171840510909898, 0.0021166182754543943, 0.0021170827857876569, 0.0021176747078206831, 0.002117622547101696, 0.0021173880180779129, 0.0021181678495442616, 0.0021223533975266496, 0.0021234771118893783, 0.0021220039432825193, 0.0021223928790342415, 0.0021217343564600378, 0.0021227935405524756, 0.0021236189625604123, 0.0021248416681319618, 0.0021260802992115677, 0.0021261064075476402, 0.002125869478011278, 0.0021256699009921868, 0.0021259521065456711, 0.0021264296350032201, 0.0021266350727047346, 0.0021292785194534417, 0.0021291982218528444, 0.0021318898975011117, 0.0021331143098849864, 0.0021339109468975757, 0.0021356809726092701, 0.0021350315036925424, 0.0021362687375841185, 0.0021376934611062963, 0.0021392721202939382, 0.0021370672466808479, 0.0021365442870958352, 0.0021390128804074887, 0.0021417867428066492, 0.0021418621570988981, 0.0021429715492913733, 0.0021438929983544474, 0.0021456959258635512, 0.0021457190693855424, 0.0021458686211856468, 0.0021440749951369994, 0.0021439688821871194, 0.0021430817376396115, 0.0021404330251260811, 0.0021397080321371425, 0.0021384650630162034, 0.0021408825350996414, 0.0021410126714839959, 0.0021404414631396415, 0.0021431990831187677, 0.0021422696767266023, 0.0021440687715057606, 0.0021466801208255733, 0.0021484608963251747, 0.002150440041171865, 0.0021505663051952554, 0.0021518205451712389, 0.0021547150906102751, 0.0021552940344298035, 0.002157605696944254, 0.0021568239086184201, 0.0021572978012171398, 0.0021576380888900901, 0.0021548950333832319, 0.0021558450309646362, 0.0021556649434484607, 0.0021571877190858287, 0.002157545077383605, 0.0021555293929827373, 0.0021554922451599051, 0.0021540060227442704, 0.0021528324028905373, 0.0021554169741892872, 0.0021582917734275504, 0.0021592710796247633, 0.002160079039894985, 0.002160884608677761, 0.0021615185400337103, 0.0021607891358186512, 0.0021594814328177356, 0.0021576127058843303, 0.0021561856645266348, 0.0021545117136383737, 0.0021537293982355371, 0.0021543266964634991, 0.0021560082911526108, 0.0021550770147451461, 0.0021555010895529395, 0.0021551615861900289, 0.0021550791366685067, 0.0021558411528400045, 0.0021558214100165941, 0.0021530586084398799, 0.002155024966768978, 0.0021544822926429625, 0.0021535115679054344, 0.0021567790272885422, 0.0021565512010512967, 0.0021558135733006152, 0.0021552770691596911, 0.0021560402581909284, 0.0021537201616366737, 0.0021513310596860845, 0.002150239827466446, 0.0021525405903037751, 0.0021532126070241135, 0.0021526262706476011, 0.0021516226971751169, 0.0021514814910754392, 0.0021547751054331972, 0.0021570109341648479, 0.0021583542937903986, 0.0021590229649626805, 0.0021601998613421359, 0.0021597617866658913, 0.0021599130524191821, 0.002160771891300118, 0.0021641549526554378, 0.002164904220261353, 0.0021646588468988686, 0.0021653097897806899, 0.0021649981084435228, 0.0021659326090153709, 0.0021651174511481408, 0.0021674191672696108, 0.002167269552345751, 0.0021681141839825758, 0.002166720319649729, 0.0021708263521565607, 0.0021714255899860863, 0.0021686238171342671, 0.002167996166738038, 0.0021671800453450002, 0.0021669553263580529, 0.0021681028655722349, 0.0021704614015263203, 0.0021712423892635896, 0.002167342157892823, 0.0021667095346324483, 0.0021648828697001911, 0.002166339998763984, 0.0021693499282880602, 0.0021733308304616042, 0.0021719452478613905, 0.0021702527163647233, 0.0021706833391196916, 0.0021678205044125599, 0.002169338418576991, 0.0021696167331171829, 0.002168969438174931, 0.0021682110518367105, 0.0021690835475418335, 0.002169270160164413, 0.0021701070078666545, 0.0021709454941310526, 0.0021710323568224729, 0.0021723956367953189, 0.0021717729274676151, 0.0021731651918309547, 0.0021742230382545387, 0.0021731979298161108, 0.0021744963442974084, 0.0021739445876146411, 0.0021750883707282844, 0.002174347058876929, 0.0021743562534684902, 0.0021733083218818281, 0.0021732103818179471, 0.0021729681502087035, 0.0021733375318094368, 0.0021723542957300637, 0.0021733168210762528, 0.0021740239366431673, 0.0021742380178618741, 0.0021743323924452154, 0.0021748271304012916, 0.0021746009314011954, 0.0021739815988643294, 0.0021732623155723239, 0.0021756367517893607, 0.0021751708362020617, 0.0021783305014053673, 0.0021774694734135241, 0.0021780195013192901, 0.00217651274361366, 0.0021759376171936614, 0.0021747011123196954, 0.0021750022723181829, 0.0021768733491804612, 0.0021788805247238271, 0.0021793102358656767, 0.0021794384037017848, 0.002180169303068713, 0.0021819728143685018, 0.0021831311732725665, 0.0021836575803443033, 0.0021853331465671545, 0.0021855348229699706, 0.0021869151800041871, 0.0021859953082145177, 0.0021857954330044014, 0.0021867364225966516, 0.0021852407605410583, 0.0021857434515186653, 0.0021869492783504127, 0.0021863787789884251, 0.0021880774651408929, 0.0021858484830896037, 0.0021890621570274599, 0.0021889135313408626, 0.0021879387127164631, 0.0021878246239968912, 0.0021877978606892265, 0.0021874490180988685, 0.0021847987764308067, 0.0021858977923783158, 0.0021874515988333852, 0.0021905900069824598, 0.0021916683625523405, 0.0021908147709886936, 0.0021905461339918734, 0.002191377072050357, 0.002191879302123982, 0.0021951024170686413, 0.0021932928458407242, 0.0021943832738838104, 0.0021951487258518639, 0.0021948942930480929, 0.002194627459413808, 0.0021974904717921187, 0.0021965202855469242, 0.002196392876525107, 0.002197344112193808, 0.0021963419183058806, 0.0021964312053925467, 0.0021985033787153825, 0.0021992566593208368, 0.0022003883195105887, 0.0022015436596196322, 0.0021996665269839212, 0.002199329361010709, 0.0021970251686287768, 0.0021977795464616259, 0.0021970984884486485, 0.0021974369747100385, 0.0021978223059609648, 0.0021961884502137421, 0.0021962830547758399, 0.0021985475584254794, 0.0022003019516228066, 0.0022002197857461376, 0.0021995789376112679, 0.0021997361535332233, 0.0022018680777205761, 0.0022035404130999621, 0.0022041391712887769, 0.0022023693734882009, 0.0022012266555062488, 0.002202786618432785, 0.00220072832143742, 0.002202278022252549, 0.0022015322419885802, 0.0022016297754043376, 0.0022041096142025899, 0.0022032572914081694, 0.002202988840069438, 0.0022024710783106107, 0.0022042093666645797, 0.0022034535009945325, 0.0022045241425228414, 0.0022051606123730462, 0.0022049477406234624, 0.0022043158888167083, 0.0022045947054482071, 0.0022039353196394297, 0.0022025908075217746, 0.0022017786101414367, 0.002204638062642933, 0.0022036642238102023, 0.0022071645696192625, 0.0022103919457125529, 0.0022101163129491226, 0.0022100399932171249, 0.002209383011844025, 0.002208435105441533, 0.0022116350671084082, 0.0022115373649970412, 0.0022108747260865592, 0.0022099701600957864, 0.0022111723734263167, 0.0022106259578902623, 0.002212577661274532, 0.002211562807052089, 0.0022116742118768033, 0.0022112243017574024, 0.0022128468525146896, 0.0022108357692359375, 0.0022123846858733125, 0.0022108221003873615, 0.0022091317816370372, 0.0022066740476005126, 0.00220456929794133, 0.0022038780908827768, 0.0022042598045494887, 0.0022080793834603563, 0.002207525764094265, 0.0022074493783652135, 0.0022077623121052399, 0.0022069421229182306, 0.0022079431606418878, 0.0022108111452046231, 0.0022103913974462503, 0.0022103075788035003, 0.002211613598784468, 0.0022114610042817924, 0.0022077877403537251, 0.002204021156341926, 0.0022042494616022904, 0.0022027006076430701, 0.0022023309859555565, 0.0022045313209037186, 0.0022049100680353977, 0.0022039246350936239, 0.0022023247008491806, 0.0021988674017883714, 0.0021975533570436244, 0.0021976299815444434, 0.0021970931548740821, 0.0021957972613530097, 0.0021955179834161846, 0.0021936020230939046, 0.0021950633600336051, 0.0021924805429887218, 0.0021920143098632925, 0.0021921561751403048, 0.0021927285872778944, 0.0021919576904230473, 0.0021922957038912065, 0.0021902697455599791, 0.0021914250314282306, 0.0021937622544660829, 0.0021916222242424678, 0.0021919842722374163, 0.0021920937270453163, 0.0021910560666852982, 0.0021909873309643919, 0.002189775413113534, 0.00218761419735124, 0.0021890685321467648, 0.0021904117623724123, 0.0021895154649256752, 0.0021872434687267377, 0.0021875287202725202, 0.0021900146997638841, 0.0021896491114674435, 0.0021864500897283809, 0.0021827270696547145, 0.0021828221273642616, 0.0021845403301077185, 0.0021826815631031029, 0.0021821111542527495, 0.0021835558932681116, 0.0021868597397821576, 0.0021863109046261868, 0.0021855176528150361, 0.0021876276669384642, 0.0021905399381263314, 0.0021915982687378431, 0.0021923645292128867, 0.0021926188541779674, 0.0021904766226812891, 0.0021885611158953466, 0.002188457326743509, 0.0021890451944558235, 0.0021881547172538322, 0.0021870386940173844, 0.0021871000283342087, 0.0021875921371392673, 0.002187495202118795, 0.0021882651705085955, 0.0021895495389429695, 0.0021907578058029146, 0.0021908237888145039, 0.0021906373843926237, 0.0021916571687770688, 0.0021934745731309381, 0.0021954073237948016, 0.0021949097830577333, 0.0021942355197610308, 0.0021945009080490932, 0.0021952780242181047, 0.0021953305010923843, 0.0021956179002391276, 0.0021959010247948373, 0.0021963790999096583, 0.0021982277183592897, 0.0021995995358719588, 0.0021981721009326008, 0.0021987655008152583, 0.0022004083365982354, 0.0022006139445742683, 0.0022030651165121655, 0.0022024677315342859, 0.0022022286773674114, 0.0022046719710587981, 0.0022048239763996784, 0.0022044597498434796, 0.0022069376580748853, 0.002207244767755735, 0.002207460350732794, 0.0022076205403614465, 0.0022062991934375985, 0.0022080762267679362, 0.0022077223216944919, 0.0022069704771019668, 0.0022067320372155357, 0.0022060595601253105, 0.0022080584981259265, 0.0022055692874359549, 0.0022061231679220291, 0.0022071524638931951, 0.0022060396716239885, 0.002205761263985152, 0.0022053932127775941, 0.0022053773943256287, 0.0022056582879632657, 0.0022074554227522183, 0.0022078392320610354, 0.0022072903694397845, 0.002207207825484273, 0.002207884472792335, 0.0022047082376857505, 0.0022015772367067156, 0.0022014728888829316, 0.0022005193567482678, 0.0021976645718852191, 0.0021962349113531028, 0.0021976884863632441, 0.0021981703941264716, 0.0021966406287320783, 0.0021977362836386556, 0.0021951810127897493, 0.0021968207107544015, 0.0021989897537708822, 0.0021984797650251562, 0.0021987173893344108, 0.0021961711952548767, 0.0021972303464074081, 0.0021978618246752631, 0.002198015102036789, 0.0021982758905231331, 0.0021984528809666651, 0.0021977374803512249, 0.0021973816202045448, 0.0021959308930202498, 0.0021987216014679768, 0.0021990524897817949, 0.0021981238899835762, 0.0021971764206670011, 0.0021972608000022311, 0.0021974416521492383, 0.0021992458755880919, 0.0021994324194604753, 0.0022011156314577967, 0.0022012198735127768, 0.00220016283355093, 0.0021995918421554026, 0.0021970932906927856, 0.0021963620679893592, 0.002195887151836788, 0.0021954512960826495, 0.0021938506496176208, 0.0021913620218475065, 0.0021902437128850506, 0.0021895719644350982, 0.0021896343410441697, 0.0021883357188665531, 0.002188651944113352, 0.0021884057124098035, 0.0021880178272113316, 0.0021882131400709818, 0.0021880183916754073, 0.0021894804658821971, 0.0021919085059521688, 0.0021921085241755861, 0.0021922653704185046, 0.0021916622482573396, 0.0021864018012218594, 0.0021866650407457453, 0.0021879330202213866, 0.0021876569752990013, 0.0021881594963478936, 0.0021874737131969088, 0.0021885509941114772, 0.0021894483184205782, 0.0021916168214574259, 0.0021918249036149895, 0.0021940894684986576, 0.0021973258612839283, 0.0021951140516573379, 0.0021951226910241644, 0.0021952121669686925, 0.0021956285047265546, 0.0021975200825872778, 0.0021969721392320063, 0.0021982010753532856, 0.0021993005978402469, 0.0021977864994441316, 0.0022005873778704198, 0.0021982449918016176, 0.0021987374372082363, 0.0021978155147726664, 0.0021986313821409993, 0.0021987200484330331, 0.0022000360771734263, 0.0022016411046623828, 0.0022006779097092995, 0.0022002263289297207, 0.0022009858252571369, 0.002199689620295011, 0.0022016827016635922, 0.0022021483932731923, 0.0021986948594533225, 0.0022012461508427472, 0.0022012065992803257, 0.002203424606667679, 0.0022022902320941629, 0.0022024980272885401, 0.0022037188880256213, 0.0022035691304210878, 0.0022034566808812137, 0.002204902901368707, 0.0022076282237803312, 0.0022090917706365826, 0.0022074430536818792, 0.0022059515228023932, 0.0022050686446923209, 0.0022065682895164875, 0.002205038136315075, 0.0022067167045543303, 0.0022058950270326732, 0.0022051167863501552, 0.0022059797941956717, 0.0022070211541454893, 0.0022086258553203784, 0.0022091001447713593, 0.0022081921938047421, 0.0022070771497644283, 0.0022078075761211417, 0.0022064692501110078, 0.002205760442902768, 0.0022050773377411954, 0.0022063585294423713, 0.0022066727506916267, 0.0022083088604961617, 0.0022095794392101515, 0.0022109191137365315, 0.0022100674428906534, 0.002209486292032391, 0.0022093978228140813, 0.0022098924634619131, 0.0022087122908602068, 0.0022093451019874369, 0.0022092340242703601, 0.0022087122858964418, 0.0022095446897988551, 0.0022089547008954757, 0.0022072202538960285, 0.0022056410239505268, 0.0022056486508002528, 0.0022057236039882272, 0.0022058012307098487, 0.0022081036935504327, 0.0022089972419017615, 0.0022096495735202948, 0.0022101810729305504, 0.0022101875003042557, 0.0022098950779507872, 0.0022093038849177078, 0.0022113438828906019, 0.0022109506990889366, 0.0022080640632472343, 0.0022070870057366121, 0.0022096374423804213, 0.0022087004595084379, 0.0022089550157981192, 0.0022080878287371142, 0.0022088546015166218, 0.0022092313844147117, 0.0022100309708212324, 0.0022114715500922044, 0.0022102898550226298, 0.002210735926496316, 0.0022148501915492504, 0.0022165514581280344, 0.002218146698311033, 0.0022185362115221224, 0.0022170813419013268, 0.0022179599743069513, 0.002218848827355532, 0.0022204930540158756, 0.0022215338486774915, 0.0022225686174808666, 0.0022234205814343892, 0.0022232196457531144, 0.0022270627402445928, 0.002226199300856289, 0.0022272044551000002, 0.0022240773970043249, 0.0022235966437825962, 0.0022231206666960984, 0.0022232788696283558, 0.0022237433514788935, 0.0022236809853337022, 0.0022241707868935136, 0.0022244224138592444, 0.0022235312753208785, 0.0022230390411517044, 0.0022230949892618184, 0.0022251119126186596, 0.002226973628730236, 0.0022289863606177697, 0.0022306962636626918, 0.002231014889742409, 0.0022327724345924593, 0.0022312223875832187, 0.0022343064720609929, 0.00223530932810575, 0.0022352464465186937, 0.0022337480426352875, 0.0022336022859301375, 0.0022339857159743882, 0.0022340727705513733, 0.0022358702286782747, 0.0022356984504430274, 0.0022363035244282584, 0.0022350847579151287, 0.0022350587975956205, 0.0022367041817411529, 0.0022360603990187974, 0.0022374052630063338, 0.0022367970093013184, 0.0022366802815697016, 0.0022365868457913793, 0.0022376655961327116, 0.002237989185941704, 0.0022396896159934346, 0.0022397383840221498, 0.002240452958934522, 0.0022403130601119808, 0.0022417248844034013, 0.0022403223971650017, 0.0022407781343092136, 0.0022399708261209698, 0.0022402377497540297, 0.0022411891448092533, 0.0022412038288503289, 0.0022411381319984891, 0.002240523362036337, 0.0022405449396025951, 0.0022416883178857459, 0.0022444793646268627, 0.0022392830888503402, 0.0022402679740183224, 0.0022418054634775549, 0.0022425164078997187, 0.0022419913543228007, 0.002242427595478953, 0.0022421873650512558, 0.0022435489681841394, 0.0022433374648728375, 0.0022425459451030673, 0.0022425872056051573, 0.0022438314388968997, 0.0022452793974038739, 0.0022448214404196375, 0.0022438941475021214, 0.002244312750814098, 0.0022445592819121886, 0.0022449151951415215, 0.0022473052515436498, 0.0022473107628525721, 0.0022466741164506188, 0.002246341373065903, 0.0022471395567400065, 0.0022477343087937939, 0.0022489135868355306, 0.0022482699571048461, 0.0022481941196655655, 0.0022483688913966424, 0.0022504583092166652, 0.0022514341557553445, 0.0022513426060357866, 0.0022518853808370333, 0.0022534143576817809, 0.0022550771293323997, 0.002253318000086115, 0.0022508131609587207, 0.002251188976588144, 0.0022524640793171332, 0.00225326758015544, 0.002253290312729674, 0.002253055800585929, 0.0022536400412811858, 0.0022524669786244196, 0.0022541272554337005, 0.0022564875646355069, 0.0022570403926767614, 0.0022578370121812412, 0.0022585555350355655, 0.0022575067677136729, 0.0022602716925806303, 0.002258639962670307, 0.0022591388745070483, 0.0022604620877066041, 0.0022610104623800153, 0.0022610587508901888, 0.0022607595194113709, 0.0022598878511811233, 0.0022607569461637221, 0.0022610637413998892, 0.0022602380930944938, 0.0022614892107159982, 0.0022613752174474194, 0.0022608689893368755, 0.0022617405039102286, 0.0022616000620759762, 0.0022626203284985395, 0.0022617969821812282, 0.0022622490207248238, 0.0022605832561174155, 0.0022608571615414436, 0.0022613029098163553, 0.0022613589304615756, 0.0022619015235306379, 0.0022600419909288567, 0.0022607802935965399, 0.0022608956065366696, 0.0022610748336765194, 0.0022615154284449583, 0.0022603527610967761, 0.0022591376396063705, 0.0022591075147190844, 0.0022574245420304523, 0.0022562693497214835, 0.0022576915227338267, 0.0022588261707909649, 0.0022594950094027219, 0.0022597555729951084, 0.0022605502562003535, 0.0022615272491273587, 0.0022612274783379302, 0.0022593787287259475, 0.0022597902369586118, 0.0022591679016266395, 0.0022579757705834961, 0.0022590060628512034, 0.0022593740365017406, 0.002260310727451518, 0.0022610391244140262, 0.0022595530698606256, 0.002258289311743916, 0.002258743930714761, 0.0022587811786948411, 0.0022599140680738686, 0.0022606370150030871, 0.0022600484944859042, 0.0022577721137904661, 0.0022584972053949358, 0.0022585042120354358, 0.0022581620586794885, 0.0022583698913399611, 0.0022574233023212285, 0.0022570965497122884, 0.0022571105243103535, 0.0022574186219356932, 0.0022598373713052572, 0.002264928176966757, 0.0022661517656377412, 0.0022657722781834885, 0.002267129102180744, 0.0022672383894483858, 0.0022678123511787685, 0.0022692130400439291, 0.0022692589845590616, 0.0022705170694477333, 0.0022698458867064226, 0.0022702768692725939, 0.0022716434279901635, 0.0022714182580037645, 0.002271101821287948, 0.0022711575755040726, 0.0022719669867671682, 0.002274781682803045, 0.0022740573145905618, 0.0022721257662656727, 0.0022738483956925961, 0.0022729749810726581, 0.0022743061312230279, 0.0022753257999983891, 0.0022757606476313456, 0.0022772201019096994, 0.0022730662830235714, 0.0022749878969765217, 0.0022757850425283279, 0.0022743967449457281, 0.0022735207143485384, 0.0022729652905849073, 0.0022725517406794364, 0.0022727131108530264, 0.002273754496134742, 0.0022748636539920465, 0.0022752472556700587, 0.002276238127987427, 0.0022773172763410443, 0.0022778238150861752, 0.002278600981407274, 0.0022787878905147991, 0.0022781421234953775, 0.0022782931218011233, 0.0022776086919728232, 0.0022780542968095043, 0.0022785646427251503, 0.0022772561397302405, 0.0022747469538452977, 0.0022741206094321039, 0.0022758746071732438, 0.0022747548508726531, 0.0022772893779271042, 0.0022773444272708916, 0.0022776585763261748, 0.0022771670190715118, 0.0022768442372240252, 0.0022778931156695876, 0.0022792238676517976, 0.002278743314203724, 0.0022789725295490047, 0.0022788640447673979, 0.0022792653193962485, 0.0022796352739926353, 0.0022788165996493752, 0.0022791548086940339, 0.0022805898807001018, 0.0022827486684774143, 0.0022830691464608538, 0.0022824842115973911, 0.002284942318998672, 0.0022862196166297416, 0.002283554658464105, 0.0022837313970319826, 0.002283959564455961, 0.0022843213864160816, 0.0022825828727502939, 0.0022823182960688307, 0.0022816336693619365, 0.0022809564468555558, 0.0022822902196971426, 0.0022810767606736876, 0.0022818807027403564, 0.0022786207982718764, 0.0022781646184575756, 0.0022795817490334769, 0.0022793158523481722, 0.0022794937816138765, 0.0022809916402030056, 0.0022822580490868341, 0.0022823415667232862, 0.0022828004284830353, 0.0022826777471884178, 0.002282696006949705, 0.0022841836741324954, 0.0022855926046158196, 0.0022861120432791969, 0.0022876435241064555, 0.0022887742894844212, 0.0022883643841064387, 0.0022879441703232192, 0.002288741365589791, 0.0022885140865160614, 0.0022903643380357826, 0.0022877182856527187, 0.0022882572794873379, 0.0022893880480889884, 0.0022895988780402173, 0.0022912707587998396, 0.0022913982566694657, 0.0022930357364690786, 0.0022916862843867262, 0.0022921664134506834, 0.002291474162360088, 0.0022904710924191514, 0.0022912615225462148, 0.0022921079921688595, 0.0022912784906913282, 0.0022908962968919734, 0.0022925287603239204, 0.0022920220109474512, 0.0022915905681056442, 0.0022909623470010157, 0.0022914353727334648, 0.0022925734789255628, 0.0022951065559525404, 0.0022962597127311381, 0.0022970711390536396, 0.0022986043636797474, 0.0022983194265869702, 0.0022986115548451654, 0.0022976281546231068, 0.0022953818391023479, 0.0022972486413329711, 0.0022976529231551277, 0.0022974955263315514, 0.0022971996689325632, 0.0022982037065234426, 0.0022978811981364173, 0.0022992733167581398, 0.0022998661348686326, 0.0023000413643564852, 0.0023002888287919792, 0.0022992735076007667, 0.0022988066510583769, 0.0022988790719674517, 0.0022966204291352617, 0.002295711824807008, 0.0022952930066603883, 0.0022964710776814711, 0.0022975157659950976, 0.002298518625338235, 0.0022995900246002856, 0.0022997251075754679, 0.0023023723953334587, 0.0023017519288849519, 0.0023013993841673433, 0.0022999548397572771, 0.0023023840478925395, 0.0023020782783065448, 0.0023017910964237367, 0.0023030249464108931, 0.0023030831794953171, 0.0023019746860803848, 0.0023031683634356626, 0.00230273645795637, 0.002303253899551016, 0.002302984198277951, 0.002303175715157672, 0.0023022464558866318, 0.0023036807614964525, 0.0023045225411426934, 0.0023063667358093876, 0.0023063582201780816, 0.0023063212226980493, 0.0023061107268720535, 0.0023066437164428835, 0.0023060458155362584, 0.0023053764889739417, 0.0023058971791356866, 0.0023062700613284536, 0.0023064517125988156, 0.0023080185191630322, 0.0023100391395550796, 0.0023094967324022273, 0.0023118883758980226, 0.0023123810920042316, 0.0023126486275757752, 0.0023127996474186486, 0.0023134259446937009, 0.0023139293002762242, 0.0023147380663894913, 0.0023143389568029381, 0.0023132079323032817, 0.0023149739450884759, 0.0023159155499319346, 0.0023172554592222547, 0.0023186905601485131, 0.0023194945190711841, 0.0023197029993463733, 0.002318594498915783, 0.0023179249914763922, 0.0023165571005883501, 0.0023160093582178746, 0.0023156477639057031, 0.0023132486234567944, 0.0023124565951325991, 0.0023140474992084993, 0.0023144825514435454, 0.0023145631647202534, 0.0023132559038016231, 0.0023136017351670208, 0.0023127384701715345, 0.002312222346480969, 0.0023127712732631035, 0.0023133811067081182, 0.0023149029554990533, 0.0023159525346702829, 0.002316186731586586, 0.0023156983902908467, 0.0023156086468376119, 0.0023150585886352586, 0.0023154149458547418, 0.0023159517950422127, 0.0023169453564380522, 0.0023178784289241273, 0.0023192294883275758, 0.0023194867497530114, 0.0023204291852305992, 0.0023189925647153619, 0.0023188360394860389, 0.0023212597035285219, 0.0023197263367245005, 0.0023202118780687349, 0.0023226549390376072, 0.0023243111892366844, 0.0023256498298310615, 0.0023268986451584622, 0.0023270032326053371, 0.0023267618752892591, 0.0023276049148384099, 0.0023272314828310012, 0.002327623524131977, 0.0023297116878424769, 0.0023296712167276907, 0.0023293950687944911, 0.0023306876969288949, 0.0023309725183619165, 0.0023305359144567875, 0.0023284348354713773, 0.0023288114391507597, 0.0023289165667305192, 0.002327301522099133, 0.0023245859033761539, 0.0023256366690810643, 0.0023266473789664175, 0.0023255406364874835, 0.0023248801056075788, 0.0023252933702484603, 0.0023279027419403362, 0.0023292397809922781, 0.0023306610081019531, 0.002331719536868628, 0.0023315650826567684, 0.002332979148608116, 0.0023302020543960137, 0.002331241642904497, 0.0023343061123562159, 0.002333414964152469, 0.0023332991615282773, 0.0023344139064951312, 0.0023355537407191789, 0.0023374164657377077, 0.0023377681398223326, 0.0023361916420851017, 0.0023376993019305708, 0.002338221993964283, 0.0023386443399849195, 0.0023403814491303673, 0.0023402029303871055, 0.0023393651479561091, 0.002340837822488884, 0.0023413414087369759, 0.002341409066645535, 0.0023400277259203761, 0.0023405045920188998, 0.0023403031310548278, 0.0023408550479112116, 0.002339816318917907, 0.0023397799890765175, 0.002339999029349297, 0.0023411595010877986, 0.0023408840944018311, 0.0023419310363873053, 0.0023427083983148743, 0.0023434448019905431, 0.002343344621164437, 0.0023440037286783949, 0.0023432729673835413, 0.0023444705385115019, 0.0023448876377991875, 0.0023458783682573361, 0.0023442465437201244, 0.0023446661041412434, 0.0023446550845081363, 0.0023459504269411728, 0.0023436761571510008, 0.0023443328980007998, 0.0023455032306313323, 0.002345708340395861, 0.0023462563669162575, 0.0023476421922817982, 0.0023465534268602784, 0.0023469358091741505, 0.0023469758376882741, 0.0023477255520415424, 0.0023483491079908819, 0.0023491960337021656, 0.002349309151073899, 0.0023478855357185781, 0.0023479079246907339, 0.0023482129798465534, 0.0023480456452692304, 0.0023479067625023502, 0.0023467460871703792, 0.0023473924652290413, 0.0023477997467624247, 0.0023476288790767307, 0.0023474588622029017, 0.0023470247541066327, 0.0023468219830924666, 0.0023470478057809104, 0.002346413491411918, 0.002346774680495105, 0.0023474334728041087, 0.0023472358368911779, 0.0023477235671347028, 0.0023486339416737504, 0.002348122657542711, 0.0023484942342711611, 0.0023485399192276152, 0.0023472945463853872, 0.0023473843665377733, 0.0023479859364269472, 0.0023469517498890722, 0.0023479157472998018, 0.0023490767946679893, 0.002350485762486366, 0.002349637914006374, 0.0023502210331182204, 0.0023498104852021631, 0.0023490057148708482, 0.0023481527518880016, 0.0023486155102059631, 0.0023505312972790051, 0.002350683425300466, 0.0023497664069314478, 0.0023478574685478395, 0.0023479441054944754, 0.0023490059768357828, 0.0023477916822808604, 0.0023496450465761167, 0.0023498882484964578, 0.0023499272765241776, 0.0023503802726612898, 0.0023505032314124186, 0.0023505913888941751, 0.0023495937400354657, 0.0023501506465168627, 0.0023502962746679719, 0.0023495390584853475, 0.0023492045800513183, 0.0023485631607313684, 0.0023494312846921565, 0.0023500541500086347, 0.0023502312893622962, 0.0023503258186959876, 0.0023517825630334156, 0.0023516129210518527, 0.0023527494460222078, 0.0023518867719315374, 0.0023502877845959147, 0.0023506167744259811, 0.0023492709028843611, 0.0023474329246867117, 0.0023499145020395534, 0.0023471720546553131, 0.0023473458649179825, 0.0023488569203597904, 0.0023460580112955789, 0.0023472555555636571, 0.0023464705848607144, 0.0023480891227496492, 0.0023483512627143401, 0.0023477897099741726, 0.002348444044395658, 0.0023505865401821121, 0.0023510607541099848, 0.0023506452563187182, 0.0023502083732540786, 0.0023503760613581642, 0.0023518235054785217, 0.0023538215417250848, 0.0023535213712380172, 0.0023490393813094899, 0.0023481983347945884, 0.0023494334908496983, 0.0023489552245441205, 0.0023480272049646294, 0.0023485205540856105, 0.0023510538934346623, 0.002352630224212178, 0.0023516520373261809, 0.0023517614046959468, 0.0023517581558681832, 0.0023518850799116573, 0.0023508880143479328, 0.0023507171262573119, 0.0023499572414962949, 0.0023492885044764122, 0.002353050072110801, 0.0023534061181302705, 0.0023539137315736146, 0.0023554440203610635, 0.0023529480993228417, 0.0023521754297274916, 0.0023527065035956101, 0.0023529269492615842, 0.0023520623893746812, 0.0023518755306545349, 0.0023509489144776445, 0.0023496963379635249, 0.0023503211814714289, 0.0023512183969167763, 0.0023514904221732832, 0.0023541087227020884, 0.0023542921272981551, 0.002354154014709179, 0.0023543375591320473, 0.0023557648978413466, 0.002354930043780382, 0.0023561449379099785, 0.002357232756501734, 0.0023601701750094969, 0.0023612293018173704, 0.002361079435930911, 0.0023606277161185127, 0.0023611548928889436, 0.0023617096739818387, 0.0023603600965427947, 0.0023607977189465138, 0.0023603639303066797, 0.0023607932638400673, 0.0023606040809452012, 0.0023606358652474512, 0.0023610349308427763, 0.0023613197549806819, 0.0023617981641045624, 0.0023615562372814539, 0.0023615677461911117, 0.0023603748917820351, 0.0023609426914543942, 0.0023607700082323487, 0.0023617619674370376, 0.0023615331498155539, 0.0023625886141231535, 0.0023610417509333679, 0.0023602515419077503, 0.0023607022136051181, 0.0023598502259202041, 0.0023600455861043502, 0.0023613952839564951, 0.0023609850556555079, 0.0023605146870199173, 0.0023613225501720719, 0.0023621346885565916, 0.002360965906667456, 0.0023603660154705437, 0.0023602607021021493, 0.0023597389502351546, 0.0023590427370044883, 0.0023596940563842457, 0.0023599522812068931, 0.002359532587155105, 0.0023605044551435442, 0.0023615387156576597, 0.0023599428301212155, 0.0023590381357939207, 0.002359147465744978, 0.002358913486659587, 0.002357966650051294, 0.0023575097351368041, 0.0023587073685194301, 0.0023582290101768263, 0.0023577046534555258, 0.0023583191292728669, 0.0023589187478554718, 0.0023594725435871254, 0.0023609666186065279, 0.0023604387789300397, 0.0023597470057416261, 0.0023599141676722748, 0.0023609809374975544, 0.0023591840145173871, 0.0023592863365776711, 0.0023575599410427701, 0.0023559813213362112, 0.0023562536792901853, 0.0023549274240577934, 0.002356352424254702, 0.0023562579795450662, 0.0023562051055897666, 0.0023564196590136898, 0.0023580040535555666, 0.0023556250585887897, 0.0023567360353233761, 0.0023559346820730678, 0.0023554420495083923, 0.0023578802606583353, 0.0023555774491153523, 0.0023553689539853339, 0.0023558088775720697, 0.0023556953790215673, 0.0023563143316635672, 0.0023569280295521761, 0.0023562549568254634, 0.0023563266984954244, 0.0023558736056680457, 0.0023564146399467842, 0.0023575383114007456, 0.0023557138508640304, 0.0023563639976478851, 0.0023577840969645943, 0.0023583446325777815, 0.0023598431052059693, 0.0023598412727448255, 0.0023589247703485368, 0.0023606850051311767, 0.0023606939652523204, 0.0023613760568725879, 0.0023606635703487047, 0.0023605391115112794, 0.0023599211747745963, 0.0023613788917355655, 0.0023623168287012016, 0.0023617582094477454, 0.002360294139096318, 0.0023613965609846715, 0.002361800233235446, 0.0023629807231598443, 0.0023615157747746214, 0.0023609922505517149, 0.0023599169426231129, 0.00235972924173694, 0.0023608243077620546, 0.0023623393774785846, 0.0023612091833828219, 0.0023625940370252218], 'auc-mean': [0.73251291157025022, 0.74757272001190789, 0.74884287344857781, 0.76326493797028028, 0.76300973229704072, 0.76679565493906698, 0.76702335926946463, 0.76749367737349838, 0.76717966353985123, 0.76886720476783976, 0.7684174466727709, 0.76923418805644272, 0.77138764121538084, 0.77030134557736329, 0.77012226623636559, 0.77051748875426074, 0.77001746314881458, 0.7705283572402587, 0.77108534229875181, 0.77137797288411958, 0.77201496580346307, 0.77191569749144584, 0.7720983592322721, 0.77187942496050466, 0.77244850696632283, 0.77412147653036567, 0.77455307015493058, 0.77513296822709532, 0.77615247168875601, 0.77621901978894114, 0.77667265437334587, 0.77702039952102286, 0.7769705226787329, 0.77682631360670928, 0.77646847565425103, 0.77622825795547301, 0.77609545380884593, 0.77579194987896349, 0.77629617422259711, 0.77642120856903807, 0.77687106395179995, 0.77694396407632715, 0.77681988116127143, 0.77679738923275699, 0.7767034559206728, 0.77668748612177152, 0.77664526838190295, 0.77658291914464095, 0.77649482703004513, 0.776658744569929, 0.77736589648579968, 0.77719401280822553, 0.77716619912465112, 0.77719546041060872, 0.77808368697512753, 0.77789131518030008, 0.7779135763154239, 0.77805885252544604, 0.77804427177483715, 0.77799085987396777, 0.77789327327860824, 0.77806069092831676, 0.77800301524672266, 0.77835422810125765, 0.77840813627640737, 0.77839312367673963, 0.77862923686176777, 0.7786520373740401, 0.77887940862245886, 0.77913310277859593, 0.77940349721210533, 0.77933553657349308, 0.77964301404818226, 0.77957190972929202, 0.77948831101075655, 0.77953844175749887, 0.77973202335532177, 0.77980608209519187, 0.77976654539288848, 0.77973212371626055, 0.77995033302822558, 0.78016830060782405, 0.78026852490907872, 0.78036457911100499, 0.78044715184314206, 0.7805108536171772, 0.7806179809620224, 0.78086741076742416, 0.7808666060312992, 0.78095152045916449, 0.7809933422220986, 0.78099976552790307, 0.78106761675633707, 0.78110041156717502, 0.78135963196450331, 0.7813213665800397, 0.78144958548224031, 0.7814767133830911, 0.78168982863454195, 0.78173777200237349, 0.78171776335501864, 0.78179624071443343, 0.781859860514583, 0.78202245942974424, 0.78217710623081205, 0.78221698452980348, 0.78235169963779061, 0.78245722057943856, 0.78251589429807444, 0.78261334686040951, 0.78257674706525016, 0.7827938905291838, 0.78286854493007441, 0.78303304541238994, 0.78303590433939796, 0.78333342087588054, 0.78337923643529039, 0.78348902703988177, 0.78351189212424477, 0.78360898288298908, 0.78364690120764591, 0.78371934218224582, 0.78376301944664617, 0.78393581990849481, 0.78397826567313489, 0.7843885291615218, 0.78459444551914648, 0.7846674781882923, 0.78468522157295639, 0.78470061191316975, 0.78486432986629406, 0.78488332176819653, 0.78521663865039293, 0.78526312019908118, 0.78534412700620837, 0.78536611498805442, 0.78541619759534231, 0.78560942782889487, 0.78577009311125701, 0.78586872193741386, 0.78591872457034273, 0.78595929560243438, 0.78594529636844335, 0.78597328673612743, 0.78594857029516008, 0.78599266044890137, 0.78610237813975281, 0.78624165597703455, 0.78639939518436486, 0.78652814664631565, 0.78657500619253917, 0.78668651106180831, 0.78679294166146518, 0.78686810929189732, 0.78686325909416432, 0.78689099179484312, 0.78693088899861474, 0.78697276102695235, 0.78707472377255416, 0.78719188901127457, 0.78723300972256327, 0.78721008810237225, 0.78737958432669441, 0.78744507812174935, 0.78747653280305419, 0.78750845142616044, 0.78763324443608496, 0.78785590328847088, 0.78789115581545688, 0.78788621376272816, 0.78797163158434591, 0.78800997655112492, 0.78816235196895934, 0.78822624975971678, 0.78835723748941244, 0.78848180575865523, 0.78859244594713851, 0.78863442748788981, 0.78871174571119163, 0.78869930208231542, 0.78884140021264337, 0.7888978648495113, 0.78903811689349868, 0.78906659038534233, 0.78912494672567701, 0.78914960621632213, 0.78926720197937972, 0.78935125122586414, 0.78937660862240722, 0.78951631881653017, 0.78954775439923375, 0.78965786483965827, 0.78973452444451531, 0.78974357887055213, 0.78982598629302125, 0.78993103810874532, 0.79002230186002398, 0.79010670255593818, 0.79018972644311825, 0.79018019311775123, 0.79018161862713698, 0.79031527320824979, 0.79039530605961572, 0.79040956492052894, 0.79044137347235721, 0.79043210862991287, 0.79053732257046072, 0.79063174452178664, 0.79068571456852776, 0.79073157507101466, 0.79082183118224791, 0.79100276225969657, 0.79110297992821565, 0.79118284137905504, 0.79125325886824016, 0.79126745817360455, 0.79137182738497658, 0.79142208760281696, 0.79151218493234299, 0.79150010686830718, 0.79158127969683179, 0.79167724372528903, 0.7917349623053056, 0.79189344340976608, 0.79194369626231764, 0.79193914834227497, 0.79199305499887163, 0.79209235439335568, 0.7922065130243009, 0.79234954553217785, 0.7925195546960847, 0.7926047079175027, 0.79267364466834078, 0.79274803164632868, 0.79290267422236405, 0.79301051719687199, 0.79305010244930418, 0.79306786712545208, 0.79310080136355388, 0.79310348219573834, 0.79324329370900926, 0.79323329389767849, 0.79339658966368609, 0.79345564229072407, 0.7935237764741172, 0.79358646019330303, 0.79371489823382135, 0.79379362339047566, 0.79380281373224915, 0.79400252874245747, 0.79402726993858286, 0.79410828024790181, 0.79417900092806692, 0.79429305435344566, 0.79436506812135055, 0.79449883380148589, 0.79468473942806439, 0.79485334676506525, 0.79492140153570967, 0.79501036160576355, 0.79509317659614431, 0.79515889924245697, 0.79522923080797714, 0.79533192319640433, 0.79553175393916553, 0.79558335316872708, 0.79558684850673544, 0.79562913152552073, 0.79574019971331755, 0.79590786406681202, 0.79604359965905713, 0.79610885242905138, 0.79618305573762105, 0.79627264000660791, 0.79634359582078706, 0.79635621556293634, 0.79645682149015307, 0.79650558876577748, 0.79658748847746375, 0.79663548030996201, 0.79663772717118198, 0.79672914987408772, 0.79678781287309031, 0.79686039589848379, 0.79701091136566293, 0.79716750354444998, 0.79722219026016494, 0.79728549557532558, 0.79735470316244406, 0.79736600545629899, 0.79746308941885846, 0.79746319949893707, 0.79752118698049279, 0.79754266925374773, 0.79769412521714389, 0.79772000022488287, 0.79780975798763698, 0.79787057350551605, 0.79791395350151606, 0.79802596500341072, 0.79806232013202683, 0.79810312521989824, 0.79814279295714252, 0.79820200713084088, 0.79824918118662447, 0.79832793922348222, 0.79838633073923515, 0.79850516540223426, 0.79854512886821694, 0.79860258064813872, 0.79865074110858925, 0.79872286495971967, 0.798760932815614, 0.79881883706031953, 0.79889900429237337, 0.79896770126135586, 0.79900583147808857, 0.79911256069506487, 0.79919805851388726, 0.79928111770694177, 0.79931708962251713, 0.79937365871893151, 0.79938527280706784, 0.79947089799447379, 0.79952586506132828, 0.79954869022026975, 0.79958375282899175, 0.79964832718144707, 0.79975916439624206, 0.79983819247382726, 0.79997335269676673, 0.80002210732463053, 0.80016743112533428, 0.80023108928660458, 0.80024014978987545, 0.80029020771773707, 0.80034138378529929, 0.80040419425675025, 0.80046767706804101, 0.80054442389078351, 0.8005885516500838, 0.80062183668966536, 0.80066915882202283, 0.80070672044090219, 0.80079187682525566, 0.8008739912534496, 0.80088426067081797, 0.80098320921779553, 0.80100498659531694, 0.80109692813317301, 0.801142779525569, 0.80118181320347903, 0.80125255946496377, 0.80132537500125967, 0.80142756526909409, 0.80148143500867963, 0.80154450483859863, 0.80162068707993672, 0.80163203107717818, 0.80164446527956323, 0.80165163387057192, 0.80172694472338013, 0.80177478272451741, 0.80180829609456583, 0.80189753753124915, 0.80201147035960152, 0.80203821149037746, 0.80208319374378245, 0.80212386217462162, 0.80218532365107165, 0.8022164329131003, 0.80226572053385126, 0.80236565158647244, 0.8024227003056893, 0.80248494078540789, 0.80253581939642038, 0.80258798317592839, 0.80265270222544216, 0.80269118312266419, 0.80275961049121458, 0.80280183201320199, 0.80290532414897464, 0.8029612338277754, 0.80303238193546833, 0.80306830900295267, 0.80313613394462302, 0.80320153793123694, 0.80324743808745414, 0.80329394124440034, 0.80335679958619133, 0.80345456142057325, 0.80352289903366858, 0.80359461828771406, 0.80363602495735864, 0.80366442698423646, 0.80372336288849355, 0.80376474371507955, 0.80380378868967384, 0.80381570584417061, 0.80390774122522957, 0.80401509751192246, 0.80405254766426792, 0.80414891683554168, 0.80420899891134656, 0.80423985348046845, 0.8043007714960787, 0.80436044707150711, 0.80439497487522582, 0.80446922729421944, 0.80452286901330639, 0.80456178559262082, 0.80466236520846446, 0.80477948351250483, 0.80480193117313736, 0.80484536417759733, 0.80488038003561557, 0.80492423906423072, 0.80494441488484492, 0.80499208161318625, 0.80505316435228824, 0.80516570844487201, 0.80522106831542151, 0.80527834218140393, 0.80532833653667146, 0.80536533627615048, 0.80539107871029114, 0.80541932176695374, 0.80547004026825153, 0.80547827580799836, 0.80552128031904502, 0.80557623990987148, 0.80564182507136994, 0.80568373083937284, 0.80571367045148445, 0.80578893359687709, 0.80583244084083105, 0.80586920559413855, 0.80590454645977183, 0.80596097279853662, 0.80603198997378001, 0.80606473664447775, 0.80610713084942631, 0.80615141159208503, 0.80623805938826654, 0.80627417589545092, 0.80631633301456984, 0.8063731740186052, 0.80639710838706447, 0.80643951123690094, 0.8064828258328518, 0.80649673054210724, 0.80652974068412875, 0.8065795828028719, 0.80663007932588626, 0.80669633640239424, 0.80675610018040589, 0.80678725068545631, 0.80682754161556791, 0.80685231000540336, 0.8068775937341286, 0.80692429447976222, 0.80696031827311043, 0.80698997694719421, 0.80706151959987016, 0.80709761448555972, 0.8070991867608418, 0.80711069962753224, 0.80713144517950242, 0.8071904299461975, 0.80722602742846605, 0.80732247745995789, 0.80734948571260501, 0.80740180531634975, 0.80747841975329693, 0.80751023728499993, 0.80755024752044446, 0.80758704032016182, 0.80765805599203078, 0.80767046874812876, 0.80767891275172199, 0.80770547003309845, 0.8077498227682367, 0.8077807838254929, 0.80782495439591229, 0.80783227824336856, 0.80789166431197845, 0.80793533106954774, 0.8079587446374793, 0.80802328037102222, 0.80808369926514434, 0.80815255766460736, 0.80818035035326974, 0.80824411372796057, 0.80827132129369894, 0.80832777677810108, 0.80835088692126755, 0.80835903092624639, 0.80840331242721475, 0.8084379907525161, 0.80846643996330148, 0.80850974489743521, 0.8085471804526575, 0.80859660529965383, 0.8086252309324431, 0.80865984599984519, 0.80867455593309034, 0.80871210869802912, 0.80872426702830835, 0.80879500429263051, 0.80883040954960728, 0.80891605303860215, 0.80896676611324436, 0.80899534417458896, 0.80904215235571431, 0.80908139726899386, 0.80910561280314641, 0.80912053181754862, 0.8091711098541392, 0.80919840927808906, 0.80923329945415112, 0.80927105075897787, 0.80928647558785782, 0.80931755648444048, 0.80935123408873311, 0.80935670910817981, 0.80938691468801172, 0.80943418825483204, 0.80946708763936981, 0.80952163191045123, 0.80959701501491366, 0.80962614871235261, 0.80968997330522985, 0.80974175468927112, 0.80982516939809523, 0.80985341530385813, 0.80987790735095211, 0.80991087254294969, 0.80998751235928634, 0.81001409316578454, 0.81005165212166941, 0.81008624314697164, 0.81010719661285191, 0.81012961769039915, 0.81015598461714222, 0.81017172041804186, 0.81020471098842983, 0.81022080564490984, 0.81023100400336445, 0.81026330265485991, 0.81027643616279121, 0.81028361414049443, 0.81030855051040585, 0.8103350745530008, 0.81036498193704865, 0.81038393920269702, 0.81040831460882357, 0.81043099158937326, 0.81044526267466177, 0.81048914472487366, 0.81059690204089452, 0.81062794859709475, 0.81064528391015889, 0.81066913797264528, 0.81069577231627166, 0.81072667606888316, 0.81075909991872286, 0.81078133186777568, 0.81082318345878002, 0.81086415909738374, 0.81088850334681228, 0.81095624840728231, 0.81106603221164197, 0.81108343282335782, 0.81111397420992792, 0.81114550622631754, 0.81118023157185204, 0.81120122149731499, 0.81122576287877224, 0.81126570593447522, 0.81130209128990471, 0.81134993744117201, 0.8113745655973279, 0.81141399822415072, 0.81143033253501573, 0.81145228097845057, 0.81146777404892556, 0.81147569008582643, 0.81152847839743725, 0.81157241249539014, 0.81160443362871493, 0.8116134902210721, 0.8116393502900332, 0.81167802881843121, 0.81168345037557954, 0.81170686955800819, 0.81176578202350247, 0.81177425271608339, 0.81180586999663296, 0.81180860103891594, 0.81183125077025386, 0.81184587590057156, 0.81187268489027176, 0.81191429952775296, 0.81194851765080889, 0.81196157315426354, 0.81198468787835354, 0.8119930724658706, 0.81200762438485774, 0.81205955236393623, 0.81208105806359154, 0.81214849008790702, 0.81220354367437864, 0.81223528484515595, 0.81226747751242312, 0.81228399670767037, 0.81233965388283202, 0.81235044243105814, 0.81236529865495011, 0.81242155591363741, 0.81248517090248418, 0.81252764786058551, 0.81255267127414788, 0.81257206580123886, 0.81263193568418202, 0.81264405467672629, 0.8126978258538633, 0.81271570542904126, 0.81278236160198958, 0.81279808078473281, 0.81281479173054672, 0.81283229051485306, 0.81285073294760879, 0.81287403580853046, 0.81293616890204667, 0.81295857584010345, 0.81297941264315843, 0.81306286724731591, 0.81307076850197291, 0.8131156807439327, 0.81313605953379153, 0.81315277960678833, 0.813193262864641, 0.81321885404389926, 0.81326950391940611, 0.81328842680929814, 0.81330278797852951, 0.81331822361008188, 0.81333998788210082, 0.81335681546765604, 0.81337265959559846, 0.81338623019195766, 0.813394386357424, 0.81340610269828029, 0.81346797631346135, 0.81347749514921064, 0.81349630168935627, 0.8135178303497993, 0.81354902827175835, 0.81359193644127537, 0.81360413332890302, 0.81362690226994872, 0.81365590576865632, 0.81367033235062025, 0.81368938402344071, 0.81373799180073103, 0.81376241807554206, 0.81381124725061105, 0.81383197495732085, 0.81386026299332248, 0.81388540509946627, 0.8139327844629165, 0.81395115698038867, 0.8139623025274938, 0.81397766391343163, 0.81401590984708838, 0.81402416466826943, 0.81405930527204406, 0.81407320906816039, 0.81413569868637281, 0.81415739272127519, 0.81417311288928251, 0.81418839735403326, 0.81420635732016033, 0.81421667882406989, 0.81428463211274527, 0.81431076979124239, 0.81432883823990621, 0.81435808548581912, 0.81438192862403014, 0.81439959742994128, 0.8144033793270582, 0.81441168015331689, 0.81442076162522592, 0.81444150287408645, 0.81444430618711683, 0.814450072136783, 0.81446955966440648, 0.81448468126265094, 0.81453415671931084, 0.81455395224621152, 0.81458572222414782, 0.8146071130719752, 0.81461451589698419, 0.814635213974096, 0.81464456463243629, 0.81465293399184002, 0.81466721812468967, 0.81470666439222994, 0.81478843515032229, 0.81480234807964291, 0.81480863478607457, 0.81482560435201656, 0.81482955808541591, 0.8148371794026108, 0.81483456477154803, 0.81485084658129026, 0.81486179744587184, 0.814873885620095, 0.81488888973573026, 0.81489949465176781, 0.81493828414410507, 0.81496153528502879, 0.81500727843785847, 0.81500427989363367, 0.81501526677302694, 0.81504362792805285, 0.81507101274958349, 0.81508047190789445, 0.81509005194594875, 0.81511989978496202, 0.81513145147681221, 0.81517170569519748, 0.81517945429584115, 0.81520679899440318, 0.81522366079152453, 0.81523483451328482, 0.81524012693448977, 0.81526410838246688, 0.81527093783512061, 0.81530022364542298, 0.81533861048510659, 0.81535793999683936, 0.81538632627690999, 0.81540161877241935, 0.81541014102112508, 0.81542677492342275, 0.81543402735122616, 0.81545011431928871, 0.81546370025514658, 0.81551878748189977, 0.81559153580632238, 0.81560828368741145, 0.81562062387146794, 0.81562736904511568, 0.81567688357405999, 0.81568682271976767, 0.81570057998368384, 0.81572314953149028, 0.81572661394027701, 0.81574329801109136, 0.81576116818564892, 0.81576923164861959, 0.81578091601847758, 0.81579458415365003, 0.81579990879820219, 0.81581256825036363, 0.81584329556172419, 0.81585400302693911, 0.81586331152868508, 0.81587926741208483, 0.81588037561365545, 0.81589831212650543, 0.81590639653541996, 0.81591206741004696, 0.81592209534278537, 0.81594228572006633, 0.81595053770931225, 0.81595484855631495, 0.81597716449158408, 0.8159885422381089, 0.8159980614506257, 0.81601464828561632, 0.81602538245507961, 0.81604251495954272, 0.81605123316988148, 0.81606083691030107, 0.81607021219986076, 0.81607949722640227, 0.8160991900989425, 0.81614783179832029, 0.81615610184967946, 0.81616349248872644, 0.81618206601931287, 0.81620269351894914, 0.81621624782030389, 0.81621835487980365, 0.81624348056370355, 0.81628466268852229, 0.8162998287347607, 0.81631406686705144, 0.81633301445987883, 0.81638464199430039, 0.81639905224654785, 0.81643015499391436, 0.81645799664022578, 0.81648195223823161, 0.81651049386145158, 0.8165176196304742, 0.8165738442918592, 0.81659158403577725, 0.81660342244553275, 0.81660731581759261, 0.81663111763459173, 0.8166387989159436, 0.81665836471833253, 0.81669353800108979, 0.81669705978207019, 0.81671222037018898, 0.81671878786849528, 0.81676617588347056, 0.81678142153711875, 0.8168002619374457, 0.81683396423368726, 0.81690985075944622, 0.8169472078459451, 0.81695473966485976, 0.81696096202932966, 0.81696418531984238, 0.81696898764711823, 0.81698523583568172, 0.81699627334288638, 0.81701269935980536, 0.81702103317589003, 0.81704280971665533, 0.81706056560209961, 0.81708642926477792, 0.81710113468916801, 0.81711471675114922, 0.8171511449761828, 0.81715897080879341, 0.81716834914728653, 0.81717409154673371, 0.81717751936418392, 0.81718086992798022, 0.8171871819134221, 0.81718816673343753, 0.8171960137991956, 0.8172038973830773, 0.81726204944790415, 0.81727636216006394, 0.81730934854356507, 0.81732014931363661, 0.81733684416939578, 0.81737726140033506, 0.81738722026170885, 0.81740741970821307, 0.81742228686866303, 0.81743446900033168, 0.81748648451815031, 0.81750280338433168, 0.8175319007667452, 0.81755760184475734, 0.81756113560041666, 0.81756742827169548, 0.81757615492703262, 0.81763243985921741, 0.81764120556461817, 0.8176588712727515, 0.81768412011802138, 0.81769599418011651, 0.81770951697801364, 0.81774445191154665, 0.8177682120416504, 0.81778535614741477, 0.81780041496357436, 0.81780888925671769, 0.81781390982304314, 0.81784676990252658, 0.8178551931713407, 0.81786304933529563, 0.81787590979458891, 0.81788065905320817, 0.81788754071748726, 0.81789667923076004, 0.8179498543404099, 0.81799130669007325, 0.8179977428030405, 0.81800773877225375, 0.81801390558043574, 0.81809024633876515, 0.81810232575573549, 0.81811014832793061, 0.81811240261701923, 0.81812168704298094, 0.818195293453997, 0.81821049826018177, 0.81824185374001446, 0.81824419807311111, 0.81825195208461798, 0.81825290027119324, 0.81826342928377027, 0.8182844939755054, 0.8183010213248656, 0.81830764594393968, 0.81833854475010381, 0.81836718664450991, 0.81844223277721295, 0.81849033030596874, 0.81850352023011452, 0.81850872007237019, 0.81855671389030715, 0.81856756973753053, 0.81857244211188385, 0.81857679214308177, 0.81858879548543739, 0.81863808360441337, 0.8186559842762472, 0.81865979630808872, 0.81865879692642252, 0.8186893048519398, 0.81870404393907759, 0.81872058232653599, 0.81872242780166093, 0.81872348476071566, 0.81874381402923946, 0.81875240073250599, 0.81876809702694031, 0.81877540615112088, 0.81877817026980593, 0.81880494135839155, 0.81886018715978837, 0.81887460806169299, 0.8188795379239302, 0.81888994600442166, 0.81889954255792718, 0.81892601662172326, 0.81892180518734359, 0.81893244651066655, 0.81898741826250954, 0.81898417908224874, 0.81899119456314973, 0.81899787417631809, 0.81900645256339644, 0.81901326954803155, 0.81900695384685862, 0.81903865042941426, 0.8190462280757782, 0.81907471844348034, 0.81908347408138149, 0.8191063761420514, 0.81914866639032691, 0.81915710461831925, 0.81916760522548004, 0.81917692963155386, 0.81918253999136825, 0.81919753294655551, 0.81921604802137638, 0.81922612024864938, 0.81924096710528382, 0.81925455925986768, 0.81927200288180391, 0.81927784726326913, 0.81928606342535226, 0.81928980739851642, 0.81930107913780414, 0.81933343025922079, 0.81933844306827619, 0.81935909331249057, 0.81939248974433665, 0.81940517280699954, 0.81941355229199164, 0.81946267961402464, 0.81950649416951715, 0.8195032401787673, 0.81951488189819199, 0.81951552103307301, 0.81951357597173491, 0.81951904863022418, 0.8195250759826328, 0.819532922939327, 0.81954098570172551, 0.81954886900454815, 0.81955428659529184, 0.81955717163085562, 0.81955766976297384, 0.81956699158175039, 0.81957346884930971, 0.8195817806612482, 0.81959350320711466, 0.8196078411447566, 0.8196191944060216, 0.81962533701350027, 0.81964879619486486, 0.81967295785644456, 0.81969758635957002, 0.81971842524947824, 0.81972912099410866, 0.81975404463926371, 0.81975795829603015, 0.81976674170307329, 0.81976763944055064, 0.81977105590060029, 0.8197796688581972, 0.81980672969110047, 0.81981436871551272, 0.81981493576396802, 0.81982523262654328, 0.81985921816604912, 0.81986444669695824, 0.8198756442643772, 0.81991387014404027, 0.81991626579175247, 0.81992406186821987, 0.81994184921952673, 0.81995109725726978, 0.81997720785616046, 0.81998249677901536, 0.81998892392851275, 0.81999260752179182, 0.81999507201674482, 0.81999509612640531, 0.81999655461196852, 0.82002415901601577, 0.8200462475808965, 0.8200556432658983, 0.82007053168940325, 0.82007096710054572, 0.82007736946576237, 0.82008160144211251, 0.82008455640723721, 0.82009104846093095, 0.82009387620194185, 0.82010629462071094, 0.82012080144972388, 0.82014181204695868, 0.82015999434008402, 0.82016941070165961, 0.82017498786067, 0.82018036727406507, 0.82018469824741103, 0.82018485777968608, 0.82019140087377251, 0.82020486840320339, 0.82020947719037129, 0.82022601778246229, 0.82022621884169067, 0.82024512696739826, 0.8202529616626435, 0.82026152611715164, 0.82026081440558474, 0.82026783278905913, 0.82029630629072992, 0.82030099514442101, 0.82031152486633729, 0.82032587901271781, 0.82032592414246575, 0.82034153396234422, 0.82034515182211565, 0.82036486209865911, 0.82036810446197139, 0.82037172110396761, 0.82038767292653669, 0.8203909682913022, 0.82039252403784102, 0.82041077325936507, 0.82041953577231541, 0.8204257828354502, 0.82043502190437001, 0.82044275494384977, 0.82044600331693773, 0.82045270664237824, 0.82047727280100025, 0.82048236084826676, 0.82055509247689939, 0.82056059175988172, 0.82056538530530843, 0.82057872539924936, 0.82060325638098863, 0.82065201923992659, 0.82065712591362716, 0.82065933248797884, 0.82067679421692075, 0.82069980835542344, 0.82070470049887168, 0.82071367050405608, 0.82072266033220242, 0.82073564690920175, 0.82078281302167144, 0.82078908075589818, 0.82078704759595611, 0.82079740682821678, 0.82082239363448828, 0.82082597445094874, 0.82083444778764691, 0.82083514382861333, 0.82085451771394402, 0.82086043369535733, 0.82087042748063033, 0.82087324929867367, 0.82087147757446088, 0.82087546998432104, 0.82087795883920711, 0.82087868850575541, 0.82088452971081638, 0.82089008804935604, 0.82090504631728733, 0.82091258756261332, 0.82093804073524534, 0.82095894270510306, 0.82099281643250299, 0.82099700068390891, 0.82100218524802637, 0.82101078761501634, 0.82102938762741362, 0.82103719851733137, 0.82106073554827042, 0.82106534106171747, 0.82106825609502165, 0.82107944407399724, 0.82108611023331668, 0.82109764145531661, 0.8211331922730235, 0.82115160547580801, 0.82120268895063364, 0.82123891820708261, 0.82124171048049688, 0.82123919960163594, 0.82126622087906809, 0.82127328449514203, 0.82128499077302419, 0.82130000150181703, 0.82130347825785999, 0.82133352371142299, 0.8213368015953636, 0.82134555201398318, 0.82134753942640815, 0.82134724819513261, 0.82136045035057048, 0.82140906261931035, 0.82142091833664388, 0.82142061800595056, 0.82142836603561553, 0.82143068320216095, 0.82144548953613161, 0.82145884289687598, 0.82145958476275927, 0.82148571050169661, 0.82149493907387128, 0.82150031230614606, 0.82151688531164679, 0.8215233892653433, 0.82153722968102838, 0.82156249238413337, 0.82157251044633006, 0.82157429900599743, 0.82157588115555846, 0.82159636463055818, 0.82159400224413193, 0.82160513249549305, 0.82161840922971141, 0.82162297132395212, 0.82162247292352908, 0.82161609098850941, 0.82162060920074986, 0.82163156310410979, 0.82164081535349953, 0.82164475031348749, 0.8216592494236783, 0.82167446577853376, 0.82168513701575863, 0.82169091203313072, 0.82170164295639159, 0.82171060991348421, 0.82172978599473834, 0.8217373346838901, 0.82174636110451049, 0.82174780809033021, 0.82174731052636696, 0.82175647540681696, 0.8217685541667763, 0.82177211169574882, 0.82179309559540692, 0.82179784147664103, 0.82180828533805828, 0.8218172575920365, 0.82182403086027878, 0.82182967825077802, 0.82183558881634422, 0.82184376898208433, 0.82185012386032752, 0.82185363391591815, 0.82186844793077274, 0.82187727000662891, 0.8218801161999153, 0.82189804650069331, 0.82189853800686907, 0.82189948994680828, 0.82190378843496725, 0.82193042303465091, 0.82194605327747505, 0.82194720064840787, 0.82194883027174437, 0.82195504680928233, 0.82195900948530909, 0.82196497446477745, 0.82197563485295766, 0.82198512343775965, 0.82198462210204704, 0.82199775791281726, 0.82199838504162082, 0.82200463748098307, 0.82201117578984417, 0.82200974281245254, 0.82203286949224785, 0.82203296895550437, 0.82205242378262167, 0.82205825023769852, 0.82206231752516512, 0.82206335322092949, 0.82206912548633038, 0.82207198325488373, 0.82207317499563359, 0.82207877635827464, 0.82208518229514616, 0.82209480038050875, 0.82209880265416013, 0.82209766255382011, 0.82210127677075806, 0.82211249725580515, 0.82212215091932495, 0.82212933112068998, 0.82213094906879536, 0.8221682080768753, 0.8221696452319065, 0.8221738147557115, 0.82217560352163177, 0.8221779632125592, 0.8221877641255666, 0.82219228457361138, 0.82219287574486599, 0.82222103043673778, 0.82222858928216758, 0.82223345553543337, 0.82223904296831773, 0.82224317942337666, 0.8222405988285546, 0.82224870066256384, 0.82225091832825525, 0.82225151846528988, 0.82226991057167553, 0.82227679644297891, 0.82228080040202656, 0.8223440919602355, 0.82234735175715579, 0.82235290810132811, 0.82240229794401165, 0.82240854218366954, 0.82240757262830244, 0.82240827825532326, 0.82241455241766004, 0.82243708949734595, 0.82244282232248234, 0.82245611753858316, 0.82246370042685157, 0.82246287444650901, 0.82247484697668161, 0.82248013972934597, 0.82248542491079013, 0.82249187610264707, 0.82255700179039359, 0.82256245649429638, 0.82256889539711042, 0.82257350598800638, 0.82257486933635549, 0.82257847149558239, 0.82257679336448142, 0.82257980461199909, 0.82259310588506429, 0.82259063298727852, 0.82259918159626577, 0.82260725526804723, 0.82260396876012065, 0.82260734884078057, 0.8226546757957891, 0.82265369453720116, 0.82265224430079731, 0.82265248161862137, 0.82265923925807161, 0.82266715490062337, 0.8226962799886921, 0.82269971722112467, 0.82272231237903881, 0.8227277962424262, 0.82274921732657114, 0.82275842994472703, 0.82276165020092251, 0.8227672783167479, 0.8227693494263365, 0.82277673941339224, 0.82277595029927075, 0.82282087570222695, 0.82282310196208219, 0.82282670955196302, 0.82284230958289173, 0.82283889633949392, 0.82287314219069696, 0.82287960820662198, 0.82287935577687055, 0.82288459108989576, 0.82289402895835395, 0.82292862370927344, 0.8229334958868687, 0.82293485265038835, 0.82297212056153624, 0.82298043331317561, 0.82301898807250817, 0.82303574145274128, 0.82303412631088002, 0.82303788494923324, 0.82305232698538067, 0.82305656739885968, 0.8230689534991672, 0.82308758260524595, 0.82309371885995419, 0.82309311848151123, 0.82310285632812863, 0.82311813816036339, 0.82313621511225354, 0.82318436735974176, 0.82319304179746955, 0.82319156183472919, 0.82319682192676491, 0.82321431019619451, 0.82321473312506299, 0.82321304355902014, 0.82321498260450066, 0.82322436353626449, 0.82323548717172168, 0.82323426808147793, 0.82325427630654069, 0.82325337231921591, 0.82329541052061828, 0.82329796464263028, 0.82330266574984545, 0.82331078483631948, 0.82331279580759398, 0.8233318321410279, 0.82333117094775721, 0.82334072701712524, 0.82335867759115655, 0.82340007908690682, 0.82341140803243607, 0.82341509713355465, 0.82341257915036825, 0.8234149825375916, 0.82342156258820132, 0.82342201334292875, 0.82342315459686444, 0.82342204000801666, 0.82344130377554769, 0.82345765342835686, 0.82345424041706594, 0.8234808668482444, 0.82350054348520307, 0.82350223693687175, 0.82356535644674145, 0.82357822317605256, 0.82358121587978594, 0.82360068999338032, 0.82360873061629614, 0.82361299956425249, 0.82365407307738026, 0.82365317852247633, 0.82365886665537347, 0.82367277394370153, 0.82367821994541079, 0.82368023493735287, 0.82367916847450595, 0.82368219383860131, 0.82370270570718129, 0.82370242098411595, 0.82372400966814807, 0.82377529725429466, 0.8237812798991202, 0.82377944046649831, 0.82377816167590479, 0.82379344429125045, 0.82379607397286936, 0.82379703480918187, 0.8238341666939929, 0.82383593848709824, 0.82384080173553542, 0.82387400315073678, 0.82387353265257413, 0.82391169477901882, 0.82391259808996398, 0.82393439122531087, 0.82394455809165079, 0.82394841155618614, 0.82395339481150009, 0.82397143333146727, 0.82398444692628914, 0.82398037380338618, 0.82397922328773865, 0.82398291859669404, 0.82400190258498307, 0.8240111118520087, 0.8240324859746273, 0.82403424729351415, 0.82405339588885551, 0.82405882635069561, 0.82407982436247451, 0.82410489809051213, 0.82410391103703362, 0.82410569128067446, 0.82410510834044248, 0.82410463404363199, 0.82410543883285325, 0.8241077982251277, 0.82411019325817514, 0.82410898042279557, 0.82413128205851804, 0.82413673075800153, 0.82415193205328341, 0.82415065343504157, 0.824167970773377, 0.82416936474445601, 0.82417156440428307, 0.82417254327731482, 0.82418613019386344, 0.82418664394785779, 0.82419425069815655, 0.82419346154658601, 0.82422550640440018, 0.82427277260382925, 0.82427385571984835, 0.82429126048855283, 0.82430992324239638, 0.82431005196231621, 0.82431810925438642, 0.82432378671082096, 0.82432198520621847, 0.82431803851770657, 0.82432320191677877, 0.82433250524326473, 0.82433675835835274, 0.82435181584464545, 0.82435854048059254, 0.82438367930307677, 0.82438317245889903, 0.82438713511278439, 0.8243890292318804, 0.82442730030268874, 0.82444525814403702, 0.82445371815182844, 0.8244545502350954, 0.82445554128928777, 0.82447291301054249, 0.82447556858261883, 0.82447953148855257, 0.82447912287663616, 0.82447686583042645, 0.82451388807889625, 0.82451966270802879, 0.8245216409108822, 0.82452144618678336, 0.82452629371203279, 0.82452830773270414, 0.82453252578510661, 0.82453225590020429, 0.82453620115971094, 0.82454131038409995, 0.82454064088901491, 0.82453757278460693, 0.82453690875834318, 0.82453711555343967, 0.8245366653564693, 0.8245418216103374, 0.82454414256372144, 0.82453921151640075, 0.82453979085207185, 0.82453830173067522, 0.82454495238251513, 0.82454602636636865, 0.82455020633664566, 0.82456221024036525, 0.82456801343421393, 0.82457164228061131, 0.82457164208994205, 0.82459662442827608, 0.82461787928984798, 0.82461366489564403, 0.82463386417785056, 0.82462935266111204, 0.82462751251557032, 0.82464453916470126, 0.82465020740137407, 0.82466415412980587, 0.82466447843495061, 0.82466322003964976, 0.82467892269575815, 0.82469135013189054, 0.82470340130411335, 0.82470232039541946, 0.82471924988997958, 0.82475466227472705, 0.82475564977653837, 0.82475248848059213, 0.82475672640522668, 0.82475714023465085, 0.82477276225672436, 0.82477012029541752, 0.82477617482224641, 0.82477507891176338, 0.82478326574821037, 0.8247949943729308, 0.82481619597122113, 0.82481254293546813, 0.8248290863456621, 0.82483191423023849, 0.82483210633503634, 0.82483130468000054, 0.82483442065213475, 0.82484485563337517, 0.82486665154015415, 0.82487030464773048, 0.82487155027034353, 0.82487354649183686, 0.82487987199410673, 0.8248827150553566, 0.82490168964687471, 0.82490440629669748, 0.82490619596257519, 0.8249188965214016, 0.82494602640413106, 0.82494374468732734, 0.82495508588713096, 0.82496500119604765, 0.82496718414317649, 0.82497716638041396, 0.8249947075513413, 0.82499104489268604, 0.82500175230254746, 0.82500620978480121, 0.82500782437739384, 0.82502126614384808, 0.82502674968930401, 0.82501969869133762, 0.82503450097451236, 0.82504594691310496, 0.82505013486808054, 0.82504902749950715, 0.82504865455925047, 0.82504947094687642, 0.82505062956055508, 0.82511042604380425, 0.82511664328633461, 0.82512040750475479, 0.82513367585360187, 0.8251330118127107, 0.82513675478118065, 0.82513711204681306, 0.82513446408609925, 0.82515886325196186, 0.82515676811761374, 0.82516680986976565, 0.82516306587113775, 0.82515919931708925, 0.82516003034556051, 0.82516071722685369, 0.82515956726301209, 0.82515806630324628, 0.8251542236216769, 0.82515827284237486, 0.82516969561291909, 0.82516735479943648, 0.82516766077497716, 0.82520623517387592, 0.82521307944552458, 0.82521943134044073, 0.82522759087180708, 0.82522501259662229, 0.82522467064401661, 0.82522485129603207, 0.82523691885925976, 0.82523860289882278, 0.82523727602934183, 0.82523455669304069, 0.82525487072642023, 0.82527311018990779, 0.82527423058884186, 0.82527532288721728, 0.82527358198151135, 0.82527657805928512, 0.82527809684763231, 0.82530159786179758, 0.82530505298035306, 0.82531322669159213, 0.82531297738922194, 0.82531500023735682, 0.82531871111506983, 0.8253328321568969, 0.82533423756707514, 0.82534144667297138, 0.82534398011793186, 0.82535649223336782, 0.82535890644000887, 0.82537299790473517, 0.82541263768081963, 0.82540991482724091, 0.82541365248146492, 0.82541403058474772, 0.82541356540027144, 0.82542492168455028, 0.82542187403264777, 0.82546141398510264, 0.82546266651066025, 0.8254585933982902, 0.82545904628896649, 0.82546412216551468, 0.82546299600020934, 0.82548479208686343, 0.82548993421176853, 0.82549893234764915, 0.82549982390777377, 0.82551328980125516, 0.82551852482108024, 0.82551917022937427, 0.82551188209647941, 0.82552271842370284, 0.82553429903692099, 0.82554034481714622, 0.82553873240279396, 0.82554498513351382, 0.82554360998272835, 0.8255441738463084, 0.82557577742067034, 0.82558677190873431, 0.82559533421127129, 0.82561251913666889, 0.82561431440331545, 0.82561028931840208, 0.82561500259926945, 0.82566579125554296, 0.82567020359500598, 0.82566940512749143, 0.82570045616780186, 0.82570693990870581, 0.82571672902471749, 0.8257223486290679, 0.82576052954284906, 0.82575911007210046, 0.82577648793188507, 0.82578639697734779, 0.82579262940358511, 0.82579797813799627, 0.82581827400237151, 0.8258491471213194, 0.82584650609463617, 0.82587817404206976, 0.82588034126929188, 0.82588004726229902, 0.825881637536658, 0.82591361640957595, 0.82591424684775172, 0.82591129339637281, 0.82594057776366636, 0.8259385929912838, 0.82594396965823069, 0.82594099193765247, 0.82594857779341202, 0.82596693462106219, 0.82599555743803332, 0.82601172727729555, 0.82602451557085954, 0.82602310192913342, 0.82603336799643912, 0.82602967272605343, 0.82603074091454887, 0.82603489651640627, 0.82605680089036149, 0.82605585497737322, 0.82605769209155822, 0.82607092729408715, 0.82607032738767727, 0.82608100229126502, 0.82607433765065397, 0.82607920445675254, 0.82607932199643819, 0.82608081088525365, 0.82608399296054102, 0.82608700751056574, 0.8261019008822863, 0.82609868324997549, 0.82610999715966749, 0.82612642382884827, 0.82612269246653047, 0.82612818194852333, 0.82613494573531854, 0.826132739944066, 0.82613101072547102, 0.82614077580169964, 0.82614109675584246, 0.82614784523434892, 0.82615446949248317, 0.82615278865246711, 0.82615488093828904, 0.82616376071826969, 0.82619732116094968, 0.82619480562725511, 0.82622509415231149, 0.82622899954240481, 0.82622933826231437, 0.82623942682321838, 0.82623961864669071, 0.82627245197057453, 0.82628103428516897, 0.82628102867379349, 0.82627978297879479, 0.82628069878592125, 0.82633182348243683, 0.82633420979454275, 0.82633553721825925, 0.82633779792817652, 0.8263514714203668, 0.82635584490362035, 0.8263538668883843, 0.8263554546697609, 0.82635696169924133, 0.82636843802566096, 0.8263673183302831, 0.82637504254685368, 0.82637909494091277, 0.8263881099993059, 0.82638854863474298, 0.82638443073786849, 0.82638995645888291, 0.82639570245049598, 0.82639343927684161, 0.82639444224934044, 0.82639259330021098, 0.82640028099898577, 0.82639894569608841, 0.82640202845405497, 0.82640065345811053, 0.82640204387076577, 0.82640859957388346, 0.82641150243881578, 0.82641680048189747, 0.82641552981907052, 0.82643042882676254, 0.82643051592599603, 0.8264330209790911, 0.82643178142538287, 0.82643583084838679, 0.82643233352498735, 0.82644200260720757, 0.82644059182213714, 0.82644145705749028, 0.8264440642416726, 0.8264459943280642, 0.826442025617521, 0.82644858760204953, 0.82644587102746081, 0.82644184289002298, 0.82644414181181425, 0.82644251423134063, 0.82645402936932721, 0.82645970936626656, 0.82651319909781196, 0.82651157247830087, 0.82651094183722462, 0.82652345398687788, 0.82652533008202911, 0.8265468136278965, 0.82656515472548586, 0.82656255812244694, 0.82656470488307965, 0.82656354867401638, 0.82656761823526459, 0.82656517786801209, 0.82657345064834831, 0.82657356472215859, 0.8265963399426417, 0.82659949217785389, 0.82660532802183728, 0.82662469003278771, 0.82664074114954289, 0.82665656382931663, 0.82665836723528785, 0.82665901882620629, 0.82665571060710952, 0.82665273847035381, 0.82665039754166647, 0.82664704779505005, 0.82664817939742719, 0.82665390672724315, 0.82665997347399855, 0.82668134400707183, 0.82668080113318643, 0.82669751051378348, 0.82671667927297321, 0.82672954237311325, 0.82673243626252613, 0.82674421338532089, 0.82674502465245747, 0.82677150116398668, 0.82677235676278615, 0.82677389369550003, 0.82679599086817768, 0.82679419888926931, 0.82681502005050156, 0.82681601011697448, 0.82681610023261309, 0.82682217662353952, 0.82682324536089491, 0.82682436730158559, 0.8268277270509754, 0.82683219352770931, 0.82683496368569198, 0.82683684045749872, 0.82683701396833675, 0.82683474518738098, 0.82683241524953444, 0.82685583648689587, 0.82685698920655248, 0.82685474136412773, 0.8268544204787005, 0.82685463999015185, 0.82686293723715987, 0.8268765207322577, 0.82687937839787651, 0.82688408230716381, 0.82688632237313675, 0.82690362262425821, 0.8269293994216802, 0.82693008383085387, 0.82693084565594999, 0.82695668286951585, 0.82695969325374519, 0.82696295647976559, 0.82696332977448372, 0.8269743592930231, 0.82698023015559097, 0.82698591813476729, 0.82698389783723736, 0.82698190229916302, 0.8269843696764424, 0.82697924002885159, 0.82697703075641671, 0.82698819153803937, 0.82698525828044767, 0.82699554184719959, 0.8270079211699507, 0.82700626113341813, 0.82701034971503873, 0.82701172774148224, 0.82704452682340079, 0.82704987911304639, 0.82705278206049504, 0.82705527010974789, 0.82705816944263544, 0.82706646815027207, 0.82706417559624035, 0.82706781059444801, 0.8270652955946709, 0.82707229605408372, 0.82707152503389181, 0.8270888097617608, 0.82711058009949112, 0.82711258246749275, 0.82714112999907352, 0.82715210811364925, 0.82715337799675537, 0.8271571397145866, 0.82715458540248998, 0.82715548341977507, 0.82715680687057314, 0.8271603219871313, 0.82715969542801759, 0.82716297103044811, 0.82716015163370005, 0.82716208421156234, 0.82716164004027137, 0.82716438356356914, 0.82716866731613448, 0.82717513127970843, 0.8271818133491875, 0.82718305601617581, 0.82717866371304694, 0.82721564273439019, 0.82721828764264238, 0.82721726370133219, 0.82721639001895275, 0.82721318443931158, 0.82723210861110608, 0.82723041557620647, 0.8272315539184627, 0.82723465205856805, 0.8272340397773863, 0.82723546471658782, 0.82723678544919266, 0.82724123634123148, 0.82724631289409234, 0.82724527674554515, 0.82724795159925235, 0.82724789163847634, 0.82726496591385534, 0.82726788372253535, 0.82727221807624962, 0.82728047059597853, 0.82729134060724685, 0.82730022402650838, 0.82730360062256469, 0.82730154493167496, 0.8273085296219479, 0.82731980492953539, 0.82733897748657326, 0.82734060174675295, 0.82733876392709005, 0.82733874352955306, 0.82733768417584808, 0.82733487747525358, 0.82733528250639166, 0.8273362642950518, 0.82733751211917439, 0.82733752732220689, 0.82734575151432599, 0.82734551341353324, 0.8273436012742712, 0.8273533456676837, 0.82735457329488504, 0.82735443483487559, 0.82738901674857002, 0.82738647999232207, 0.82738709824075518, 0.82738526403121748, 0.82738209720347045, 0.82739014136815281, 0.8273931549892648, 0.82739229625439692, 0.82739031527604256, 0.8273939265390613, 0.82739572780118475, 0.82740081597415238, 0.8273956043533659, 0.82740946356383982, 0.82740731792466737, 0.82740402832684712, 0.82740341012372287, 0.82741191960063976, 0.82741364603080425, 0.82740996305998171, 0.82740793948201163, 0.82741185410564899, 0.82741574188492406, 0.82741650679891043, 0.82741365185688154, 0.82744925130759006, 0.82748478203349851, 0.82750242971565524, 0.82751202708998917, 0.82750863762239601, 0.82752938094563788, 0.82754221350558377, 0.82759280111461897, 0.82759428743295937, 0.82759263357727553, 0.82759658366641131, 0.82759669173133132, 0.82759841434172132, 0.82759671850831285, 0.82759779605336781, 0.82759960320521286, 0.82760006812052023, 0.82760356831545112, 0.82760384192455272, 0.82760780939092038, 0.82761193329312233, 0.8276180174477199, 0.82761967444964313, 0.82763664059422415, 0.82763769128166209, 0.82763811159560219, 0.82763809360165497, 0.82765225992454872, 0.82765385395879787, 0.8276668941209977, 0.82767227115012165, 0.82768515239457852, 0.82769836380591122, 0.82769649702370884, 0.82770576087131253, 0.82773218617431699, 0.82773452151752003, 0.82773212928767559, 0.82773409010349464, 0.82773369985314837, 0.82773507770238552, 0.82774945968314173, 0.82775712079935437, 0.82775784828530763, 0.82775667735621727, 0.82775786625693237, 0.82776815057520226, 0.82776551509904195, 0.82777802681755708, 0.8277761231977957, 0.82778603615467627, 0.82778398882663728, 0.8277861410478351, 0.82783807651982522, 0.82784261576254825, 0.82784179010186665, 0.82788332123018371, 0.82787704163025888, 0.82788266338691641, 0.82788159187810173, 0.82788075054784027, 0.82788601005299345, 0.82790188652605834, 0.82791376511394632, 0.82792828247033512, 0.82795684998760777, 0.82798317262240817, 0.82798187594588857, 0.82797627479652758, 0.82797482825837709, 0.82800071570480305, 0.82801131231207825, 0.82801251294784739, 0.82800912726015097, 0.82800772247189847, 0.82800319831380842, 0.82800035883318723, 0.82800004361658119, 0.82801267504828358, 0.82801570128372626, 0.82801279215719492, 0.82802878654018774, 0.82803478447536505, 0.82804590634252428, 0.82805299984812653, 0.82805467554885848, 0.82806030977327727, 0.8280633902570953, 0.82806210609870345, 0.82810991667131328, 0.82811255536649031, 0.82811165051357205, 0.82811492238021889, 0.82812384076717138, 0.82812435470839585, 0.82812503341695154, 0.82812221418285303, 0.82811807490891487, 0.82811914282723842, 0.82812280467130728, 0.82812316830984189, 0.82812733509006298, 0.82812934673743555, 0.82812883593062525, 0.82813000665029401, 0.82814141751970638, 0.8281424742114899, 0.82814070902808656, 0.82816353539875287, 0.82816685826099368, 0.82817071820150423, 0.82818530775563703, 0.82818541240868748, 0.82818305333702558, 0.82819104153056478, 0.82819169932661529, 0.82818740097659393, 0.82818886325441277, 0.82818987234478103, 0.82819424891009574, 0.8281899384127771, 0.8281924689841953, 0.82818672649714764, 0.82819712152782776, 0.82819923748373747, 0.82822530543372519, 0.82823766458520487, 0.82823315619261206, 0.82823009142528647, 0.82822812423551029, 0.8282408724433814, 0.82825734068415391, 0.82826814110868074, 0.82826756208316699, 0.82826805965488082, 0.82826866669774879, 0.82827446613803191, 0.82827511479444094, 0.82827564947937071, 0.82827681076341997, 0.82828085715981281, 0.82828139787078514, 0.82828356290955951, 0.8282815605415268, 0.82831049920332833, 0.82831240801287298, 0.8283079022142521, 0.82830976963882219, 0.82831108508245666, 0.82832594370674195, 0.82833116742557833, 0.8283317648607218, 0.82832768208709806, 0.82832802159474284, 0.82835956372741815, 0.82837191624644846, 0.82838727743233243, 0.82838762291165846, 0.82838454843959342, 0.82837908812673522, 0.82839368577153372, 0.82840871304373676, 0.82841194846448918, 0.82841027667147082, 0.82841168499256879, 0.8284067252946764, 0.82840790799510489, 0.82840900962391417, 0.8284073552014114, 0.82840990105298862, 0.8284193692895625, 0.82842152499182742, 0.82842238060021156, 0.82841811507398122, 0.82841735052608212, 0.8284227534879729, 0.82844527546947, 0.82844706078573505, 0.82845109523149563, 0.8284612235127129, 0.82846277271607105, 0.82846158367328171, 0.82850034680896856, 0.8285034387796173, 0.82850539969980175, 0.82851200746266773, 0.8285211337194287, 0.82853993457799668, 0.82853980565713647, 0.82853607438131083, 0.82854769389085003, 0.8285466941986156, 0.82854267201905252, 0.82854229123992373, 0.82854157375313342, 0.82854570720926657, 0.82854784150872085, 0.82854890077517263, 0.82855536062521296, 0.8285543757587458, 0.8285529324287193, 0.82855438582296181, 0.82854922823826926, 0.82856294043162271], 'auc-stdv': [0.0039273400275318315, 0.0043085610327870533, 0.0038098819680477234, 0.0044687531596371384, 0.0042159047256537466, 0.0041820955664082426, 0.0039181504551309438, 0.0040467122737633011, 0.0038499171926264287, 0.003975363499636805, 0.0040591386585339716, 0.0039813032676712518, 0.0040543504232780029, 0.0039651715319890318, 0.0039253212388777126, 0.0039796634240438344, 0.0041949040906498151, 0.0040995772601997655, 0.0041888896892650733, 0.0041746895549988113, 0.0040479374230362975, 0.0040249444091543475, 0.0040325168745440915, 0.0040758462489215918, 0.0040031496052437311, 0.0039812583529241774, 0.004007050059018467, 0.0040330921757906124, 0.0041419079497246967, 0.004137865721464227, 0.004029587338402865, 0.003998224220121872, 0.0039914355100869785, 0.0040332856165308387, 0.0039980465065628714, 0.0040841263862979099, 0.0040893009447046883, 0.0040534875144092571, 0.0041337547768786924, 0.0041375506981059622, 0.0041633025330643789, 0.0041861340755737489, 0.0041493641959830622, 0.0041146399934458879, 0.0041134634644072977, 0.0040698946116284086, 0.0040779184976563873, 0.0041004904146003237, 0.0040431164921629929, 0.0040767672429831113, 0.0040238232638279249, 0.0040260316673452756, 0.0040336311808636975, 0.0040582711564072947, 0.0041005035081929566, 0.0041339386647654287, 0.0041679544991744723, 0.0041607647074213385, 0.0041542924733056633, 0.0041576905514160659, 0.004177956247698518, 0.0041354794510804977, 0.0041683336585451002, 0.0042081105810509644, 0.0041792946502947842, 0.0041706829660660746, 0.00416195597083663, 0.004179061923843921, 0.0041634675483034197, 0.0041537874949196156, 0.0041769434984590421, 0.0041999302717510723, 0.0041836003224976388, 0.0042221967955730884, 0.0042521635524697919, 0.0042441115104505569, 0.0042555725487108042, 0.0042402362910615938, 0.0042325906406968528, 0.0042156052384462635, 0.0042028394295368652, 0.0042225164101980343, 0.0042508071186334517, 0.0042410364966005056, 0.0042365984341337683, 0.0042262009278293336, 0.004209951174898181, 0.0042314835682438386, 0.0042310418058758249, 0.0042224943143976115, 0.0042296222204819892, 0.0042631883973510134, 0.0042642544336301312, 0.0042726660421907548, 0.0042701850275948337, 0.0042663030702482421, 0.0042701981355426208, 0.0042625693458570564, 0.0043074128804789563, 0.0043203670596254334, 0.0042913742473014663, 0.0042719718739918158, 0.0042846697823152365, 0.0042702176597815937, 0.0042409894295024004, 0.0042566547420705061, 0.0042743096994062937, 0.0042510772918715571, 0.004263141296828336, 0.0042285321390297895, 0.0042234568127571528, 0.0042324703449817293, 0.0042130746459049535, 0.0042135893278274035, 0.0042043631354464796, 0.0042362173939291172, 0.0042310394954777125, 0.0042054330252530641, 0.0042168037287100349, 0.0042163742122947982, 0.004222070835680056, 0.0042174647145333003, 0.0042496659202878352, 0.0042294789086274303, 0.0042608916068872273, 0.0042666869457289834, 0.0042673673548187458, 0.0042958330887072913, 0.0043143180449395939, 0.004330981889150814, 0.0043243147133044356, 0.0043356087828354305, 0.0043354738249671201, 0.0043389528160333512, 0.0043101451867629814, 0.0043423628294098444, 0.004362077281146186, 0.004356449463016397, 0.0043469750444222188, 0.0043520580198346485, 0.0043460301268782439, 0.0043371292290693194, 0.0043172629239779667, 0.0043032789521715055, 0.0043060799573335969, 0.004317743992986664, 0.0043160741582139728, 0.0043076050110587025, 0.0042965938397558434, 0.004314764568722775, 0.004302739380002348, 0.0043125444759343082, 0.0043212868075922523, 0.0043400826540523673, 0.0043345645762559006, 0.0043483440648628639, 0.0043528616356600866, 0.0043567326149632447, 0.0043499290552015662, 0.0043644193031286372, 0.0043679129873906925, 0.0043640835276342162, 0.0043798889499010781, 0.0043711970052474555, 0.0043761098458126278, 0.0043722195905294154, 0.0043623118381087046, 0.0043605875100869414, 0.0043700839388941806, 0.0043577661453508059, 0.0043448452145892569, 0.004370542906847681, 0.0043470534652196337, 0.0043652484454217806, 0.00436070104276127, 0.0043486131821600703, 0.0043540599228291416, 0.0043610333785329509, 0.0043599879398277964, 0.0043638294221163745, 0.0043633867585370426, 0.0043733981001275075, 0.0043685413038655266, 0.0043906042825306588, 0.0043993353690964495, 0.0044076578201035282, 0.0044170156867137508, 0.004443758840787358, 0.004436294427857479, 0.0044477760500035851, 0.0044478969445346682, 0.0044366194541044859, 0.0044225303800959834, 0.0044212443961471216, 0.00444971817598521, 0.0044441096222507708, 0.0044682898580986263, 0.0044769527422805358, 0.0044864301809988414, 0.0044939857629067159, 0.0044961225794170716, 0.0044818490019655462, 0.0044685997777751648, 0.0044841769832613477, 0.0044829880355300236, 0.0044799626335756638, 0.0044612116922485194, 0.0044518930507709293, 0.0044538830718877593, 0.0044417605361507413, 0.0044383869740073226, 0.0044376537400813067, 0.0044405657517280588, 0.0044269042920652563, 0.0044192204764503659, 0.0044190286597076706, 0.0044501116399677789, 0.0044362528194658553, 0.004434532327460561, 0.0044405407624541378, 0.0044492446461866552, 0.0044649145424266552, 0.00447961606635179, 0.0044852916077186469, 0.0044914271982562509, 0.0044808731332512831, 0.0044730884839506851, 0.0044615040297493408, 0.0044233186171467953, 0.0044741221448338754, 0.0044742717234207167, 0.0044566580854304748, 0.0044614440002801042, 0.0044694505135624086, 0.0044495578811830222, 0.0044544377899570272, 0.0044552335865434885, 0.0044515322771047057, 0.0044526804441496048, 0.0044521203464290872, 0.004443079896512183, 0.0044432721124830305, 0.004463866768750214, 0.004443495131885032, 0.004436103661236074, 0.0044326807031385626, 0.0044308086854508858, 0.0044371645828523319, 0.0044312783391603043, 0.004434900080054708, 0.0044306834016807142, 0.004403979249791825, 0.0043870545627506747, 0.0043796338891726495, 0.0044088942934947016, 0.0043941871477022826, 0.0043925873216590578, 0.0043768216768787869, 0.0043702127083314318, 0.0043812855822397941, 0.0043587418781745272, 0.004364713673480753, 0.0043639276017118289, 0.0043715829877016407, 0.004361500743875325, 0.004342649589228015, 0.0043426212693742135, 0.0043374402624709527, 0.0043306789421793434, 0.0043395315372973418, 0.0043588282895667902, 0.0043403073079488225, 0.0043384337399329786, 0.0043260579348333794, 0.0043251237848630532, 0.0043153455052323498, 0.0043351388930225106, 0.0043365678297430195, 0.0043461411068270337, 0.0043456607283676937, 0.004347395010962889, 0.004359621665120824, 0.0043590242093063314, 0.0043553651071455354, 0.004370687164804864, 0.0043625376859850124, 0.0043556216480761982, 0.0043694970778275733, 0.0043630229906539263, 0.0043651641587605988, 0.0043712554305904237, 0.0043671444800223993, 0.0043634700006395426, 0.0043596123079637852, 0.0043403102368230369, 0.0043271141039861322, 0.0043413442720303883, 0.0043560546603579582, 0.0043314845604404857, 0.0043267884614833938, 0.0043283422153702084, 0.0043051460585614709, 0.0042898085918564545, 0.0042984131535893149, 0.0043033467666135959, 0.0042917917485676832, 0.0043054051170574453, 0.0042939665399049694, 0.0043084859480845856, 0.0043107925351135055, 0.0043027044288465096, 0.0042936323168658013, 0.0042914352276422676, 0.0043085193843432944, 0.0043080447554630615, 0.0043156125159976941, 0.0043327389942966156, 0.0043213732793246891, 0.004307496745339888, 0.0043015815235921233, 0.0042949304784715564, 0.0043044110700578703, 0.0043002509936022606, 0.0042982748183861118, 0.0042800645661854458, 0.0042772457445511098, 0.0042943643755317456, 0.0042791142858027622, 0.0042808453924408673, 0.0042755242395192067, 0.0042660373925117621, 0.0042905060011373484, 0.0042964415714369514, 0.004280168507845075, 0.0042742643693796222, 0.0042645755076308901, 0.0042649641836052868, 0.0042724361866416814, 0.0042652275225431076, 0.0042563222738580273, 0.0042651978055170568, 0.0042579434631448447, 0.0042539130570817211, 0.0042477015743609186, 0.0042338157308441041, 0.004233470627909971, 0.0042347849230698993, 0.0042266356073683729, 0.0042249101501293735, 0.0042275782659592937, 0.0042327610328996446, 0.0042519890974544094, 0.0042566249125975753, 0.0042458026311211171, 0.0042530340593186907, 0.0042446518666042496, 0.004245582681068347, 0.0042741063071457495, 0.0042750967002535833, 0.0042815886282222999, 0.0042783719094684966, 0.0042654818997772976, 0.0042760827380632586, 0.0042784317229368875, 0.00427524286433369, 0.0042696787832035114, 0.0042800504529246601, 0.0042827641616653986, 0.0042968102068445405, 0.0042950572502742455, 0.0042828483418847147, 0.0042745068237143727, 0.0042624529488870895, 0.0042461097235301911, 0.0042473251318198776, 0.0042514030273038987, 0.0042475414873821248, 0.0042514538007938371, 0.0042522952060804762, 0.0042312628521139388, 0.0042266151668439575, 0.0042216727042514073, 0.004226442785273659, 0.0042187498117405887, 0.0042216664826731278, 0.0042255517253602737, 0.0042382292314823468, 0.0042590645770976777, 0.0042605187200894668, 0.0042666266140888708, 0.0042693334454005005, 0.0042695414415319585, 0.0042684652393532446, 0.0042598983994466579, 0.0042503353210961497, 0.0042568829134768272, 0.0042583682888961399, 0.0042613085341760915, 0.004260693088855497, 0.0042724004921915583, 0.0042664500564556767, 0.0042793291437730413, 0.0042806170072456932, 0.0042769109495258912, 0.0042861680613411569, 0.0042796787442996101, 0.0042861321296791734, 0.0042722318169709367, 0.0042760946852767028, 0.0042884748224469235, 0.0042788264095805443, 0.004261804091892793, 0.0042575383836278881, 0.0042638710097965844, 0.0042583337001534998, 0.0042643929646549433, 0.0042728190198479225, 0.0042691788234586001, 0.0042681306348727127, 0.004293737272591038, 0.0043018863061641916, 0.0043054938865564928, 0.0043128451364676376, 0.0043148892387915123, 0.0043124976386309157, 0.0043167266610352056, 0.0043309012757254033, 0.0043307661646817455, 0.0043295109867398018, 0.0043153231585767891, 0.0043049904370848794, 0.0043081659082665541, 0.0043168757822531229, 0.0043149890414883865, 0.0043023149279147067, 0.0042977880099363394, 0.004300656518760779, 0.0042985665275850678, 0.0043043775505082248, 0.0042941733386726881, 0.0042937622736323941, 0.0043004421373058609, 0.0042865663052212713, 0.0042767654038589542, 0.0042740111628259041, 0.0042715570515892923, 0.0042625534626574034, 0.0042543984941283627, 0.0042602641021526125, 0.0042612624977133541, 0.0042561930461377856, 0.0042552558817806332, 0.0042533424154821229, 0.0042672666360087982, 0.0042562373626990248, 0.0042609300684468311, 0.004267327464522395, 0.0042665689896131519, 0.0042619686523792857, 0.0042517570961723731, 0.0042494891470252196, 0.0042444232998300703, 0.0042454118845809668, 0.0042387773004342469, 0.0042371049557373254, 0.004235350669541363, 0.004235638077350585, 0.0042364352359288851, 0.0042409003386805321, 0.0042421955216615801, 0.0042414436627442615, 0.004239534783064108, 0.0042323197774988526, 0.004245778396832817, 0.0042371633027746738, 0.0042352991697381989, 0.0042365941599004121, 0.0042241166323757021, 0.0042241791138050997, 0.004223883259055301, 0.0042225486979205499, 0.004205767804731364, 0.0042057506069897296, 0.0042060661076769694, 0.0042042908671757195, 0.0042129868455968288, 0.0042047879770889147, 0.0042058416118518141, 0.0042045215881269943, 0.0042026773919627012, 0.0041994096533484807, 0.0042009594365101256, 0.0041997700185539104, 0.0042024584639120694, 0.004199872086154465, 0.0042028787851243069, 0.0041969884694174828, 0.0041900420699076939, 0.0041877906045804649, 0.0041826567440284785, 0.0041826220907809604, 0.0041835063746081674, 0.0041846659006798083, 0.0041882452661498653, 0.004189139923584685, 0.0041814557963475753, 0.0041744971531372972, 0.0041764695866184521, 0.0041769695340617956, 0.0041825443245828656, 0.0041921472299798306, 0.004200866858353935, 0.004202436555638912, 0.0041912778336778015, 0.0041803123164800334, 0.0041686308028195335, 0.0041712791897353472, 0.0041770272081267865, 0.0041709210018515269, 0.0041735927120467595, 0.0041796200301225652, 0.0041784892030469556, 0.004186764797914415, 0.0041832075750360834, 0.0041850396571041825, 0.0041854034480012601, 0.004196570530543244, 0.004194047966449314, 0.0042017210727363285, 0.0041824567853073038, 0.0041845794225135961, 0.0041880592901085683, 0.0041808592706988777, 0.004184592827635023, 0.0041795301976332031, 0.0041743762969815119, 0.0041750765512888876, 0.0041705636880204197, 0.0041633099366976513, 0.0041608874418441683, 0.0041637572128453149, 0.0041573278609600825, 0.0041561564408212019, 0.0041578024387534515, 0.0041676491932478759, 0.0041690360941952099, 0.0041622916903510908, 0.004162324851936231, 0.0041516981034336667, 0.0041474920207668778, 0.0041501564208609445, 0.0041392162041566831, 0.0041296053479313397, 0.0041337928681704876, 0.0041290563090840005, 0.0041300940887321102, 0.0041319503438085516, 0.0041238125271554132, 0.0041173859737520168, 0.0041211783422513571, 0.0041316366434576363, 0.0041315656573923032, 0.0041301451562913575, 0.0041285654505604091, 0.0041356394124545445, 0.0041299724657409397, 0.0041336900069682313, 0.0041326970263234957, 0.004123955211382993, 0.0041252245643525857, 0.0041251110306152498, 0.0041148724697484609, 0.0041109337719250384, 0.0041073346742551741, 0.0041003252588984384, 0.0041073191732223261, 0.0041018525568592918, 0.004099121708328473, 0.0041061024082414705, 0.0041066327720409371, 0.00410277138384583, 0.0041012729235823768, 0.0041004504191387951, 0.0040995013012201185, 0.0040974639025974039, 0.0041060786198333062, 0.0041074163443360189, 0.0040984409783708384, 0.0041021852428689632, 0.0040946228265191969, 0.0040867534996615129, 0.0040801369225855469, 0.0040794857095983227, 0.0040751898132095066, 0.004078980373567972, 0.004077142508003455, 0.0040740066852615409, 0.0040745896645934193, 0.0040732392916466272, 0.0040751012198177779, 0.0040751975427929556, 0.0040691984300168247, 0.0040701476744261266, 0.0040808857598621465, 0.0040712440721842489, 0.0040725617282104352, 0.0040681693894350736, 0.0040658058216126083, 0.0040678334561767143, 0.0040666484113791391, 0.0040724071669225531, 0.0040633803462592861, 0.0040599696988515149, 0.0040701735349796837, 0.0040705620158450516, 0.0040741163989759131, 0.0040815767385973889, 0.0040774724478194278, 0.0040768572800321803, 0.0040740713415157422, 0.0040721688875252062, 0.0040770438472886349, 0.0040636303288541066, 0.0040694353641276236, 0.0040782490677629445, 0.0040876579330291277, 0.0040880918625872979, 0.0040868986072407415, 0.0040862597569772674, 0.0040899640165293774, 0.0040854344200698459, 0.0040926234353535727, 0.0040908250372235421, 0.0040908450835307154, 0.0040837081382391156, 0.0040888566652701915, 0.0040853676703834513, 0.0040823432972496854, 0.0040802078464427143, 0.0040815163199108039, 0.0040858482795118533, 0.0040806679483353569, 0.0040643320561817974, 0.0040722288221638247, 0.0040694919816874793, 0.0040685107729019395, 0.0040727712415582007, 0.0040675707468154614, 0.0040677596799915594, 0.0040625510813292387, 0.0040602552355225559, 0.0040614768108836281, 0.0040603460175325963, 0.0040647770805185002, 0.0040620085100373524, 0.0040589567140667032, 0.0040659900670796823, 0.0040681116046846193, 0.0040672418143046802, 0.0040620485224778496, 0.0040650795560813495, 0.0040712670806744003, 0.0040710062535196014, 0.0040722471177420724, 0.0040657354100059953, 0.0040647275365220991, 0.004064074675757594, 0.0040560729202929859, 0.0040640724182951123, 0.0040682193619626531, 0.0040663381803616736, 0.004064681722285338, 0.004062728214290089, 0.0040652027175131543, 0.0040614420342543001, 0.0040584416116913064, 0.0040604295674791105, 0.0040642856281103033, 0.0040664769608054234, 0.0040681637522349606, 0.0040700743535353754, 0.0040727683776905482, 0.0040721155521539956, 0.0040681468888675628, 0.0040682293670596219, 0.0040745566751674758, 0.0040734278466857581, 0.0040752458295408214, 0.0040694707672036153, 0.0040713828491927995, 0.0040700476642189327, 0.0040719130155266086, 0.0040816265511960425, 0.0040772773261154522, 0.0040723292069161816, 0.0040687403798864144, 0.0040685331894812693, 0.0040695852813658331, 0.0040722563624996828, 0.0040784170659147806, 0.0040802941994357925, 0.0040847252651558912, 0.0040821175271847218, 0.0040856340333713971, 0.0040865656397334948, 0.0040893063091052726, 0.0040888609421133419, 0.0040764523303290761, 0.0040803398024818289, 0.0040794325765597123, 0.0040795556767371033, 0.0040745581772018754, 0.0040697528127612605, 0.0040709371353383622, 0.0040706786232695101, 0.0040765059259673326, 0.0040714251263843952, 0.0040693959850426351, 0.0040629769700684139, 0.0040680967750339279, 0.0040687764911738096, 0.0040638492315995143, 0.0040634884381031142, 0.0040632344101398333, 0.0040661135668045179, 0.0040686345739077033, 0.0040748694251050988, 0.0040680413954505695, 0.0040711842046117471, 0.0040668556869590654, 0.0040645281616024731, 0.0040620622269991555, 0.0040594299161595526, 0.004062284053567667, 0.0040604524261026627, 0.004055388059742147, 0.0040498878565785853, 0.0040534540511520303, 0.0040530792782964191, 0.0040577845683834029, 0.0040527403820526595, 0.0040434277643184051, 0.0040371978832008252, 0.0040415109487008305, 0.0040452266113274928, 0.0040424709721478625, 0.0040409103546651462, 0.0040505969797216389, 0.0040574035847206914, 0.0040539099860757944, 0.0040561374757025733, 0.0040530599781324556, 0.004044733079220415, 0.0040488285150444715, 0.0040386750941935187, 0.0040325421699458916, 0.0040355968264762128, 0.0040410965243866913, 0.0040429139506169189, 0.0040430509297813225, 0.0040374425328283262, 0.0040352210742057798, 0.004039240618019399, 0.0040440307772539544, 0.0040362211348944285, 0.0040424107162226278, 0.0040372128958831117, 0.0040282339689931956, 0.0040311632426238966, 0.0040337865312827011, 0.0040349702503607232, 0.0040280624180400884, 0.0040274611930811444, 0.0040320172826482596, 0.0040254415328722518, 0.0040276404803625768, 0.00402170731133435, 0.0040241764561866249, 0.0040287198681644527, 0.0040286622451171962, 0.0040315066009552498, 0.004033306642933116, 0.0040371033242929243, 0.0040210036905600488, 0.0040206883098505416, 0.0040172542677514621, 0.0040219973454852388, 0.0040238434897327847, 0.0040194630406143062, 0.0040210081789137974, 0.0040207735951193367, 0.0040186295301801473, 0.0040173064597783092, 0.0040233895495359537, 0.0040175628276606819, 0.0040184275407204101, 0.0040152270770425838, 0.0040091645377968105, 0.0040015413565334277, 0.0040066588155095988, 0.0040092526818167164, 0.0040023936172708897, 0.004004535178569058, 0.0039939026506983343, 0.0039959870306839427, 0.0039968237101083489, 0.0039918304833523504, 0.0039878661842296598, 0.0039922780914325787, 0.0039914261885984469, 0.0039981581845205862, 0.0040002891371823538, 0.0040063051889951936, 0.0040099647468930348, 0.0040167387636180791, 0.0040204560406582306, 0.0040153413217000628, 0.0040168103163837638, 0.0040151197479093882, 0.0040099579608498309, 0.0040152968395169878, 0.0040210432651295765, 0.0040254437824724827, 0.0040233634760141459, 0.0040196852967032872, 0.0040236501894427923, 0.0040225345525419891, 0.0040235379426494869, 0.004017103049135962, 0.004015517896972293, 0.0040201518993179287, 0.0040196407654444141, 0.0040201150229932361, 0.0040284436565492521, 0.0040315182634943943, 0.0040305379387101711, 0.0040361362631948939, 0.00403137348345259, 0.0040261307693053592, 0.0040265194858763497, 0.0040289506577399024, 0.0040313775846621738, 0.0040318810244177163, 0.0040320673240363137, 0.0040301536634494133, 0.0040321780536200251, 0.004034718301810597, 0.004037229457291726, 0.0040392317238655785, 0.0040384324274900657, 0.00404459877045572, 0.0040462038872276108, 0.004048674487022748, 0.0040496949756594503, 0.0040484090767439326, 0.0040467557597292604, 0.0040477704390166408, 0.0040480605407275392, 0.0040429411877173881, 0.0040376639983060224, 0.004049584046386466, 0.0040586230138171852, 0.0040542012444618244, 0.0040506032721915886, 0.0040439382435176726, 0.0040409552241427371, 0.0040378923392428876, 0.0040373434468262996, 0.0040370078798830367, 0.0040365908191890313, 0.0040386220203355435, 0.0040327002649619631, 0.004029607382397089, 0.0040237880781010208, 0.0040305255709199095, 0.0040281633191140368, 0.0040282758227457539, 0.0040355489368317024, 0.0040313043981710711, 0.0040324080464964312, 0.0040377956240424152, 0.0040377579940075398, 0.0040389414549447649, 0.0040418718427960845, 0.004042266731739296, 0.0040539335521112161, 0.0040557928784699087, 0.0040649373957652168, 0.0040639548649218163, 0.0040628199931810214, 0.004065304388777064, 0.0040584125391536115, 0.0040600156916343767, 0.0040560298546914626, 0.0040562561988817377, 0.0040590279249941644, 0.0040563347008371348, 0.0040560901224634396, 0.0040543209641765721, 0.0040527775809859048, 0.0040611372092656736, 0.004072026392948013, 0.0040731614173338745, 0.0040756414320899581, 0.0040755538800046716, 0.0040720531238947963, 0.0040671222556065258, 0.0040688421278865039, 0.0040641622547244972, 0.0040589796176240854, 0.0040559372475256587, 0.0040529873063944663, 0.0040575077779378072, 0.0040552235416458334, 0.0040528609005529492, 0.0040558224038882273, 0.0040529232188572751, 0.0040556974893381616, 0.0040551433911501294, 0.0040588452293875577, 0.004051792785950933, 0.0040570853854375042, 0.0040528329253235037, 0.0040432802749932103, 0.0040483390890794814, 0.0040522878266518163, 0.0040527872281907384, 0.00405627309668151, 0.0040567091049477054, 0.0040463173445504739, 0.0040448810331035505, 0.0040358263965181067, 0.0040420425534582532, 0.004043971316733147, 0.0040404861451768951, 0.0040347133310915688, 0.0040355733925598629, 0.004041248336193709, 0.0040416289328544562, 0.0040439036929549657, 0.0040464962091810146, 0.004050214308643019, 0.0040455413887616016, 0.004043302806609827, 0.0040448926257583675, 0.0040537526495390457, 0.0040525907625065588, 0.0040509240608212716, 0.0040529793776279549, 0.0040505786310696826, 0.0040555116842060169, 0.0040486390659807414, 0.0040539133595409839, 0.0040497020576206536, 0.0040506239094408245, 0.0040529071611376272, 0.0040650717110106149, 0.0040681318865817247, 0.0040622031196875241, 0.0040613980263868449, 0.0040559898445809489, 0.0040546958739142045, 0.0040556416099130233, 0.004056817220739699, 0.0040552922425347339, 0.0040442719493053187, 0.0040475435730097185, 0.0040491529491818775, 0.0040521978904863891, 0.0040622757161489463, 0.0040729992779263077, 0.004068641279882791, 0.0040672169179856599, 0.0040658244434502566, 0.0040576298340532337, 0.0040585196935963852, 0.0040599170730446195, 0.0040566310006959786, 0.0040542851398068331, 0.0040535841621102634, 0.0040575149023234329, 0.0040583698975387326, 0.0040614996548665348, 0.0040593075576102547, 0.0040568121082721811, 0.0040498240395372698, 0.0040547470428855484, 0.0040535927348172209, 0.0040552935350933227, 0.0040610959998800935, 0.0040588418418433091, 0.0040588929527189113, 0.0040620898678228644, 0.004058111758142571, 0.0040565386006359517, 0.0040536819013950679, 0.0040514085547725259, 0.0040511414143582693, 0.0040489864329043036, 0.0040492815186234567, 0.004050144042300881, 0.0040514782364529447, 0.0040498308895898227, 0.0040579582337666332, 0.0040637803648319679, 0.0040625400735735369, 0.0040595335879325557, 0.0040661890801310351, 0.0040655281755829697, 0.0040727615711597448, 0.0040695902060951606, 0.0040708951945047948, 0.0040642288420941345, 0.0040650721014407563, 0.0040618672127699269, 0.0040596402951546652, 0.0040608770016814094, 0.0040636787306924859, 0.0040628931640846275, 0.0040600565463466114, 0.0040639443403962068, 0.0040718801029738612, 0.0040749983079127316, 0.0040758244084400297, 0.0040794995797118934, 0.0040786006908032106, 0.0040779578270588373, 0.0040739434806771405, 0.0040706342848422514, 0.0040697568589782218, 0.004068508915032373, 0.0040678860465885609, 0.0040702644795216339, 0.0040686645578186121, 0.0040709560400149008, 0.0040616481724050435, 0.0040702381543794037, 0.0040704190550556333, 0.0040673244170585342, 0.0040662331351210826, 0.004067474926494119, 0.0040678860464404649, 0.0040648564677411552, 0.0040663520834007148, 0.0040714371827704392, 0.0040788184045874197, 0.0040869904236178695, 0.0040836551000930687, 0.0040796363569063059, 0.0040781795306553074, 0.0040809817915728472, 0.0040882786855570025, 0.0040849914222809936, 0.0040929821775607597, 0.0040939847829701758, 0.004096365461704529, 0.0040919845779911644, 0.0040983316782688311, 0.0040942229694967804, 0.0040952808404672398, 0.0040968858885524198, 0.0040961084343705241, 0.0040952801555954946, 0.0040968928321029382, 0.0040981150131591242, 0.0040993592404457739, 0.0041024892789441368, 0.004094654225808095, 0.004093900454491187, 0.004086052389314824, 0.0040876280911468351, 0.004089751907993551, 0.0040901573747541067, 0.0040853658752991306, 0.0040848972022873896, 0.0040849426713579348, 0.004094616737556827, 0.0040968571853613866, 0.0040992799365391734, 0.0040979109352026269, 0.0040958895942358508, 0.0041020342724784661, 0.0041105063822663218, 0.0041162976943036559, 0.0041084260123927744, 0.0041095112198119999, 0.004113065242078163, 0.0041105380284667346, 0.0041172005344569437, 0.0041152337292705253, 0.0041156958049184105, 0.0041209287391536075, 0.0041176346935752652, 0.0041204418730715559, 0.0041197105054780209, 0.0041213606535761196, 0.004123434605814635, 0.0041269215488213568, 0.0041294515918958424, 0.0041294102776897809, 0.004129050379492045, 0.0041291425266277437, 0.0041270419094391496, 0.0041240200128714877, 0.00412274149654588, 0.0041305149417099115, 0.0041275842464692184, 0.0041320345828680296, 0.0041380159418880243, 0.0041430955768442653, 0.0041410381145321577, 0.004140507999668073, 0.0041394825890260058, 0.0041448191791817598, 0.0041448857982085063, 0.0041436536501636432, 0.0041392668232579271, 0.0041406734307775596, 0.0041366480398844817, 0.004141328263313677, 0.0041397763887550546, 0.0041382059521010558, 0.0041326385945334339, 0.0041352988501787312, 0.0041315103065298741, 0.0041375242485719305, 0.0041347358559808284, 0.0041259397638608118, 0.0041183641412220759, 0.0041145839020857635, 0.0041167446177373807, 0.0041180104714097689, 0.0041278920185184916, 0.0041237848493737815, 0.0041265786881581916, 0.00412666281299313, 0.0041236780748782932, 0.004124983275735121, 0.0041298761550626027, 0.0041306149264591129, 0.0041282073705183296, 0.0041329086604885183, 0.0041289253873833592, 0.0041206031506915196, 0.0041128829871676636, 0.004112653970215892, 0.0041090022893199019, 0.0041076121087857236, 0.0041148461581637114, 0.0041164250089094929, 0.0041110739437673844, 0.0041082158046375711, 0.0040951647764692787, 0.004092593687969763, 0.0040929129875097309, 0.0040870790644217782, 0.0040829113562138994, 0.0040803601042685459, 0.0040780149248138154, 0.0040795323251386377, 0.0040745340389498784, 0.0040725876908065074, 0.0040688334100984561, 0.0040687862758004021, 0.0040668613109355687, 0.0040654588809530554, 0.0040634622051688779, 0.0040672583514350493, 0.0040756695466520835, 0.0040707901436062028, 0.0040729193700589222, 0.0040707852499009623, 0.0040639215666100938, 0.0040638051679914222, 0.0040630509102716336, 0.0040562870040455603, 0.0040600772066668456, 0.0040602188862163572, 0.0040555770963729629, 0.0040509490209567848, 0.0040494189521714814, 0.0040543415225454049, 0.0040546546177259689, 0.0040484314716350382, 0.0040389261933149369, 0.004039994578519523, 0.0040418873970303497, 0.0040331869388356414, 0.0040314112889750361, 0.0040336710416681429, 0.0040386184225779456, 0.0040350078570022705, 0.0040326668264339298, 0.0040413983034959453, 0.0040452526143483223, 0.0040477571740312277, 0.0040518483157899795, 0.0040514894487632463, 0.0040440601615783119, 0.0040386809164201978, 0.0040390995289469739, 0.004041324337428499, 0.0040377009447927527, 0.0040314661261264513, 0.0040280552456827025, 0.0040296712896175958, 0.0040307617314745622, 0.0040309332014244609, 0.0040342651590268303, 0.004037796402343123, 0.004038548905466603, 0.0040387784019831098, 0.0040432681329998464, 0.0040476499190908993, 0.0040513092485589602, 0.0040496618723753737, 0.0040501108748477515, 0.0040499786112828569, 0.0040538077427461497, 0.0040537897261828278, 0.0040512560131566906, 0.0040516851065059709, 0.0040492679437736396, 0.0040502318601827979, 0.0040547260811541657, 0.0040500176035443043, 0.0040508006999086712, 0.0040559940996777455, 0.0040516966782940386, 0.0040582865977566241, 0.0040572724459263113, 0.0040576493865467503, 0.0040615925350130644, 0.004065798555397287, 0.004063622963982866, 0.0040665921541802518, 0.0040665490989539527, 0.0040669653504581829, 0.0040684685332142596, 0.004068172870484589, 0.0040759028182741379, 0.0040731860161809371, 0.0040710419298010522, 0.0040716119961641001, 0.0040681047546030124, 0.0040711442510039508, 0.0040657134896367403, 0.0040693109405179817, 0.0040726092773704743, 0.0040725233557429973, 0.0040703986055198984, 0.0040702245775771761, 0.004063336418973734, 0.0040624936431687061, 0.0040687560127170026, 0.004069244784778262, 0.0040671659661878166, 0.0040693121346580567, 0.0040707406819322492, 0.0040656310067723692, 0.0040617667265352942, 0.0040650736641898721, 0.0040623674589116299, 0.0040544592421149651, 0.0040522790424020261, 0.0040558531196569674, 0.0040560769772566944, 0.0040522204976601557, 0.0040544906522553088, 0.004045439637842871, 0.0040498687555376627, 0.0040550574379370945, 0.0040515084546746596, 0.0040503793707960404, 0.0040438398911857559, 0.0040455474388983214, 0.0040462153504231902, 0.004042222453272278, 0.0040405041099269273, 0.0040435474265833804, 0.0040453998869471484, 0.0040480867610753098, 0.0040452538404022577, 0.0040454043844423467, 0.0040461482327253271, 0.0040469337181731338, 0.0040418322199949758, 0.0040440606604626129, 0.0040446312097695376, 0.0040411328882752184, 0.0040429975123798837, 0.0040466649482775174, 0.0040486301701041054, 0.0040433903527494711, 0.0040478374414247929, 0.0040416632435905818, 0.0040387670494955469, 0.0040331083410185915, 0.00403232222472788, 0.0040277711373117274, 0.0040233632648501846, 0.0040232119972430673, 0.0040255952085818045, 0.0040241808695365344, 0.004021813119901292, 0.004021646865040784, 0.0040210903989922765, 0.0040237799666035115, 0.0040242812887706387, 0.0040219239959605807, 0.0040243655163287274, 0.0040274235802487965, 0.0040243201050550841, 0.0040206121062138118, 0.0040222469920668299, 0.0040076538703014818, 0.0040093294993034552, 0.0040111120615620071, 0.0040098922087265388, 0.0040127494342112307, 0.0040129516194744516, 0.0040127733465379943, 0.0040174943141521929, 0.0040204665812167912, 0.0040189408685849526, 0.0040248181542670673, 0.0040379046585142164, 0.0040366208919429243, 0.0040392700459355932, 0.0040382245023740642, 0.004036453082812308, 0.0040412198171639029, 0.0040387646185296062, 0.0040437751049089742, 0.0040434534359968695, 0.004039085363464345, 0.0040443438519794121, 0.0040425425938939571, 0.0040426868239556383, 0.0040364522965766472, 0.0040390371705542371, 0.0040354435156813463, 0.004040569565266711, 0.0040422677657680588, 0.0040414848266358116, 0.0040459024768095247, 0.0040488248404264163, 0.0040450003709236364, 0.0040532551598465028, 0.004054498242601544, 0.0040436556955084418, 0.0040520406639043731, 0.0040516865463747767, 0.0040546077850882167, 0.0040526171510576791, 0.0040500402481960683, 0.0040530724294053555, 0.0040532352846546207, 0.0040482110604251097, 0.0040525949687390478, 0.0040582247194618859, 0.0040620418642486125, 0.0040565602894399068, 0.0040511204785383574, 0.0040484932828496094, 0.0040465639411772416, 0.0040368418835799724, 0.004042463830388969, 0.0040427156607371019, 0.0040416726251121292, 0.0040432662491666467, 0.0040463086886212305, 0.0040491554821024392, 0.0040490822826510699, 0.0040486884423360549, 0.0040469635551391549, 0.0040474450838320596, 0.004042812932599611, 0.0040407204887689284, 0.0040411070476002228, 0.0040417780874809064, 0.0040434779950838107, 0.0040446461167542103, 0.0040475025344852561, 0.0040518622925211113, 0.0040522670582471502, 0.0040509729902479417, 0.0040527573134258154, 0.0040521389588741652, 0.0040456796964850623, 0.0040488927800163979, 0.0040460101520594267, 0.0040443820858390277, 0.004050180717266939, 0.0040464760690466263, 0.0040428118388516968, 0.0040381989042240938, 0.0040409485421200051, 0.0040444895552127456, 0.0040433919281113954, 0.0040456947669355262, 0.0040452852598390479, 0.0040425723375807649, 0.0040441555828104594, 0.0040443816327841997, 0.0040406470442920159, 0.0040419313011151583, 0.0040462631791100953, 0.0040462499086338236, 0.004038242610299456, 0.0040358568188580413, 0.0040411414291431583, 0.0040392384378260581, 0.0040377673598231923, 0.004033232013624044, 0.0040311464814468097, 0.004032819906891757, 0.0040292651328369192, 0.0040345674380624233, 0.0040345331780587664, 0.0040337209950345107, 0.0040417488598765705, 0.0040447941357142116, 0.0040511907357537432, 0.0040486578224628597, 0.0040411815211425809, 0.0040399439874853511, 0.0040382950177275199, 0.0040414710787363079, 0.0040403675439220701, 0.0040407628824597292, 0.0040450634198711055, 0.0040461375941922822, 0.0040540488569876798, 0.0040617461582122694, 0.0040686447707743217, 0.0040621425243645487, 0.0040601214525160419, 0.0040583677182046054, 0.0040561812439261873, 0.0040582823600684194, 0.0040589964249030074, 0.0040592726173810639, 0.0040590339796216859, 0.0040543800726307797, 0.004055825760934992, 0.0040502590023425216, 0.0040519834527631866, 0.0040590261521166755, 0.0040623515794005237, 0.0040664258794478056, 0.0040677569697994027, 0.0040721239035392204, 0.0040649507848046799, 0.0040726018342257028, 0.0040737667908326584, 0.0040683976552069404, 0.0040659590654529738, 0.004066447724176349, 0.0040663334232411957, 0.0040655087180272434, 0.0040673624867909718, 0.004066475957664635, 0.0040645241104789879, 0.0040611592550413378, 0.0040611408132167227, 0.0040661668776249254, 0.0040689613526740638, 0.0040704204449937796, 0.0040684205195882809, 0.004063759620290491, 0.0040654330285727867, 0.0040662168261023932, 0.0040651902098119887, 0.0040717667523142032, 0.0040707583400810821, 0.0040734807790799629, 0.0040717709383830203, 0.0040746377013057356, 0.0040692415303008236, 0.0040753387467847577, 0.0040685580740647091, 0.0040704717819891175, 0.0040711619769051403, 0.0040730685731644933, 0.0040734981367144853, 0.0040728644449810536, 0.0040723482653717774, 0.0040723636888597104, 0.0040808008327517546, 0.0040719695129419952, 0.0040778792272135453, 0.0040806523025824683, 0.0040798612117440899, 0.0040768413782289242, 0.0040751941509128116, 0.0040713990901990651, 0.0040729503904691133, 0.0040661729987007235, 0.004065255535040454, 0.0040649125073174332, 0.0040680885140751931, 0.0040704511158783296, 0.0040694896416805819, 0.0040681096369566301, 0.0040701385027916339, 0.0040742068230820716, 0.004075335671609788, 0.004083770365648688, 0.0040852291952409416, 0.0040861536822840759, 0.0040838734409814241, 0.0040866004116301945, 0.0040877381278902804, 0.0040883301317600025, 0.0040865813035938989, 0.0040885847042523137, 0.0040910815592871495, 0.0040957018572900898, 0.0040997152635717593, 0.0041002832111952641, 0.0041016395565751148, 0.0041040099275760529, 0.0041054527049933153, 0.004102960647486489, 0.0040968481385112668, 0.0040994598222442004, 0.0041009689541385468, 0.0041035845787652774, 0.0041014404611230534, 0.0040982389738677683, 0.0040979853293799124, 0.0040949886365555981, 0.0040985538098909126, 0.0041022150249755427, 0.0041038531187457487, 0.0041014003100090339, 0.0041015760907297232, 0.0040974920333446997, 0.0041048812248600649, 0.0041051475264937808, 0.0041029016186081204, 0.0041070942190342963, 0.0041067611977703925, 0.0041060676521306374, 0.0041074724493309813, 0.0041067386116015406, 0.0041098434513042893, 0.0041133766421925034, 0.0041104358242561008, 0.004111618467397821, 0.0041084297354807902, 0.0041074498057399825, 0.0041079226086155485, 0.0041068199708813738, 0.004104629133365849, 0.0041015482340270842, 0.0041006653562397685, 0.0040932395975399882, 0.0040955588848407658, 0.0040964230709248008, 0.004098318674404829, 0.0040981241022533949, 0.0040947711719589754, 0.0040961672158851548, 0.0040966001882092188, 0.004097411344133898, 0.0040986443506921127, 0.0040924716248350382, 0.0040921283839013166, 0.0040921945560656488, 0.0040895227687037889, 0.0040885118162569548, 0.0040935370977361876, 0.0040923253344689976, 0.0040920093973061738, 0.0040929354484261363, 0.0040981931742892852, 0.0040996886154712397, 0.0041011043394443437, 0.0040973644365953693, 0.004095277258980126, 0.0040897493315094277, 0.0040861666663554886, 0.0040914494045082364, 0.0040930998820372377, 0.0040954995067631807, 0.0040976695381634417, 0.0040899844091603337, 0.0040854480196560452, 0.004084923094337215, 0.0040861224546873774, 0.0040893803377057324, 0.0040906903259077653, 0.0040901725257737912, 0.0040859985348246199, 0.0040851983503651353, 0.0040835350484961074, 0.004082463536168905, 0.0040840540400102392, 0.0040793412332388252, 0.0040765309197411765, 0.0040753681017476565, 0.0040798809142504888, 0.0040882211449676052, 0.0041010766790959188, 0.0041058368692917955, 0.0041017763880719527, 0.0041045920355508312, 0.0041020387000840538, 0.0041009659356559458, 0.0041053668333307343, 0.0041038300678266646, 0.0041065121956071144, 0.0041040259055784064, 0.0041089922760512423, 0.0041090986095470685, 0.0041074175564028226, 0.004107505861024807, 0.0041056541601257879, 0.0041070576599134372, 0.004114918909157576, 0.0041151911554786948, 0.0041104977667963151, 0.0041127034452775628, 0.0041124878902063459, 0.0041133384610420338, 0.0041155182092281663, 0.0041165321199834364, 0.0041196917694792, 0.0041096995882490533, 0.0041168775705990937, 0.0041177391600922368, 0.0041109195911115982, 0.0041096696687655679, 0.0041074399714756933, 0.0041039567026987505, 0.0041061723529687965, 0.0041059163988295269, 0.0041029952246520928, 0.0041042235659482732, 0.0041060008522655067, 0.0041070903977933866, 0.0041049742201431528, 0.0041050485483132455, 0.0041039293457107075, 0.004097320854076685, 0.0040980892775558785, 0.0040954840739229051, 0.0040986868513327229, 0.0040970780440727399, 0.0040941725576693899, 0.0040910029652487887, 0.0040911955146358432, 0.0040917791797880632, 0.0040876817710237138, 0.0040911772451359988, 0.0040937649865581769, 0.0040939579725034177, 0.0040880906047564254, 0.0040847533491130215, 0.0040887803934647167, 0.0040876435946040433, 0.0040862484979748061, 0.0040855688712208692, 0.0040832534839032666, 0.004083102452584004, 0.0040838663074712275, 0.0040809482802950901, 0.0040808594350839967, 0.0040821770042004839, 0.0040890547067563366, 0.0040897743699020014, 0.0040875096084104093, 0.0040925741445981883, 0.0040922173189430555, 0.0040900905868225951, 0.0040895407209192037, 0.0040914440367478553, 0.0040881409103713223, 0.0040817619767856929, 0.0040808033980674766, 0.0040800554660797113, 0.0040789950890607824, 0.0040826563124755477, 0.0040774184528697055, 0.0040791732512835358, 0.0040711460204209838, 0.0040688884476756584, 0.0040679038904096933, 0.00406461983931813, 0.0040642219534278816, 0.0040651842569692925, 0.0040656598425591484, 0.0040662064966451427, 0.0040628372838374073, 0.0040616455966886586, 0.004062134179200508, 0.0040621483568436768, 0.0040645701601543013, 0.004065117244655656, 0.0040683521371234436, 0.004071718139473054, 0.0040719759980289115, 0.0040711425705796077, 0.0040769261297914838, 0.0040758004050165378, 0.0040805187108473272, 0.0040734978497388909, 0.004074261979737784, 0.0040770867524033989, 0.0040759488674000669, 0.0040813078914016801, 0.0040806725360419249, 0.0040837071098342795, 0.0040814941322265394, 0.0040831516099233568, 0.0040810698268532106, 0.0040787090337458956, 0.0040779936536777415, 0.0040804081977857442, 0.0040827391549415702, 0.0040833295463894104, 0.0040874407914493404, 0.0040840405624593496, 0.0040810801381209703, 0.0040822901169962188, 0.0040871669897217687, 0.0040894494823205691, 0.0040958839552251037, 0.0040992696980510085, 0.0040982900918431174, 0.0041001558856701877, 0.0040954612996778432, 0.0040969676763060771, 0.004093582683048839, 0.00408933030009979, 0.0040949573901426263, 0.004095353081598742, 0.0040911614316714055, 0.0040885098420622122, 0.0040899115050278345, 0.0040888055505480395, 0.0040920037948255364, 0.0040948486859889877, 0.0040965436947373287, 0.0040991452633476946, 0.0041000192295573721, 0.0040984561474680927, 0.0040981211276930564, 0.0040939505688611438, 0.0040928614541900889, 0.0040917062432829282, 0.0040941032296587989, 0.0040958571599959692, 0.0040962229414240143, 0.0040972422966461947, 0.00409673463232876, 0.0041044240193550416, 0.0041023016207807931, 0.0041017691797947328, 0.0041012883746397661, 0.0041066131195460929, 0.0041052362684738586, 0.0041056983235491749, 0.0041082674473188127, 0.0041117638248856347, 0.0041075632504501839, 0.0041090847781231014, 0.0041060874338317054, 0.0041077693441831766, 0.0041059755492819172, 0.0041039595530019072, 0.0041006399004243736, 0.0041021094019578208, 0.004104196762698159, 0.0041105038565859165, 0.0041110164052980116, 0.0041123445081646575, 0.0041137991049838771, 0.0041152353052044338, 0.0041149213405980095, 0.0041146822826847078, 0.0041183879439774, 0.0041197275654179149, 0.0041189842489938672, 0.0041234779800880224, 0.0041297183928914695, 0.0041292244029886302, 0.0041370893189634564, 0.0041378010077677075, 0.0041391436720669252, 0.0041410153649450695, 0.0041418408295232139, 0.0041424660598888951, 0.0041449150868769543, 0.0041427256591972279, 0.0041409850685200133, 0.0041449341680087539, 0.0041463268964175978, 0.0041464869200748674, 0.0041493613408663966, 0.0041508309501155881, 0.0041520415800245748, 0.0041447199184286741, 0.004141495666628618, 0.0041392522319143215, 0.0041389759891037768, 0.0041393667000449441, 0.0041312225697611744, 0.0041305737893148203, 0.0041290408519443515, 0.0041298604779291285, 0.0041314645835313383, 0.0041302167546232883, 0.0041297616796073895, 0.0041291185281300278, 0.0041265678177375168, 0.0041290740477607363, 0.0041298272399878403, 0.0041353953474931116, 0.0041370474280692593, 0.004137294302251111, 0.0041328934522316361, 0.0041312077511909571, 0.0041297630136097798, 0.0041317207350064734, 0.0041332263251935347, 0.0041341157311620717, 0.0041364301974440576, 0.0041359603694609169, 0.0041371939764943758, 0.0041380203162048462, 0.0041347580485460445, 0.0041345210736703957, 0.0041379509722267709, 0.004132464129457304, 0.004132732500592394, 0.0041374634236176697, 0.0041411387706705152, 0.0041421774091416966, 0.0041415185267682795, 0.0041400686343773547, 0.0041402103745073679, 0.0041445102846992639, 0.0041440045048617997, 0.0041450413450715255, 0.0041487241263236481, 0.0041451163149261363, 0.004144121061673319, 0.0041455586439987941, 0.004145764023927332, 0.0041445145740668082, 0.0041403394483843512, 0.0041404581670122049, 0.0041376837099188577, 0.004135785975346612, 0.0041276726102485225, 0.0041283595821648075, 0.0041312075136983022, 0.0041282075243098612, 0.0041260803059474408, 0.0041280334434169947, 0.0041335119445993565, 0.0041347973049641901, 0.0041381451641848092, 0.0041406291094178297, 0.0041383452176559973, 0.0041416740365511299, 0.0041337269470842763, 0.0041400416228433001, 0.0041502420420701644, 0.0041535785902856613, 0.0041503286659040324, 0.0041502023395122919, 0.0041525588948347295, 0.0041594906644244166, 0.00416009363052773, 0.0041547844453782784, 0.0041568368556858856, 0.0041594394792902308, 0.0041604941896382181, 0.0041653495117422949, 0.004162582646824153, 0.0041591402478135166, 0.0041627014337829856, 0.0041572197377524522, 0.0041576711538511453, 0.0041537285175456138, 0.0041556617870094591, 0.0041551861503567877, 0.0041574632645767096, 0.0041526336766399951, 0.0041545604148376198, 0.0041571927031070419, 0.0041586907124524829, 0.0041599363308339023, 0.0041623214800878853, 0.0041624688008443742, 0.0041614867498157111, 0.0041647468749105177, 0.0041661765406955731, 0.0041612621625737659, 0.0041650231033801546, 0.0041659176475026495, 0.0041711422540617205, 0.0041661350264363597, 0.004164824589078105, 0.0041651828146127738, 0.0041689946893291828, 0.0041602068200824408, 0.0041614907891393965, 0.0041646239400084687, 0.0041629225194628533, 0.0041594969880749226, 0.0041625363607634873, 0.0041598771749597084, 0.004161709716502311, 0.0041588908114370991, 0.0041585944789337831, 0.004159110743533815, 0.0041621163779937062, 0.0041617124562684437, 0.0041584157075549568, 0.004159335566250014, 0.0041621565441061822, 0.0041605335150218838, 0.0041599291947345754, 0.0041593232242572633, 0.0041590475595330622, 0.0041590982948663154, 0.0041587424283913866, 0.0041567163093589182, 0.0041533662838803718, 0.0041518546817588906, 0.0041527203702074493, 0.0041507906075765053, 0.0041497401996442463, 0.0041531763996261531, 0.0041514218189944432, 0.0041548796634281408, 0.0041548347974319768, 0.004154059858836867, 0.0041568344058410967, 0.0041549008148478016, 0.0041523552572645447, 0.0041514704791444333, 0.0041516210108393922, 0.004147444024451827, 0.0041511100456499434, 0.004155290643329204, 0.0041602948857017725, 0.0041604806804840608, 0.0041619550858148892, 0.0041628641710927125, 0.0041611107370064394, 0.0041582005613598032, 0.0041597978141468027, 0.0041615589620167511, 0.0041624171018500647, 0.0041597566811650112, 0.0041542506634651361, 0.0041549066801118443, 0.0041594409238599897, 0.0041543478557007586, 0.0041598642920200263, 0.0041599982578547018, 0.0041596921533517475, 0.0041593705358764473, 0.0041575932508270642, 0.0041569005317410321, 0.0041532314441932892, 0.0041545325877207651, 0.0041546968803061269, 0.0041514844341652883, 0.0041523524009894862, 0.0041526305530171503, 0.0041535184385573789, 0.0041538106131049982, 0.0041540314913706758, 0.0041524379595138583, 0.0041543813636451439, 0.0041550505770363488, 0.0041599317145210009, 0.0041597335452537949, 0.0041521121692205164, 0.0041498559247970532, 0.0041457545247101379, 0.0041408116755820307, 0.0041459036372811986, 0.0041355961727889239, 0.0041370298035163337, 0.0041428879649473508, 0.0041426830816639719, 0.0041432265008024974, 0.0041412873103246189, 0.0041444705288087268, 0.0041429759316758755, 0.0041419531284013723, 0.0041433483057483518, 0.0041443879589567042, 0.0041425007940536608, 0.0041424042026596995, 0.0041394750218790369, 0.0041348669967219889, 0.0041367636004955213, 0.0041393981141242308, 0.0041388004902981976, 0.0041257964831385857, 0.0041247648256895458, 0.0041259837032703427, 0.0041265702985380671, 0.0041216774763803169, 0.0041213659028944746, 0.0041211349056245851, 0.0041249681328166237, 0.004123615245144879, 0.0041241376633384813, 0.0041237856934546371, 0.0041258175300671381, 0.0041215379970955691, 0.0041194005025260065, 0.0041174454474970012, 0.004118506516620375, 0.0041262535235232871, 0.0041262187833362848, 0.0041279170242520475, 0.0041329821420389148, 0.0041292032848243843, 0.0041251750506286324, 0.0041249370956604765, 0.0041247262867414565, 0.0041198131833622663, 0.0041195862195723115, 0.0041195793251764803, 0.0041173097272140507, 0.004116085788567707, 0.0041192708181372805, 0.0041167799605737743, 0.0041201409833664078, 0.0041215893919358381, 0.0041204491306746681, 0.0041173775954977121, 0.0041219921870340433, 0.0041243668873563351, 0.0041247725724461752, 0.0041263211746396748, 0.0041320711550648101, 0.0041324565504721198, 0.0041318139438764438, 0.0041289615020731318, 0.0041277028213538864, 0.0041296916323349505, 0.0041274698122257961, 0.0041264211337537215, 0.0041273804238683683, 0.0041292699088326433, 0.0041283279565713293, 0.0041285621043526729, 0.0041261325168177243, 0.0041269480947931647, 0.0041286798463846331, 0.0041285896733526803, 0.0041275589057450631, 0.0041262472416520157, 0.0041266449308321523, 0.0041261044886889099, 0.0041287323568808282, 0.0041268630342608066, 0.0041271698889537977, 0.0041242984818459947, 0.0041226717835659227, 0.0041233170098614628, 0.0041193740982431015, 0.0041222285466709736, 0.0041251956541538178, 0.0041252695287473376, 0.0041240815068348717, 0.0041268732981374172, 0.0041292876709645683, 0.0041251391093847078, 0.0041235554394378274, 0.0041246779064527854, 0.004121993520706857, 0.0041184579381388286, 0.0041209514676792524, 0.0041195923034120646, 0.0041190421242729347, 0.0041254034007210932, 0.0041237153560114165, 0.004121731914477525, 0.0041225392008101321, 0.0041224375097629311, 0.0041214600593397965, 0.0041220933425586545, 0.0041205352327263103, 0.0041222481224550177, 0.0041194453704447402, 0.0041169114045262953, 0.004117036973153518, 0.0041147526007004791, 0.0041163958678521097, 0.0041175247831781638, 0.0041153922532875392, 0.0041152804750902413, 0.0041151232319642343, 0.0041181545458570312, 0.0041121396084552279, 0.0041093284643011407, 0.004108878938159013, 0.0041041500918394206, 0.0041068111333851147, 0.0041069304688428239, 0.0041106285829082684, 0.0041050743327879493, 0.0041051759299150573, 0.0041052922110413797, 0.0041097648399002328, 0.0041019803004182015, 0.0041048263004858651, 0.004102764600239223, 0.0041010433774548776, 0.0041059734690363902, 0.0040991894733031931, 0.0041001471856685319, 0.004101495358906527, 0.0041005148298834514, 0.004101445560831537, 0.0041000473204796184, 0.0040994196636949176, 0.0040974157225061904, 0.0040988105690373793, 0.0040993383373658758, 0.0041033818537724431, 0.004099136769979808, 0.0041002754775745732, 0.0041030895552569766, 0.0041042191274178646, 0.0041077581552689648, 0.0041102681330333311, 0.0041102338935728967, 0.0041135446909697729, 0.0041086721653349487, 0.0041075856201947753, 0.0041054996639883498, 0.0041039869434019317, 0.004102734187471359, 0.0041051626819123077, 0.0041113431797706931, 0.0041108251295858119, 0.004110065502832703, 0.0041128207331201832, 0.0041125287932743714, 0.0041146331700089595, 0.0041111633898649634, 0.0041107897140041615, 0.0041058202747455055, 0.0041047047331959933, 0.0041069554998724488, 0.0041116120569319068, 0.0041085699798078471, 0.0041105450171816217]}\n"
     ]
    }
   ],
   "source": [
    "# Now w/ stratified cv\n",
    "\n",
    "cv_result = lgb.cv(\n",
    "params3, \n",
    "lgb_train, \n",
    "nfold=10,\n",
    "stratified=True,\n",
    "num_boost_round=5000, \n",
    "early_stopping_rounds=20,\n",
    "metrics=['auc','rmse'],\n",
    "verbose_eval=100, \n",
    "show_stdv=False,\n",
    "seed=2002\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2162\n"
     ]
    }
   ],
   "source": [
    "print(len(cv_result['auc-mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x119199518>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD3CAYAAADmBxSSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvgy/5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c5613c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check AUC vs. iter\n",
    "plt.plot(cv_result['auc-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train until valid scores didn't improve in 40 rounds.\n",
      "[40]\tvalid_0's auc: 0.778693\n",
      "[80]\tvalid_0's auc: 0.781729\n",
      "[120]\tvalid_0's auc: 0.784811\n",
      "[160]\tvalid_0's auc: 0.788501\n",
      "[200]\tvalid_0's auc: 0.791614\n",
      "[240]\tvalid_0's auc: 0.794324\n",
      "[280]\tvalid_0's auc: 0.797948\n",
      "[320]\tvalid_0's auc: 0.800463\n",
      "[360]\tvalid_0's auc: 0.802721\n",
      "[400]\tvalid_0's auc: 0.804959\n",
      "[440]\tvalid_0's auc: 0.807072\n",
      "[480]\tvalid_0's auc: 0.80866\n",
      "[520]\tvalid_0's auc: 0.81028\n",
      "[560]\tvalid_0's auc: 0.811517\n",
      "[600]\tvalid_0's auc: 0.812615\n",
      "[640]\tvalid_0's auc: 0.813945\n",
      "[680]\tvalid_0's auc: 0.814974\n",
      "[720]\tvalid_0's auc: 0.815693\n",
      "[760]\tvalid_0's auc: 0.816448\n",
      "[800]\tvalid_0's auc: 0.816981\n",
      "[840]\tvalid_0's auc: 0.817617\n",
      "[880]\tvalid_0's auc: 0.818428\n",
      "[920]\tvalid_0's auc: 0.819228\n",
      "[960]\tvalid_0's auc: 0.81977\n",
      "[1000]\tvalid_0's auc: 0.820248\n",
      "[1040]\tvalid_0's auc: 0.820727\n",
      "[1080]\tvalid_0's auc: 0.821267\n",
      "[1120]\tvalid_0's auc: 0.821802\n",
      "[1160]\tvalid_0's auc: 0.822288\n",
      "[1200]\tvalid_0's auc: 0.822684\n",
      "[1240]\tvalid_0's auc: 0.823015\n",
      "[1280]\tvalid_0's auc: 0.823467\n",
      "[1320]\tvalid_0's auc: 0.823928\n",
      "[1360]\tvalid_0's auc: 0.824238\n",
      "[1400]\tvalid_0's auc: 0.8247\n",
      "[1440]\tvalid_0's auc: 0.825036\n",
      "[1480]\tvalid_0's auc: 0.825325\n",
      "[1520]\tvalid_0's auc: 0.825778\n",
      "[1560]\tvalid_0's auc: 0.826156\n",
      "[1600]\tvalid_0's auc: 0.826398\n",
      "[1640]\tvalid_0's auc: 0.826728\n",
      "[1680]\tvalid_0's auc: 0.827144\n",
      "[1720]\tvalid_0's auc: 0.82741\n",
      "[1760]\tvalid_0's auc: 0.827609\n",
      "[1800]\tvalid_0's auc: 0.827835\n",
      "[1840]\tvalid_0's auc: 0.828113\n",
      "[1880]\tvalid_0's auc: 0.828347\n",
      "[1920]\tvalid_0's auc: 0.828453\n",
      "[1960]\tvalid_0's auc: 0.8285\n",
      "[2000]\tvalid_0's auc: 0.828838\n",
      "[2040]\tvalid_0's auc: 0.829177\n",
      "[2080]\tvalid_0's auc: 0.829274\n",
      "[2120]\tvalid_0's auc: 0.829447\n",
      "[2160]\tvalid_0's auc: 0.829575\n",
      "[2200]\tvalid_0's auc: 0.829745\n",
      "[2240]\tvalid_0's auc: 0.830023\n",
      "[2280]\tvalid_0's auc: 0.83021\n",
      "[2320]\tvalid_0's auc: 0.830297\n",
      "[2360]\tvalid_0's auc: 0.830392\n",
      "[2400]\tvalid_0's auc: 0.83044\n",
      "[2440]\tvalid_0's auc: 0.830568\n",
      "[2480]\tvalid_0's auc: 0.830755\n",
      "[2520]\tvalid_0's auc: 0.830954\n",
      "[2560]\tvalid_0's auc: 0.830982\n",
      "[2600]\tvalid_0's auc: 0.831125\n",
      "[2640]\tvalid_0's auc: 0.831295\n",
      "[2680]\tvalid_0's auc: 0.831433\n",
      "[2720]\tvalid_0's auc: 0.831582\n",
      "[2760]\tvalid_0's auc: 0.831722\n",
      "[2800]\tvalid_0's auc: 0.831794\n",
      "[2840]\tvalid_0's auc: 0.831921\n",
      "[2880]\tvalid_0's auc: 0.832029\n",
      "[2920]\tvalid_0's auc: 0.832183\n",
      "[2960]\tvalid_0's auc: 0.832338\n",
      "[3000]\tvalid_0's auc: 0.832413\n",
      "[3040]\tvalid_0's auc: 0.83254\n",
      "[3080]\tvalid_0's auc: 0.832563\n",
      "[3120]\tvalid_0's auc: 0.832578\n",
      "[3160]\tvalid_0's auc: 0.832753\n",
      "[3200]\tvalid_0's auc: 0.832835\n",
      "[3240]\tvalid_0's auc: 0.832998\n",
      "[3280]\tvalid_0's auc: 0.833094\n",
      "[3320]\tvalid_0's auc: 0.83323\n",
      "[3360]\tvalid_0's auc: 0.833312\n",
      "[3400]\tvalid_0's auc: 0.833413\n",
      "[3440]\tvalid_0's auc: 0.833478\n",
      "[3480]\tvalid_0's auc: 0.833581\n",
      "[3520]\tvalid_0's auc: 0.833705\n",
      "[3560]\tvalid_0's auc: 0.833778\n",
      "[3600]\tvalid_0's auc: 0.833863\n",
      "[3640]\tvalid_0's auc: 0.833975\n",
      "[3680]\tvalid_0's auc: 0.834019\n",
      "[3720]\tvalid_0's auc: 0.834104\n",
      "[3760]\tvalid_0's auc: 0.834155\n",
      "[3800]\tvalid_0's auc: 0.834218\n",
      "[3840]\tvalid_0's auc: 0.834341\n",
      "[3880]\tvalid_0's auc: 0.83434\n",
      "[3920]\tvalid_0's auc: 0.834416\n",
      "Early stopping, best iteration is:\n",
      "[3909]\tvalid_0's auc: 0.834432\n",
      "AUC Score:\n",
      "0.834431701798\n",
      "\n",
      "LogLoss:\n",
      "0.391118041887\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.835     0.967     0.896     25951\n",
      "       True      0.779     0.381     0.512      8023\n",
      "\n",
      "avg / total      0.822     0.828     0.805     33974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit final LightGBM gradient-boosted tree model.\n",
    "# Go ahead and use early stopping, although it's technically a minor source of overfitting.\n",
    "# It would be more statistically \"correct\" to use CV rounds from above (2162), \n",
    "#   but, empirically, using early stopping produces better results on e.g. Kaggle.\n",
    "\n",
    "gbm = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    num_leaves=127,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=5000,\n",
    "    boosting_type='gbdt',\n",
    "    max_depth=17,\n",
    "    max_bin=255,\n",
    "    min_split_gain=0,\n",
    "    min_child_weight=6.5,\n",
    "    min_child_samples=40,\n",
    "    subsample=.76,\n",
    "    subsample_freq=1,\n",
    "    colsample_bytree=.73,\n",
    "    bin_construct_sample_cnt=999999, # use entire dataset\n",
    "    min_data_in_bin=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=0.2,\n",
    "    seed=0,\n",
    "    nthread=-1,\n",
    "    silent=True)\n",
    "\n",
    "gbm.fit(X_train_enc, Y_train_enc, early_stopping_rounds=40, verbose=40,\n",
    "            eval_metric=\"auc\", eval_set=[(X_test_enc, Y_test_enc)])\n",
    "\n",
    "# .825 AUC w/ 63 bins\n",
    "# .8344 AUC w/ 255 bins\n",
    "# .8338 AUC w/ 255 bins & even lower lrate (.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score:\n",
      "0.833885028714\n",
      "\n",
      "LogLoss:\n",
      "0.391772387343\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False      0.834     0.969     0.896     25951\n",
      "       True      0.788     0.378     0.511      8023\n",
      "\n",
      "avg / total      0.823     0.829     0.805     33974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "probs = score_classif_on_test(gbm, X_test_enc, Y_test_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although LightGBM wasn't quite able to beat XGBoost on this dataset (.834 vs. .840 AUC), it came very close and while running several times faster. Random forest came pretty close (.817) with virtually zero tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ba6f048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test_enc, probs)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
