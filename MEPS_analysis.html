<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>MEPS_analysis</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.6 (http://getbootstrap.com)
 * Copyright 2011-2015 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    color: #000 !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: thin dotted;
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: thin dotted;
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: thin dotted;
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.2.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.2.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.2.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.2.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.2.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=1);
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2);
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=3);
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
@media (max-width: 991px) {
  #ipython_notebook {
    margin-left: 10px;
  }
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#login_widget {
  float: right;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  text-align: center;
  vertical-align: middle;
  display: inline;
  opacity: 0;
  z-index: 2;
  width: 12ex;
  margin-right: -12ex;
}
.alternate_upload .btn-upload {
  height: 22px;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: baseline;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
@-moz-document url-prefix() {
  div.inner_cell {
    overflow-x: hidden;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul {
  list-style: disc;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ul ul {
  list-style: square;
  margin: 0em 2em;
}
.rendered_html ul ul ul {
  list-style: circle;
  margin: 0em 2em;
}
.rendered_html ol {
  list-style: decimal;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
  margin: 0em 2em;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  background-color: #fff;
  color: #000;
  font-size: 100%;
  padding: 0px;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: 1px solid black;
  border-collapse: collapse;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  border: 1px solid black;
  border-collapse: collapse;
  margin: 1em 2em;
}
.rendered_html td,
.rendered_html th {
  text-align: left;
  vertical-align: middle;
  padding: 4px;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget {
  float: right !important;
  float: right;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 20ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  margin-top: 6px;
}
span.save_widget span.filename {
  height: 1em;
  line-height: 1em;
  padding: 3px;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  display: none;
}
.command-shortcut:before {
  content: "(command)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}

@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MEPS-Dataset-Analysis">MEPS Dataset Analysis<a class="anchor-link" href="#MEPS-Dataset-Analysis">&#182;</a></h2><p>So we have two files - one with basic demographic info and one with medications. They're small and pandas will deal with them fine - no need for SQL in an offline analysis setting. We'll just want to join them on id to make predictions.</p>
<p>Some obvious things to keep in mind from looking at them -<br>
1) a significant proportion of subjects are younger than sixteen. For these individuals, we have basically zero info - best to drop them from our tables and not make predictions for them. A cursory search of meds reveals no medication info for them.<br>
2) We have tons of duplicate meds - we'll probably want to consolidate these.</p>
<p>Since this is an informal ad hoc analysis, I'll just be checking results of operations rather than building out proper tests. Of course, this isn't intended to look like production code :)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># python=3.6.2, but I&#39;ve tested this file with Python 2.7.13 as of 2017/06/12</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">scipy</span>

<span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
<span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">roc_curve</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">auc</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">roc_auc_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">log_loss</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">make_scorer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegressionCV</span>
<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="k">import</span> <span class="n">DummyRegressor</span>


<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_columns</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="s1">&#39;pastel&#39;</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.
  &#34;`IPython.html.widgets` has moved to `ipywidgets`.&#34;, ShimWarning)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load data.</span>

<span class="n">subjects</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./input/meps_base_data.csv&quot;</span><span class="p">)</span>
<span class="n">meds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./input/meps_meds.csv&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">display_summary</span><span class="p">(</span><span class="n">dfs_to_describe</span><span class="p">,</span> <span class="n">preview_size</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2"> Summary of data: </span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">dfs_to_describe</span><span class="p">:</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Preview: &quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">preview_size</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2"> Stats: &quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>

    
<span class="n">display_summary</span><span class="p">([</span><span class="n">subjects</span><span class="p">,</span><span class="n">meds</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>

 Summary of data: 


Preview: 
   Unnamed: 0        id  panel  pooledWeight  age     sex      race  \
0           1  10007101     15   3603.881236   28    Male     White   
1           2  10007102     15   2544.550424   25  Female     White   
2           3  10007103     15   4050.397468    4    Male     White   
3           4  10007104     15   3064.059720    3  Female     White   
4           5  10008101     15   3635.552466   51    Male  Multiple   

                   married highBPDiagnosed diabetesDiagnosed  chdDiagnosed  \
0                  MARRIED             Yes                No            No   
1                  MARRIED              No                No            No   
2  UNDER 16 - INAPPLICABLE    Inapplicable      Inapplicable  Inapplicable   
3  UNDER 16 - INAPPLICABLE    Inapplicable      Inapplicable  Inapplicable   
4                  MARRIED              No                No            No   

    miDiagnosed anginaDiagnosed strokeDiagnosed emphysemaDiagnosed  \
0            No              No              No                 No   
1            No              No              No                 No   
2  Inapplicable    Inapplicable    Inapplicable       Inapplicable   
3  Inapplicable    Inapplicable    Inapplicable       Inapplicable   
4            No              No              No                 No   

  asthmaDiagnosed otherHDDiagnosed heartFailureDiagnosed  
0              No               No                    No  
1             Yes               No                    No  
2              No     Inapplicable                    No  
3              No     Inapplicable                    No  
4              No               No                    No  


 Stats: 
         Unnamed: 0            id         panel  pooledWeight           age
count  61489.000000  6.148900e+04  61489.000000  61489.000000  61489.000000
mean   30745.000000  5.534638e+07     13.534453   5063.701982     33.578396
std    17750.489688  2.759592e+07      1.061329   3815.885387     22.887576
min        1.000000  1.000710e+07     12.000000    127.710358     -1.000000
25%    15373.000000  4.045510e+07     13.000000   2217.419038     14.000000
50%    30745.000000  4.965010e+07     14.000000   3989.180418     32.000000
75%    46117.000000  8.161711e+07     14.000000   6905.677619     51.000000
max    61489.000000  8.968810e+07     15.000000  38828.153564     85.000000



Preview: 
   Unnamed: 0        id  rxStartMonth  rxStartYear  \
0           1  10007104             3         2011   
1           2  10007104             3         2011   
2           3  10008102             3         2011   
3           4  10008102             3         2011   
4           5  10008102             9         2011   

                           rxName        rxNDC  rxQuantity rxForm  
0                     AMOXICILLIN    143988775        75.0   SUSR  
1              OTIC EDGE SOLUTION  68032032814        14.0    SOL  
2  NASAL DECONGESTANT 0.05% SPRAY  63981056903        15.0    SPR  
3  NASAL DECONGESTANT 0.05% SPRAY  63981056903        15.0    SPR  
4                    DIPHENHYDRAM    603333921        30.0    CAP  


 Stats: 
         Unnamed: 0            id  rxStartMonth   rxStartYear         rxNDC  \
count  1.148347e+06  1.148347e+06  1.148347e+06  1.148347e+06  1.148347e+06   
mean   1.240252e+06  5.523596e+07  5.184600e-01  1.172427e+03  2.226109e+10   
std    9.590400e+05  2.728584e+07  4.087907e+00  9.895660e+02  2.834269e+10   
min    1.000000e+00  1.000710e+07 -9.000000e+00 -1.400000e+01 -9.000000e+00   
25%    2.870875e+05  4.063810e+07 -1.000000e+00 -1.000000e+00  1.490472e+08   
50%    1.093262e+06  4.947510e+07 -1.000000e+00  2.000000e+03  5.910385e+08   
75%    2.103602e+06  8.118210e+07  1.000000e+00  2.008000e+03  5.486835e+10   
max    3.336212e+06  8.968810e+07  1.200000e+01  2.011000e+03  9.920707e+10   

         rxQuantity  
count  1.148347e+06  
mean   5.942380e+01  
std    3.702845e+02  
min   -9.000000e+00  
25%    3.000000e+01  
50%    3.000000e+01  
75%    6.800000e+01  
max    1.200000e+05  



</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Data-Cleanup">Data Cleanup<a class="anchor-link" href="#Data-Cleanup">&#182;</a></h3><p>Let's think about what data we can drop -</p>
<p>1) from our subjects,<br>
    a) <code>panel</code> seems to be panel year and is unlikely to be terribly useful to us, at least for now. With a sufficiently large amount of data, it could be used in tree algorithms and the like as-is. Or we could subdivide by groups of years and treat as a categorical so that a classifier could find trends for each period of time bin.<br>
    b) <code>pooledWeight</code> seems to be used for a weighting function / to correct for demographics. I couldn't immediately find details on how to use the weighting function, so I'm ignoring it for now.</p>
<p>2)  from our medications
    a) we have tons of duplicates of drugs - presumably we have one record per prescription written; let's just keep one record of a prescription of a given drug per person (we'll assume that they've stayed on it)<br>
    b) right now, let's drop a few columns - rxNDC is apparently a "NATIONAL DRUG CODE (IMPUTED)"; this might initially seem useful, but it actually isn't because there are multiple codes for many drugs</p>
<p>Let's also drop <code>rxStartMonth</code> - we could use this and rxStartYear to build a more standardized datetime scheme (ignoring that months are missing very frequently), but for now let's just use year; year could be useful - lets us control for differing prescriptions used to treat diseases as medicine progresses.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define a simple drop function.</span>
<span class="k">def</span> <span class="nf">drop_cols</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">cols</span><span class="p">):</span>
    <span class="c1"># Find which cols in requested list are present.</span>
    <span class="n">cols_drop</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">cols</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">cols_drop</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No cols found to drop.&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Inplace to avoid namespace/scope issues.</span>
        <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
<span class="c1"># Drop fields mentioned above.</span>
<span class="n">drop_cols</span><span class="p">(</span><span class="n">subjects</span><span class="p">,[</span><span class="s1">&#39;Unnamed: 0&#39;</span><span class="p">,</span> <span class="s1">&#39;panel&#39;</span><span class="p">,</span><span class="s1">&#39;pooledWeight&#39;</span><span class="p">])</span>
<span class="n">drop_cols</span><span class="p">(</span><span class="n">meds</span><span class="p">,[</span><span class="s1">&#39;Unnamed: 0&#39;</span><span class="p">,</span> <span class="s1">&#39;rxNDC&#39;</span><span class="p">,</span><span class="s1">&#39;rxStartMonth&#39;</span><span class="p">])</span>

<span class="c1"># Drop individuals younger than 16.</span>
<span class="n">younglings</span> <span class="o">=</span> <span class="n">subjects</span><span class="p">[</span><span class="n">subjects</span><span class="o">.</span><span class="n">age</span> <span class="o">&lt;</span> <span class="mi">16</span><span class="p">]</span><span class="o">.</span><span class="n">index</span>
<span class="k">try</span><span class="p">:</span> <span class="n">subjects</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">younglings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Already dropped underage subjects.&#39;</span><span class="p">)</span>

<span class="c1"># Impute mean prescription year for prescriptions with invalid years (e.g -8).  </span>
<span class="n">valid_by_year</span> <span class="o">=</span> <span class="n">meds</span><span class="p">[</span><span class="n">meds</span><span class="o">.</span><span class="n">rxStartYear</span> <span class="o">&gt;</span> <span class="mi">1900</span><span class="p">]</span>
<span class="n">avg_rx_year</span> <span class="o">=</span> <span class="n">valid_by_year</span><span class="o">.</span><span class="n">rxStartYear</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1"># Apply that value to prescriptions with invalid years.  </span>
<span class="n">meds</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">meds</span><span class="o">.</span><span class="n">rxStartYear</span> <span class="o">&lt;</span> <span class="mi">1900</span><span class="p">]</span> <span class="o">=</span> <span class="n">avg_rx_year</span>

<span class="n">meds</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="n">subset</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span><span class="s1">&#39;rxName&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">display_summary</span><span class="p">([</span><span class="n">subjects</span><span class="p">,</span><span class="n">meds</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>

 Summary of data: 


Preview: 
         id  age     sex      race        married highBPDiagnosed  \
0  10007101   28    Male     White        MARRIED             Yes   
1  10007102   25  Female     White        MARRIED              No   
4  10008101   51    Male  Multiple        MARRIED              No   
5  10008102   53  Female     Asian        MARRIED              No   
7  10009101   61  Female     Black  NEVER MARRIED             Yes   

  diabetesDiagnosed chdDiagnosed miDiagnosed anginaDiagnosed strokeDiagnosed  \
0                No           No          No              No              No   
1                No           No          No              No              No   
4                No           No          No              No              No   
5                No           No          No              No              No   
7                No           No          No              No              No   

  emphysemaDiagnosed asthmaDiagnosed otherHDDiagnosed heartFailureDiagnosed  
0                 No              No               No                    No  
1                 No             Yes               No                    No  
4                 No              No               No                    No  
5                 No              No               No                    No  
7                 No              No               No                    No  


 Stats: 
                 id           age
count  4.426600e+04  44266.000000
mean   5.512156e+07     44.062667
std    2.755210e+07     18.037354
min    1.000710e+07     16.000000
25%    4.040310e+07     29.000000
50%    4.949510e+07     43.000000
75%    8.143210e+07     57.000000
max    8.968810e+07     85.000000



Preview: 
           id  rxStartYear                          rxName  rxQuantity rxForm
0  10007104.0       2011.0                     AMOXICILLIN        75.0   SUSR
1  10007104.0       2011.0              OTIC EDGE SOLUTION        14.0    SOL
2  10008102.0       2011.0  NASAL DECONGESTANT 0.05% SPRAY        15.0    SPR
4  10008102.0       2011.0                    DIPHENHYDRAM        30.0    CAP
5  10008102.0       2011.0                    CHLD ALLERGY       100.0   LIQD


 Stats: 
                 id    rxStartYear     rxQuantity
count  1.966740e+05  196674.000000  196674.000000
mean   5.500321e+07    2006.451618      62.511634
std    2.709755e+07       5.427340     464.765955
min    2.005437e+03    1940.000000      -9.000000
25%    4.080210e+07    2006.000000      21.000000
50%    4.907110e+07    2008.000000      30.000000
75%    8.126610e+07    2009.000000      63.000000
max    8.968810e+07    2011.000000  120000.000000



</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="JOIN">JOIN<a class="anchor-link" href="#JOIN">&#182;</a></h4><p>I'm going to inner join, so we keep all prescriptions for people who we have demographic info.<br>
It would also be reasonable to left join so that we keep info on all people even if they don't have any prescriptions. I went ahead and also tried a left join, and it only affected the number of subjects pretty marginally (by &lt;10%).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">subj_and_meds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">subjects</span><span class="p">,</span> <span class="n">meds</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;inner&#39;</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="n">sort</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cols before merge:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subjects</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">meds</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cols after merge:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Cols before merge:
[&#39;id&#39; &#39;age&#39; &#39;sex&#39; &#39;race&#39; &#39;married&#39; &#39;highBPDiagnosed&#39; &#39;diabetesDiagnosed&#39;
 &#39;chdDiagnosed&#39; &#39;miDiagnosed&#39; &#39;anginaDiagnosed&#39; &#39;strokeDiagnosed&#39;
 &#39;emphysemaDiagnosed&#39; &#39;asthmaDiagnosed&#39; &#39;otherHDDiagnosed&#39;
 &#39;heartFailureDiagnosed&#39;]
[&#39;id&#39; &#39;rxStartYear&#39; &#39;rxName&#39; &#39;rxQuantity&#39; &#39;rxForm&#39;]
Cols after merge:
[&#39;id&#39; &#39;age&#39; &#39;sex&#39; &#39;race&#39; &#39;married&#39; &#39;highBPDiagnosed&#39; &#39;diabetesDiagnosed&#39;
 &#39;chdDiagnosed&#39; &#39;miDiagnosed&#39; &#39;anginaDiagnosed&#39; &#39;strokeDiagnosed&#39;
 &#39;emphysemaDiagnosed&#39; &#39;asthmaDiagnosed&#39; &#39;otherHDDiagnosed&#39;
 &#39;heartFailureDiagnosed&#39; &#39;rxStartYear&#39; &#39;rxName&#39; &#39;rxQuantity&#39; &#39;rxForm&#39;]
(169868, 19)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Visually inspect result of JOIN to make sure it worked as expected.</span>
<span class="c1"># subj_and_meds.to_csv(&quot;joined_data.csv&quot;)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Variable-Encoding">Variable Encoding<a class="anchor-link" href="#Variable-Encoding">&#182;</a></h3><p>We currently don't have much of our info in an easily-interpretable form for our model(s) - we'll want to re-encode a whole bunch of categorial variables -</p>
<p>1) We need to turn our sex column and diagnoses into a boolean value - e.g. "isFemale"<br>
2) We need to one-hot encode race and married. (If we were ignoring logisitic regression and its kin, factorizing with e.g. pd.factorize() would also work and would result in a slightly smaller memory footprint and faster model training for tree-based models.)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Start with re-encoding sex.</span>

<span class="k">try</span><span class="p">:</span> <span class="n">subj_and_meds</span><span class="p">[</span><span class="s1">&#39;isFemale&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">subj_and_meds</span><span class="o">.</span><span class="n">sex</span> <span class="o">==</span> <span class="s2">&quot;Female&quot;</span>
<span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;isFemale already created. Skipping.&quot;</span><span class="p">)</span>
<span class="n">drop_cols</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Example of diagnosis col before value correction:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="o">.</span><span class="n">diabetesDiagnosed</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>

<span class="c1"># Now we&#39;ll re-encode all the diagnoses.</span>
<span class="c1"># or == True so I can run this code multiple times without getting glitches</span>
<span class="c1"># (Don&#39;t want to force you to restart the notebook if you run a cell twice.)</span>
<span class="n">yn_to_bool</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">value</span><span class="p">:</span> <span class="n">value</span> <span class="o">==</span> <span class="s2">&quot;Yes&quot;</span> <span class="ow">or</span> <span class="n">value</span> <span class="o">==</span> <span class="kc">True</span> 
<span class="n">diagnoses</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;highBPDiagnosed&#39;</span><span class="p">,</span> <span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">,</span> <span class="s1">&#39;chdDiagnosed&#39;</span><span class="p">,</span> <span class="s1">&#39;miDiagnosed&#39;</span><span class="p">,</span> <span class="s1">&#39;anginaDiagnosed&#39;</span><span class="p">,</span>
             <span class="s1">&#39;strokeDiagnosed&#39;</span><span class="p">,</span> <span class="s1">&#39;emphysemaDiagnosed&#39;</span><span class="p">,</span> <span class="s1">&#39;asthmaDiagnosed&#39;</span><span class="p">,</span> <span class="s1">&#39;otherHDDiagnosed&#39;</span><span class="p">,</span> <span class="s1">&#39;heartFailureDiagnosed&#39;</span><span class="p">]</span>
<span class="c1"># for diagnosis in diagnoses:</span>
<span class="c1">#     subj_and_meds[diagnosis] = subj_and_meds[diagnosis].apply(yn_to_bool)</span>
<span class="n">subj_and_meds</span><span class="p">[</span><span class="n">diagnoses</span><span class="p">]</span> <span class="o">=</span> <span class="n">subj_and_meds</span><span class="p">[</span><span class="n">diagnoses</span><span class="p">]</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="n">yn_to_bool</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Example of diagnosis col after value correction:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="o">.</span><span class="n">diabetesDiagnosed</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Cols incl. newly-created bools:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>


<span class="c1"># For the sake of code cleanliness, I&#39;ll create both sets here, but you could argue that </span>
<span class="c1"># I should create them separately to have more descriptive variable names (e.g. &quot;race_&quot;</span>
<span class="c1"># and &quot;marital_&quot; prefixes rather than &quot;d_&quot;)</span>
<span class="k">try</span><span class="p">:</span> 
    <span class="n">subj_and_meds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;race&#39;</span><span class="p">,</span> <span class="s1">&#39;married&#39;</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;d_&#39;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Couldn&#39;t find dummy sources - likely already created.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Columns including one-hot encoded cols: &#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2"> Now including dummies: </span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">drop_cols</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;race&#39;</span><span class="p">,</span> <span class="s1">&#39;married&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Example of diagnosis col before value correction:
0    No
1    No
2    No
3    No
4    No
Name: diabetesDiagnosed, dtype: object

 Example of diagnosis col after value correction:
0    False
1    False
2    False
3    False
4    False
Name: diabetesDiagnosed, dtype: bool

 Cols incl. newly-created bools:
Index([&#39;id&#39;, &#39;age&#39;, &#39;race&#39;, &#39;married&#39;, &#39;highBPDiagnosed&#39;, &#39;diabetesDiagnosed&#39;,
       &#39;chdDiagnosed&#39;, &#39;miDiagnosed&#39;, &#39;anginaDiagnosed&#39;, &#39;strokeDiagnosed&#39;,
       &#39;emphysemaDiagnosed&#39;, &#39;asthmaDiagnosed&#39;, &#39;otherHDDiagnosed&#39;,
       &#39;heartFailureDiagnosed&#39;, &#39;rxStartYear&#39;, &#39;rxName&#39;, &#39;rxQuantity&#39;,
       &#39;rxForm&#39;, &#39;isFemale&#39;],
      dtype=&#39;object&#39;)
Columns including one-hot encoded cols: 
Index([&#39;id&#39;, &#39;age&#39;, &#39;highBPDiagnosed&#39;, &#39;diabetesDiagnosed&#39;, &#39;chdDiagnosed&#39;,
       &#39;miDiagnosed&#39;, &#39;anginaDiagnosed&#39;, &#39;strokeDiagnosed&#39;,
       &#39;emphysemaDiagnosed&#39;, &#39;asthmaDiagnosed&#39;, &#39;otherHDDiagnosed&#39;,
       &#39;heartFailureDiagnosed&#39;, &#39;rxStartYear&#39;, &#39;rxName&#39;, &#39;rxQuantity&#39;,
       &#39;rxForm&#39;, &#39;isFemale&#39;, &#39;d__Amer Indian/Alaska Native&#39;, &#39;d__Asian&#39;,
       &#39;d__Black&#39;, &#39;d__Multiple&#39;, &#39;d__Native Hawaiian/Pacific Islander&#39;,
       &#39;d__White&#39;, &#39;d__DIVORCED&#39;, &#39;d__DIVORCED IN ROUND&#39;, &#39;d__MARRIED&#39;,
       &#39;d__MARRIED IN ROUND&#39;, &#39;d__NEVER MARRIED&#39;, &#39;d__SEPARATED&#39;,
       &#39;d__SEPARATED IN ROUND&#39;, &#39;d__WIDOWED&#39;, &#39;d__WIDOWED IN ROUND&#39;],
      dtype=&#39;object&#39;)


 Now including dummies: 


No cols found to drop.
Index([&#39;id&#39;, &#39;age&#39;, &#39;highBPDiagnosed&#39;, &#39;diabetesDiagnosed&#39;, &#39;chdDiagnosed&#39;,
       &#39;miDiagnosed&#39;, &#39;anginaDiagnosed&#39;, &#39;strokeDiagnosed&#39;,
       &#39;emphysemaDiagnosed&#39;, &#39;asthmaDiagnosed&#39;, &#39;otherHDDiagnosed&#39;,
       &#39;heartFailureDiagnosed&#39;, &#39;rxStartYear&#39;, &#39;rxName&#39;, &#39;rxQuantity&#39;,
       &#39;rxForm&#39;, &#39;isFemale&#39;, &#39;d__Amer Indian/Alaska Native&#39;, &#39;d__Asian&#39;,
       &#39;d__Black&#39;, &#39;d__Multiple&#39;, &#39;d__Native Hawaiian/Pacific Islander&#39;,
       &#39;d__White&#39;, &#39;d__DIVORCED&#39;, &#39;d__DIVORCED IN ROUND&#39;, &#39;d__MARRIED&#39;,
       &#39;d__MARRIED IN ROUND&#39;, &#39;d__NEVER MARRIED&#39;, &#39;d__SEPARATED&#39;,
       &#39;d__SEPARATED IN ROUND&#39;, &#39;d__WIDOWED&#39;, &#39;d__WIDOWED IN ROUND&#39;],
      dtype=&#39;object&#39;)
         id  age  highBPDiagnosed  diabetesDiagnosed  chdDiagnosed  \
0  10007101   28             True              False         False   
1  10007101   28             True              False         False   
2  10007102   25            False              False         False   

   miDiagnosed  anginaDiagnosed  strokeDiagnosed  emphysemaDiagnosed  \
0        False            False            False               False   
1        False            False            False               False   
2        False            False            False               False   

   asthmaDiagnosed  otherHDDiagnosed  heartFailureDiagnosed  rxStartYear  \
0            False             False                  False       2005.0   
1            False             False                  False       2010.0   
2             True             False                  False       2009.0   

         rxName  rxQuantity rxForm  isFemale  d__Amer Indian/Alaska Native  \
0      ATENOLOL        30.0   TABS     False                             0   
1  AZITHROMYCIN        30.0   SUSR     False                             0   
2      TREXIMET        18.0   TABS      True                             0   

   d__Asian  d__Black  d__Multiple  d__Native Hawaiian/Pacific Islander  \
0         0         0            0                                    0   
1         0         0            0                                    0   
2         0         0            0                                    0   

   d__White  d__DIVORCED  d__DIVORCED IN ROUND  d__MARRIED  \
0         1            0                     0           1   
1         1            0                     0           1   
2         1            0                     0           1   

   d__MARRIED IN ROUND  d__NEVER MARRIED  d__SEPARATED  d__SEPARATED IN ROUND  \
0                    0                 0             0                      0   
1                    0                 0             0                      0   
2                    0                 0             0                      0   

   d__WIDOWED  d__WIDOWED IN ROUND  
0           0                    0  
1           0                    0  
2           0                    0  
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># visually inspect result of concat above to make sure it worked as expected</span>
<span class="c1"># subj_and_meds.to_csv(&quot;with_dummies.csv&quot;)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I noticed that we have categories that we might want to merge - e.g. "widowed in round" is pretty rare and should, perhaps, be merged into "widowed."<br>
One other caveat - we have no default for our dummy variables, which will cause multicollinearity issues with regression models - we should fix this later if we want to use them.</p>
<h2 id="Let's-actually-do-some-analyses-:)">Let's actually do some analyses :)<a class="anchor-link" href="#Let's-actually-do-some-analyses-:)">&#182;</a></h2><h4 id="1)-What-are-the-most-common-medications-for-each-disease?">1) What are the most common medications for each disease?<a class="anchor-link" href="#1)-What-are-the-most-common-medications-for-each-disease?">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># TODO: this could be expressed in a more efficient manner as a .groupby(), </span>
<span class="c1">#   although I think the for may be more readable.</span>
<span class="c1"># Also, you&#39;d have to convert the one-hot-encoded diseases into a single categorical column, </span>
<span class="c1">#   and handling individuals with multiple diseases would be a pain.</span>

<span class="k">for</span> <span class="n">diagnosis</span> <span class="ow">in</span> <span class="n">diagnoses</span><span class="p">:</span>
    <span class="n">current_diagnosis</span> <span class="o">=</span> <span class="n">subj_and_meds</span><span class="p">[</span><span class="n">subj_and_meds</span><span class="p">[</span><span class="n">diagnosis</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2"> For diagnosis </span><span class="si">%s</span><span class="s2">, most common prescription is: </span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">diagnosis</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">current_diagnosis</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>

 For diagnosis highBPDiagnosed, most common prescription is: 

count          93623
unique          5650
top       LISINOPRIL
freq            2476
Name: rxName, dtype: object


 For diagnosis diabetesDiagnosed, most common prescription is: 

count         40116
unique         3823
top       METFORMIN
freq           1158
Name: rxName, dtype: object


 For diagnosis chdDiagnosed, most common prescription is: 

count          22039
unique          2867
top       LISINOPRIL
freq             466
Name: rxName, dtype: object


 For diagnosis miDiagnosed, most common prescription is: 

count          14725
unique          2333
top       LISINOPRIL
freq             339
Name: rxName, dtype: object


 For diagnosis anginaDiagnosed, most common prescription is: 

count          11603
unique          2164
top       LISINOPRIL
freq             194
Name: rxName, dtype: object


 For diagnosis strokeDiagnosed, most common prescription is: 

count          14319
unique          2464
top       LISINOPRIL
freq             283
Name: rxName, dtype: object


 For diagnosis emphysemaDiagnosed, most common prescription is: 

count           9594
unique          1997
top       LISINOPRIL
freq             145
Name: rxName, dtype: object


 For diagnosis asthmaDiagnosed, most common prescription is: 

count         28376
unique         3568
top       ALBUTEROL
freq            536
Name: rxName, dtype: object


 For diagnosis otherHDDiagnosed, most common prescription is: 

count          31770
unique          3715
top       LISINOPRIL
freq             584
Name: rxName, dtype: object


 For diagnosis heartFailureDiagnosed, most common prescription is: 

count           4940
unique          1350
top       FUROSEMIDE
freq             173
Name: rxName, dtype: object
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Gut check - lisinopril is an ACE inhibitor, and ACE inhibitors are a frontline treatment for hypertension. Metformin is a frontline treatment for type 2 diabetes (and type 2 is far more common than type 1).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="2)-Which-medications-are-most-indicative-of-each-disease?">2) Which medications are most indicative of each disease?<a class="anchor-link" href="#2)-Which-medications-are-most-indicative-of-each-disease?">&#182;</a></h4><p>Let's figure out which drug is the most specific for each disease. So I'll calculate:</p>
<p>instances of drug for disease / instances of drug overall</p>
<p>In other words, I'm getting the precision of a given drug in making a diagnosis for a given disease.</p>
<p>One caveat: if we have drugs with exceedingly low counts - say one doctor has homeopathic leanings and prescribes a diabetic patient ginseng - it could seemingly be a perfect identifier by this metric. Instead, I'll only look at the drugs that are reasonably common - using exceedingly rare drugs to diagnoses common diseases seems like it would only be marginally useful, anyway.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># We&#39;ll take our list of prescriptions, and get their counts.</span>
<span class="c1"># All of our most common drugs per diagnosis above have high frequencies (&gt;1000), </span>
<span class="c1">#   but I&#39;ll go ahead and grab the top 100 to be safe.</span>
<span class="c1"># Use counts as the index, so we have them sorted by descending frequency.</span>
<span class="c1"># Using function-chaining syntax here. I find it pretty readable, but some people don&#39;t.</span>
<span class="n">drugs</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">subj_and_meds</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span>
        <span class="c1"># Get value_counts() for drugs - it&#39;s sorted and descending by default, </span>
        <span class="c1"># but I&#39;ve specified them as kwargs for maximum explicitness.</span>
        <span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">sort</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  
        <span class="c1"># Get the top 100 most-common.</span>
        <span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span>
        <span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Fix column names.</span>
<span class="n">drugs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="s1">&#39;rxName&#39;</span><span class="p">,</span> <span class="s1">&#39;rxName&#39;</span><span class="p">:</span> <span class="s1">&#39;count&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Confirm size of list.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of drugs extracted: </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">drugs</span><span class="o">.</span><span class="n">rxName</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="c1"># Confirm that our most common drugs are here, indexed by count.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Most common drugs that we&#39;ve extracted:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">drugs</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Least common drugs that we&#39;ve extracted:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">drugs</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of drugs extracted: 100

Most common drugs that we&#39;ve extracted:
         rxName  count
0  AZITHROMYCIN   3358
1    LISINOPRIL   2865
2   AMOXICILLIN   2687
3   SIMVASTATIN   2312
4     IBUPROFEN   2243
5    PREDNISONE   1712
6  HYDROCO/APAP   1703
7       LIPITOR   1630
8    OMEPRAZOLE   1496
9     METFORMIN   1422
Least common drugs that we&#39;ve extracted:
                   rxName  count
90             LORATADINE    309
91           ADVAIR DISKU    308
92         APAP/OXYCODONE    306
93             DIOVAN HCT    303
94             SMZ/TMP DS    302
95            GLIMEPIRIDE    302
96           PENICILLN VK    294
97  ACETAMINOPHEN/CODEINE    294
98                NORVASC    293
99     PRAVASTATIN SODIUM    293
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">drug_precisions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="c1"># Iterate through each dx and each drug - O(n^2)</span>
<span class="c1"># There&#39;s plenty of room to optimize this code if it gets run frequently</span>
<span class="k">for</span> <span class="n">drug</span> <span class="ow">in</span> <span class="n">drugs</span><span class="o">.</span><span class="n">rxName</span><span class="p">:</span>
    <span class="c1"># get number of users for the drug</span>
    <span class="n">users</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">subj_and_meds</span><span class="p">[</span><span class="n">subj_and_meds</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">drug</span><span class="p">][</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="c1"># checked that these counts are equal to the counts above - commented it out now</span>
    <span class="k">for</span> <span class="n">diagnosis</span> <span class="ow">in</span> <span class="n">diagnoses</span><span class="p">:</span>
        <span class="c1"># get number of users of the drug who have the diagnosis</span>
        <span class="n">fil_dx_and_rx</span> <span class="o">=</span> <span class="p">((</span><span class="n">subj_and_meds</span><span class="p">[</span><span class="n">diagnosis</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">subj_and_meds</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">drug</span><span class="p">))</span>
        <span class="n">users_with_dx</span> <span class="o">=</span> <span class="n">subj_and_meds</span><span class="p">[</span><span class="n">fil_dx_and_rx</span><span class="p">][</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span>
        <span class="n">this_precision</span> <span class="o">=</span> <span class="n">users_with_dx</span> <span class="o">/</span> <span class="n">users</span>
        <span class="n">drug_precisions</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;rxName&#39;</span><span class="p">:</span> <span class="n">drug</span><span class="p">,</span> <span class="s1">&#39;diagnosis&#39;</span><span class="p">:</span> <span class="n">diagnosis</span><span class="p">,</span> <span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="n">this_precision</span><span class="p">})</span>
        
<span class="c1"># print(drug_precisions)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Convert list of dict rows to pd.DataFrame. </span>
<span class="c1"># (Want to do this all at once since extending a DataFrame is an expensive operation.)</span>
<span class="n">precisions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">drug_precisions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">precisions</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>           diagnosis  precision        rxName
0    highBPDiagnosed   0.335616  AZITHROMYCIN
1  diabetesDiagnosed   0.097677  AZITHROMYCIN
2       chdDiagnosed   0.060453  AZITHROMYCIN
3        miDiagnosed   0.035736  AZITHROMYCIN
4    anginaDiagnosed   0.033949  AZITHROMYCIN
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[40]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Now get the max precision for each diagnosis. </span>
<span class="c1"># It would be cleaner to just groupby diagnosis and get the max precision,</span>
<span class="c1">#   but that wouldn&#39;t yield the drug name for each max.</span>
<span class="c1"># Instead, we&#39;ll sort our precisions and then take the max for each diagnosis.</span>

<span class="n">sorted_</span> <span class="o">=</span> <span class="n">precisions</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;precision&#39;</span><span class="p">)</span>
<span class="n">max_precisions</span> <span class="o">=</span> <span class="n">sorted_</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="s1">&#39;diagnosis&#39;</span><span class="p">,</span> <span class="n">keep</span><span class="o">=</span><span class="s1">&#39;last&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;precision&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">max_precisions</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>                 diagnosis  precision        rxName
669  heartFailureDiagnosed   0.204188    CARVEDILOL
916     emphysemaDiagnosed   0.211039  ADVAIR DISKU
324        anginaDiagnosed   0.296117        PLAVIX
325        strokeDiagnosed   0.307443        PLAVIX
323            miDiagnosed   0.401294        PLAVIX
322           chdDiagnosed   0.529126        PLAVIX
698       otherHDDiagnosed   0.552632      WARFARIN
917        asthmaDiagnosed   0.675325  ADVAIR DISKU
501      diabetesDiagnosed   0.965591         ACTOS
930        highBPDiagnosed   0.970297    DIOVAN HCT
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[42]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="c1"># FIXME!</span>

<span class="n">formatted_dx</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;High BP, Diovan&#39;</span><span class="p">,</span> <span class="s1">&#39;Diabetes, Actos&#39;</span><span class="p">,</span> <span class="s1">&#39;CHD, Plavix&#39;</span><span class="p">,</span> <span class="s1">&#39;MI, Plavix&#39;</span><span class="p">,</span>
                <span class="s1">&#39;Angina, Plavix&#39;</span><span class="p">,</span> <span class="s1">&#39;Stroke, Plavix&#39;</span><span class="p">,</span> <span class="s1">&#39;Emphysema, Advair&#39;</span><span class="p">,</span> <span class="s1">&#39;Asthma, Advair&#39;</span><span class="p">,</span>
                <span class="s1">&#39;Other HD, Warfarin&#39;</span><span class="p">,</span> <span class="s1">&#39;Heart Failure, Carvedilol&#39;</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Diagnostic Sensitivities&#39;</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">max_precisions</span><span class="o">.</span><span class="n">diagnosis</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">max_precisions</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">precisions</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Precision&quot;</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Diagnosis, Drug&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">formatted_dx</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s1">&#39;0%&#39;</span><span class="p">,</span> <span class="s1">&#39;20%&#39;</span><span class="p">,</span> <span class="s1">&#39;40%&#39;</span><span class="p">,</span> <span class="s1">&#39;60%&#39;</span><span class="p">,</span> <span class="s1">&#39;80%&#39;</span><span class="p">,</span> <span class="s1">&#39;100%&#39;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>


<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABC0AAAFKCAYAAAA5RaVOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Gut check - these make sense - Diovan HCT is a combo ACE inhibitor and diuretic, two extremely common classes of drugs for high blood pressure; Plavix is an anticoagulant, so it's used for various heart diseases; advair is a bronchodilator, so it's used for respiratory disorders.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="3)-Let's-model-one-disease.">3) Let's model one disease.<a class="anchor-link" href="#3)-Let's-model-one-disease.">&#182;</a></h4><p>First we have to pick a disease. If I want to pick one that's easy to model, it would be nice to choose one that has lots of instances and that is also well-distinguished by a small number of criteria. As you can see below, HBP would be an obvious choice, but HBP is really common, even in people who are mostly healthy. It would be cool to train a more complex model on it, but for the sake of simplicity, I'll model diabetes, which is a classic stats/ML problem.</p>
<p>By the way, with regards to model performance, we probably care more about recall than precision in this instance - we really want to pick up cases of the disease based on medicines and don't care all that much if we falsely flag the potential presence of a disease. Also, raw accuracy isn't far off from worthless, since (most) of these disease are pretty rare. Diabetes affects around 10% of people in a random sample of Americans, so a model that predicts y=0 is 90% accurate in a random samples (and 74% accurate in our sample - our sample isn't a random sample of Americans, as we'll soon find out).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[48]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Let&#39;s pick.</span>
<span class="c1"># Again, syntax here is slightly awkward because of one-hot encoded </span>
<span class="c1">#   disease columns and potential for each subject to have multiple </span>
<span class="c1">#   diagnoses.</span>
<span class="k">for</span> <span class="n">diagnosis</span> <span class="ow">in</span> <span class="n">diagnoses</span><span class="p">:</span>
    <span class="n">subj_with_dx</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">subj_and_meds</span>
            <span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="n">diagnosis</span><span class="p">])</span>  <span class="c1"># For this diagnosis, True or False</span>
            <span class="o">.</span><span class="n">id</span>  <span class="c1"># Arbitrary column to count.</span>
            <span class="o">.</span><span class="n">count</span><span class="p">()</span>  <span class="c1"># Take the count.</span>
            <span class="p">[</span><span class="kc">True</span><span class="p">]</span>  <span class="c1"># Only care about the count that have this disease.</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of subjects with </span><span class="si">%s</span><span class="s1">:&#39;</span> <span class="o">%</span> <span class="n">diagnosis</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">subj_with_dx</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of subjects with highBPDiagnosed:
93623
Number of subjects with diabetesDiagnosed:
40116
Number of subjects with chdDiagnosed:
22039
Number of subjects with miDiagnosed:
14725
Number of subjects with anginaDiagnosed:
11603
Number of subjects with strokeDiagnosed:
14319
Number of subjects with emphysemaDiagnosed:
9594
Number of subjects with asthmaDiagnosed:
28376
Number of subjects with otherHDDiagnosed:
31770
Number of subjects with heartFailureDiagnosed:
4940
Number with disease highBPDiagnosed = 93623
Number with disease diabetesDiagnosed = 40116
Number with disease chdDiagnosed = 22039
Number with disease miDiagnosed = 14725
Number with disease anginaDiagnosed = 11603
Number with disease strokeDiagnosed = 14319
Number with disease emphysemaDiagnosed = 9594
Number with disease asthmaDiagnosed = 28376
Number with disease otherHDDiagnosed = 31770
Number with disease heartFailureDiagnosed = 4940
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">diabetes_df</span> <span class="o">=</span> <span class="n">subj_and_meds</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Drop other diagnoses from the set - while it would be lovely to have diagnosis info</span>
<span class="c1">#   for the other diseases to train our model, it&#39;s not terribly realistic.</span>
<span class="n">other_diag</span> <span class="o">=</span> <span class="p">[</span><span class="n">diag</span> <span class="k">for</span> <span class="n">diag</span> <span class="ow">in</span> <span class="n">diagnoses</span> <span class="k">if</span> <span class="n">diag</span> <span class="ow">is</span> <span class="ow">not</span> <span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">]</span>
<span class="n">drop_cols</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">other_diag</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">diabetes_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Index([&#39;id&#39;, &#39;age&#39;, &#39;diabetesDiagnosed&#39;, &#39;rxStartYear&#39;, &#39;rxName&#39;, &#39;rxQuantity&#39;,
       &#39;rxForm&#39;, &#39;isFemale&#39;, &#39;d__Amer Indian/Alaska Native&#39;, &#39;d__Asian&#39;,
       &#39;d__Black&#39;, &#39;d__Multiple&#39;, &#39;d__Native Hawaiian/Pacific Islander&#39;,
       &#39;d__White&#39;, &#39;d__DIVORCED&#39;, &#39;d__DIVORCED IN ROUND&#39;, &#39;d__MARRIED&#39;,
       &#39;d__MARRIED IN ROUND&#39;, &#39;d__NEVER MARRIED&#39;, &#39;d__SEPARATED&#39;,
       &#39;d__SEPARATED IN ROUND&#39;, &#39;d__WIDOWED&#39;, &#39;d__WIDOWED IN ROUND&#39;],
      dtype=&#39;object&#39;)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">diabetes_df</span><span class="p">[</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>

<span class="c1"># We have (169868-129752)/169868 = 23.6% with diabetes in our sample; </span>
<span class="c1">#   this is more than 2x the population proportion!</span>
<span class="c1"># In our sample, diabetesDiagnosed=True isn&#39;t a terribly rare class.</span>
<span class="c1">#   (But we should still be wary of class imbalance.)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt output_prompt">Out[9]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>count     169868
unique         2
top        False
freq      129752
Name: diabetesDiagnosed, dtype: object</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Do-our-non-drug-variables-carry-interesting-information?">Do our non-drug variables carry interesting information?<a class="anchor-link" href="#Do-our-non-drug-variables-carry-interesting-information?">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="c1"># let&#39;s see if our other fields have any useful info...</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="s1">&#39;isFemale&#39;</span><span class="p">,</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Proportion w/ diabetes&#39;</span><span class="p">)</span>

<span class="c1"># Below regression takes around a while to run - feel free to comment out this code</span>
<span class="c1"># it just shows diabetes incidence increases pretty dramatically with age.</span>
<span class="c1"># This isn&#39;t exactly shocking :)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">logistic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_boot</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ModuleNotFoundError</span>                       Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-10-6ffc22306bdb&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">      8</span> <span class="ansi-red-fg"># it just shows diabetes incidence increases pretty dramatically with age.</span>
<span class="ansi-green-intense-fg ansi-bold">      9</span> <span class="ansi-red-fg"># This isn&#39;t exactly shocking :)</span>
<span class="ansi-green-fg">---&gt; 10</span><span class="ansi-red-fg"> </span>sns<span class="ansi-blue-fg">.</span>lmplot<span class="ansi-blue-fg">(</span>x<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">&#39;age&#39;</span><span class="ansi-blue-fg">,</span> y<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">&#39;diabetesDiagnosed&#39;</span><span class="ansi-blue-fg">,</span> data<span class="ansi-blue-fg">=</span>diabetes_df<span class="ansi-blue-fg">,</span> size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">7</span><span class="ansi-blue-fg">,</span> logistic<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span> n_boot<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">80</span><span class="ansi-blue-fg">,</span> ci<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">5</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/linearmodels.py</span> in <span class="ansi-cyan-fg">lmplot</span><span class="ansi-blue-fg">(x, y, data, hue, col, row, palette, col_wrap, size, aspect, markers, sharex, sharey, hue_order, col_order, row_order, legend, legend_out, x_estimator, x_bins, x_ci, scatter, fit_reg, ci, n_boot, units, order, logistic, lowess, robust, logx, x_partial, y_partial, truncate, x_jitter, y_jitter, scatter_kws, line_kws)</span>
<span class="ansi-green-intense-fg ansi-bold">    577</span>         scatter_kws<span class="ansi-blue-fg">=</span>scatter_kws<span class="ansi-blue-fg">,</span> line_kws<span class="ansi-blue-fg">=</span>line_kws<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">    578</span>         )
<span class="ansi-green-fg">--&gt; 579</span><span class="ansi-red-fg">     </span>facets<span class="ansi-blue-fg">.</span>map_dataframe<span class="ansi-blue-fg">(</span>regplot<span class="ansi-blue-fg">,</span> x<span class="ansi-blue-fg">,</span> y<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>regplot_kws<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    580</span> 
<span class="ansi-green-intense-fg ansi-bold">    581</span>     <span class="ansi-red-fg"># Add a legend</span>

<span class="ansi-green-fg">~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/axisgrid.py</span> in <span class="ansi-cyan-fg">map_dataframe</span><span class="ansi-blue-fg">(self, func, *args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    792</span> 
<span class="ansi-green-intense-fg ansi-bold">    793</span>             <span class="ansi-red-fg"># Draw the plot</span>
<span class="ansi-green-fg">--&gt; 794</span><span class="ansi-red-fg">             </span>self<span class="ansi-blue-fg">.</span>_facet_plot<span class="ansi-blue-fg">(</span>func<span class="ansi-blue-fg">,</span> ax<span class="ansi-blue-fg">,</span> args<span class="ansi-blue-fg">,</span> kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    795</span> 
<span class="ansi-green-intense-fg ansi-bold">    796</span>         <span class="ansi-red-fg"># Finalize the annotations and layout</span>

<span class="ansi-green-fg">~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/axisgrid.py</span> in <span class="ansi-cyan-fg">_facet_plot</span><span class="ansi-blue-fg">(self, func, ax, plot_args, plot_kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    810</span> 
<span class="ansi-green-intense-fg ansi-bold">    811</span>         <span class="ansi-red-fg"># Draw the plot</span>
<span class="ansi-green-fg">--&gt; 812</span><span class="ansi-red-fg">         </span>func<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>plot_args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>plot_kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    813</span> 
<span class="ansi-green-intense-fg ansi-bold">    814</span>         <span class="ansi-red-fg"># Sort out the supporting information</span>

<span class="ansi-green-fg">~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/linearmodels.py</span> in <span class="ansi-cyan-fg">regplot</span><span class="ansi-blue-fg">(x, y, data, x_estimator, x_bins, x_ci, scatter, fit_reg, ci, n_boot, units, order, logistic, lowess, robust, logx, x_partial, y_partial, truncate, dropna, x_jitter, y_jitter, label, color, marker, scatter_kws, line_kws, ax)</span>
<span class="ansi-green-intense-fg ansi-bold">    777</span>     scatter_kws<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">&#34;marker&#34;</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> marker
<span class="ansi-green-intense-fg ansi-bold">    778</span>     line_kws <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">{</span><span class="ansi-blue-fg">}</span> <span class="ansi-green-fg">if</span> line_kws <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span> <span class="ansi-green-fg">else</span> copy<span class="ansi-blue-fg">.</span>copy<span class="ansi-blue-fg">(</span>line_kws<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 779</span><span class="ansi-red-fg">     </span>plotter<span class="ansi-blue-fg">.</span>plot<span class="ansi-blue-fg">(</span>ax<span class="ansi-blue-fg">,</span> scatter_kws<span class="ansi-blue-fg">,</span> line_kws<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    780</span>     <span class="ansi-green-fg">return</span> ax
<span class="ansi-green-intense-fg ansi-bold">    781</span> 

<span class="ansi-green-fg">~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/linearmodels.py</span> in <span class="ansi-cyan-fg">plot</span><span class="ansi-blue-fg">(self, ax, scatter_kws, line_kws)</span>
<span class="ansi-green-intense-fg ansi-bold">    330</span>             self<span class="ansi-blue-fg">.</span>scatterplot<span class="ansi-blue-fg">(</span>ax<span class="ansi-blue-fg">,</span> scatter_kws<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    331</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>fit_reg<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 332</span><span class="ansi-red-fg">             </span>self<span class="ansi-blue-fg">.</span>lineplot<span class="ansi-blue-fg">(</span>ax<span class="ansi-blue-fg">,</span> line_kws<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    333</span> 
<span class="ansi-green-intense-fg ansi-bold">    334</span>         <span class="ansi-red-fg"># Label the axes</span>

<span class="ansi-green-fg">~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/linearmodels.py</span> in <span class="ansi-cyan-fg">lineplot</span><span class="ansi-blue-fg">(self, ax, kws)</span>
<span class="ansi-green-intense-fg ansi-bold">    375</span> 
<span class="ansi-green-intense-fg ansi-bold">    376</span>         <span class="ansi-red-fg"># Fit the regression model</span>
<span class="ansi-green-fg">--&gt; 377</span><span class="ansi-red-fg">         </span>grid<span class="ansi-blue-fg">,</span> yhat<span class="ansi-blue-fg">,</span> err_bands <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>fit_regression<span class="ansi-blue-fg">(</span>ax<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    378</span> 
<span class="ansi-green-intense-fg ansi-bold">    379</span>         <span class="ansi-red-fg"># Get set default aesthetics</span>

<span class="ansi-green-fg">~/anaconda/envs/py3/lib/python3.6/site-packages/seaborn/linearmodels.py</span> in <span class="ansi-cyan-fg">fit_regression</span><span class="ansi-blue-fg">(self, ax, x_range, grid)</span>
<span class="ansi-green-intense-fg ansi-bold">    194</span>             yhat<span class="ansi-blue-fg">,</span> yhat_boots <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>fit_poly<span class="ansi-blue-fg">(</span>grid<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>order<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    195</span>         <span class="ansi-green-fg">elif</span> self<span class="ansi-blue-fg">.</span>logistic<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 196</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">from</span> statsmodels<span class="ansi-blue-fg">.</span>genmod<span class="ansi-blue-fg">.</span>generalized_linear_model <span class="ansi-green-fg">import</span> GLM
<span class="ansi-green-intense-fg ansi-bold">    197</span>             <span class="ansi-green-fg">from</span> statsmodels<span class="ansi-blue-fg">.</span>genmod<span class="ansi-blue-fg">.</span>families <span class="ansi-green-fg">import</span> Binomial
<span class="ansi-green-intense-fg ansi-bold">    198</span>             yhat, yhat_boots = self.fit_statsmodels(grid, GLM,

<span class="ansi-red-fg">ModuleNotFoundError</span>: No module named &#39;statsmodels&#39;</pre>
</div>
</div>

<div class="output_area"><div class="prompt"></div>


<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAfUAAAEFCAYAAAD62n4IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

<div class="output_area"><div class="prompt"></div>


<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAekAAAHtCAYAAAA9X4aBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's make our train-test split - 100k+ rows so we have a good amount of data. Of course, we'l use cross-validation to get the most out of our training data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># For now, drop our qualitative variables - obviously there&#39;s a tremendous amount of info</span>
<span class="c1">#   in rxName, so we&#39;ll want to add them back later.</span>
<span class="c1"># Need to drop &#39;id&#39; since some people have multiple rx and show up twice. </span>
<span class="c1"># Don&#39;t want model to memorize their disease status via ID col on training and apply on test.</span>
<span class="n">dropped_qual</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;rxName&#39;</span><span class="p">,</span> <span class="s1">&#39;rxForm&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># TODO: refactor train-test split into function; </span>
<span class="c1"># we&#39;ll still need the variables as globals so we can refer to them outside the function.</span>

<span class="c1"># Split training and test</span>
<span class="c1"># Large dataset (&gt;100k rows), so 20% should be adequate for our test.</span>
<span class="c1"># Since we have a nontrivial class imbalance, we&#39;ll want to stratify on our outcome.</span>
<span class="c1"># (Of course, it&#39;s best practice to always stratify classification outcomes.)</span>
<span class="n">all_x</span> <span class="o">=</span> <span class="n">dropped_qual</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">all_y</span> <span class="o">=</span> <span class="n">dropped_qual</span><span class="o">.</span><span class="n">copy</span><span class="p">()[</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">all_x</span><span class="p">,</span> <span class="n">all_y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">all_y</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="p">[</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Dataset --&gt; </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
    
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>
Dataset --&gt; 

        age  rxStartYear  rxQuantity  isFemale  d__Amer Indian/Alaska Native  \
147565   59       2008.0        90.0      True                             0   
1231     66       2010.0        60.0     False                             0   
152541   61       2008.0        10.0      True                             0   

        d__Asian  d__Black  d__Multiple  d__Native Hawaiian/Pacific Islander  \
147565         0         0            0                                    0   
1231           0         0            0                                    0   
152541         0         0            1                                    0   

        d__White  d__DIVORCED  d__DIVORCED IN ROUND  d__MARRIED  \
147565         1            0                     0           0   
1231           1            0                     0           1   
152541         0            0                     0           1   

        d__MARRIED IN ROUND  d__NEVER MARRIED  d__SEPARATED  \
147565                    0                 0             1   
1231                      0                 0             0   
152541                    0                 0             0   

        d__SEPARATED IN ROUND  d__WIDOWED  d__WIDOWED IN ROUND  
147565                      0           0                    0  
1231                        0           0                    0  
152541                      0           0                    0  

Dataset --&gt; 

       age  rxStartYear  rxQuantity  isFemale  d__Amer Indian/Alaska Native  \
66379   62       2009.0        30.0      True                             0   
73405   45       2008.0        60.0      True                             0   
29974   47       2008.0        30.0      True                             0   

       d__Asian  d__Black  d__Multiple  d__Native Hawaiian/Pacific Islander  \
66379         0         1            0                                    0   
73405         0         0            0                                    0   
29974         0         1            0                                    0   

       d__White  d__DIVORCED  d__DIVORCED IN ROUND  d__MARRIED  \
66379         0            1                     0           0   
73405         1            0                     0           1   
29974         0            0                     0           0   

       d__MARRIED IN ROUND  d__NEVER MARRIED  d__SEPARATED  \
66379                    0                 0             0   
73405                    0                 0             0   
29974                    0                 0             1   

       d__SEPARATED IN ROUND  d__WIDOWED  d__WIDOWED IN ROUND  
66379                      0           0                    0  
73405                      0           0                    0  
29974                      0           0                    0  

Dataset --&gt; 

147565    False
1231       True
152541    False
Name: diabetesDiagnosed, dtype: bool

Dataset --&gt; 

66379    True
73405    True
29974    True
Name: diabetesDiagnosed, dtype: bool
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Split-our-train-and-test-sets">Split our train and test sets<a class="anchor-link" href="#Split-our-train-and-test-sets">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Logistic-Regression">Logistic Regression<a class="anchor-link" href="#Logistic-Regression">&#182;</a></h4><p>We're doing this to gain some insight into the underlying correlates of diabetes, not because it'll be an ideal model. (It won't.)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># TODO: change model to sklearn.linear_model.LogisticRegressionCV</span>
<span class="c1"># We&#39;ll use SciKit-Learn&#39;s LogisticRegressionCV model. </span>
<span class="c1"># It uses CV to set its regularization strengths.</span>

<span class="c1"># logistic = LogisticRegression(random_state=2001, solver=&#39;liblinear&#39;, class_weight=&#39;balanced&#39;, n_jobs=-1)</span>
<span class="n">logistic</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;liblinear&#39;</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Need to normalize for a logistic regression&#39;s coefficients to be meaningful.</span>
<span class="n">diabetes_norm</span> <span class="o">=</span> <span class="n">dropped_qual</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">diabetes_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">diabetes_norm</span> <span class="o">-</span> <span class="n">diabetes_norm</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">diabetes_norm</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Don&#39;t want to normalize our outcome, so set it to the non-normalized version :)</span>
<span class="n">diabetes_norm</span><span class="p">[</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dropped_qual</span><span class="p">[</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">]</span>

<span class="c1"># Split train and test again.</span>
<span class="n">all_x_norm</span> <span class="o">=</span> <span class="n">diabetes_norm</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">all_y_norm</span> <span class="o">=</span> <span class="n">diabetes_norm</span><span class="p">[</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">]</span>
<span class="n">X_train_norm</span><span class="p">,</span> <span class="n">X_test_norm</span><span class="p">,</span> <span class="n">Y_train_norm</span><span class="p">,</span> <span class="n">Y_test_norm</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">all_x</span><span class="p">,</span> <span class="n">all_y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="p">[</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Normalized dataset --&gt; </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>

<span class="n">logistic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_norm</span><span class="p">,</span> <span class="n">Y_train_norm</span><span class="p">)</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">logistic</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_norm</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>
 Normalized dataset --&gt; 

              id  age  rxStartYear  rxQuantity isFemale  \
147565  85146101   59       2008.0        90.0     True   
1231    10255101   66       2010.0        60.0    False   
152541  86112101   61       2008.0        10.0     True   

        d__Amer Indian/Alaska Native  d__Asian  d__Black  d__Multiple  \
147565                           0.0       0.0       0.0          0.0   
1231                             0.0       0.0       0.0          0.0   
152541                           0.0       0.0       0.0          1.0   

        d__Native Hawaiian/Pacific Islander  d__White  d__DIVORCED  \
147565                                  0.0       1.0          0.0   
1231                                    0.0       1.0          0.0   
152541                                  0.0       0.0          0.0   

        d__DIVORCED IN ROUND  d__MARRIED  d__MARRIED IN ROUND  \
147565                   0.0         0.0                  0.0   
1231                     0.0         1.0                  0.0   
152541                   0.0         1.0                  0.0   

        d__NEVER MARRIED  d__SEPARATED  d__SEPARATED IN ROUND  d__WIDOWED  \
147565               0.0           1.0                    0.0         0.0   
1231                 0.0           0.0                    0.0         0.0   
152541               0.0           0.0                    0.0         0.0   

        d__WIDOWED IN ROUND  
147565                  0.0  
1231                    0.0  
152541                  0.0  

 Normalized dataset --&gt; 

             id  age  rxStartYear  rxQuantity isFemale  \
66379  45239101   62       2009.0        30.0     True   
73405  46556102   45       2008.0        60.0     True   
29974  16762101   47       2008.0        30.0     True   

       d__Amer Indian/Alaska Native  d__Asian  d__Black  d__Multiple  \
66379                           0.0       0.0       1.0          0.0   
73405                           0.0       0.0       0.0          0.0   
29974                           0.0       0.0       1.0          0.0   

       d__Native Hawaiian/Pacific Islander  d__White  d__DIVORCED  \
66379                                  0.0       0.0          1.0   
73405                                  0.0       1.0          0.0   
29974                                  0.0       0.0          0.0   

       d__DIVORCED IN ROUND  d__MARRIED  d__MARRIED IN ROUND  \
66379                   0.0         0.0                  0.0   
73405                   0.0         1.0                  0.0   
29974                   0.0         0.0                  0.0   

       d__NEVER MARRIED  d__SEPARATED  d__SEPARATED IN ROUND  d__WIDOWED  \
66379               0.0           0.0                    0.0         0.0   
73405               0.0           0.0                    0.0         0.0   
29974               0.0           1.0                    0.0         0.0   

       d__WIDOWED IN ROUND  
66379                  0.0  
73405                  0.0  
29974                  0.0  

 Normalized dataset --&gt; 

147565    False
1231       True
152541    False
Name: diabetesDiagnosed, dtype: bool

 Normalized dataset --&gt; 

66379    True
73405    True
29974    True
Name: diabetesDiagnosed, dtype: bool
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[80]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># First, with more interpretable metrics based on classification accuracy.</span>

<span class="nb">print</span><span class="p">(</span><span class="n">logistic</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_norm</span><span class="p">,</span> <span class="n">Y_test_norm</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>0.593306646259
             precision    recall  f1-score   support

      False      0.766     0.644     0.700     25951
       True      0.240     0.364     0.289      8023

avg / total      0.642     0.578     0.603     33974

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[79]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Then, with more useful metrics based on predicted probabilities. </span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">logistic</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_norm</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;AUC Score:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test_norm</span><span class="p">,</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogLoss:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">Y_test_norm</span><span class="p">,</span> <span class="n">probs</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>AUC Score:
(33974,)
0.528046267198
LogLoss:
0.691388348598
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>AUC is barely over .50, indicating the logistic regression classifier isn't much better than chance. Not shocking - we haven't given it drug info, just demographics.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[66]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Now look at importances.</span>

<span class="c1"># Flatten logistic regression coefficients to a list and show corresponding quantities.</span>
<span class="c1"># I&#39;m loosely calling them &#39;importances.&#39;</span>
<span class="k">def</span> <span class="nf">print_coef_importances</span><span class="p">(</span><span class="n">importances</span><span class="p">,</span> <span class="n">columns</span><span class="p">):</span>
    <span class="n">coefficients</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">importances</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
    <span class="n">sorted_coef</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">coefficients</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">tup</span><span class="p">:</span> <span class="nb">abs</span><span class="p">(</span><span class="n">tup</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sorted_coef</span><span class="p">)</span>

<span class="n">print_coef_importances</span><span class="p">(</span><span class="n">logistic</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[(&#39;age&#39;, 6.2033913779375457e-05), (&#39;rxQuantity&#39;, 2.0160215799022857e-05), (&#39;rxStartYear&#39;, 1.080605020961186e-05), (&#39;d__NEVER MARRIED&#39;, -6.7139635208516233e-07), (&#39;d__White&#39;, -5.0018513020634522e-07), (&#39;d__Black&#39;, 4.5915361365064964e-07), (&#39;d__WIDOWED&#39;, 3.4427461081279073e-07), (&#39;isFemale&#39;, -3.1160106014185239e-07), (&#39;d__DIVORCED&#39;, 1.537909523861212e-07), (&#39;d__WIDOWED IN ROUND&#39;, 7.7935604065506666e-08), (&#39;d__SEPARATED&#39;, 7.2772061093636999e-08), (&#39;d__MARRIED&#39;, 5.5146081097351865e-08), (&#39;d__Amer Indian/Alaska Native&#39;, 4.1613911307577813e-08), (&#39;d__MARRIED IN ROUND&#39;, -3.095100368847204e-08), (&#39;d__Asian&#39;, -1.845101762170867e-08), (&#39;d__SEPARATED IN ROUND&#39;, 1.8231307298067554e-08), (&#39;d__Native Hawaiian/Pacific Islander&#39;, 1.6660190432015383e-08), (&#39;d__Multiple&#39;, 1.1147354693465299e-08), (&#39;d__DIVORCED IN ROUND&#39;, -9.8643387242391467e-09), (&#39;id&#39;, -5.900488616708205e-10)]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's move on to</p>
<h2 id="Tree-Based-Models">Tree-Based Models<a class="anchor-link" href="#Tree-Based-Models">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[81]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Random forest is a great black box classifier </span>
<span class="c1">#   (and in a slightly altered form, regressor). </span>
<span class="c1"># We should be able to get good results with little tuning.</span>
<span class="c1"># Note: we should be using balanced class weights here to compensate for class imbalance,</span>
<span class="c1">#   but I&#39;m intentionally omitting that to show how it skews results.</span>
<span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">max_features</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> 
    <span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> 
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">Y_pred</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Make sure we aren&#39;t using our old logistic y-hats.</span>
<span class="c1"># No need to normalize. </span>
<span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt output_prompt">Out[81]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=10,
            min_samples_split=20, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=-1, oob_score=False,
            random_state=2001, verbose=0, warm_start=False)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[82]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Score it.</span>

<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">random_forest</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>

<span class="n">probs</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;AUC Score:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">probs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogLoss:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">probs</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>0.816094660623
             precision    recall  f1-score   support

      False      0.812     0.987     0.891     25951
       True      0.864     0.262     0.403      8023

avg / total      0.825     0.816     0.776     33974

AUC Score:
0.863780424102

LogLoss:
0.401109195697
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For simplicity's sake, we'll look at F1-score, since it captures both precision and recall pretty well. We'll also pay attention to accuracy and recall since they can be easily explained to non-data scientists.</p>
<p>We'll also look at log-loss and AUC, which describe model quality rather than the performance of a particular cutoff for predicting classes. (In other words, they describe the accuracy of our predicted probabilities of class membership rather than our ultimate class labels.)</p>
<p>With our first naive model, we're already getting .86 AUC, which is very respectable.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[86]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">print_coef_importances</span><span class="p">(</span><span class="n">random_forest</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[(&#39;age&#39;, 0.3577093357527954), (&#39;id&#39;, 0.29247474708816801), (&#39;rxStartYear&#39;, 0.10669557814615584), (&#39;rxQuantity&#39;, 0.1065093341713839), (&#39;d__NEVER MARRIED&#39;, 0.021151961353537304), (&#39;isFemale&#39;, 0.020272319287124554), (&#39;d__Black&#39;, 0.016388611164839129), (&#39;d__White&#39;, 0.015970265269041023), (&#39;d__WIDOWED&#39;, 0.012336259083978635), (&#39;d__MARRIED&#39;, 0.0097865329548047619), (&#39;d__DIVORCED&#39;, 0.0076916834185343274), (&#39;d__SEPARATED&#39;, 0.0052070261919044359), (&#39;d__Asian&#39;, 0.0046158120198111705), (&#39;d__Amer Indian/Alaska Native&#39;, 0.0044406406277527448), (&#39;d__DIVORCED IN ROUND&#39;, 0.0041289514773978561), (&#39;d__WIDOWED IN ROUND&#39;, 0.0038531097734076591), (&#39;d__SEPARATED IN ROUND&#39;, 0.0032982258117143187), (&#39;d__Multiple&#39;, 0.0030978237470242025), (&#39;d__MARRIED IN ROUND&#39;, 0.0026873381131272394), (&#39;d__Native Hawaiian/Pacific Islander&#39;, 0.0016844445474972995)]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I included a logistic regression here so we can inspect it for information. For example, it shows that ethnicity has a pretty significant impact on the probability of having diabetes. However, as a predictive model, it's not terribly impressive. In particular, its recall for the positive class is awful.</p>
<p>Our RF accuracy is around what we'd expect, given how little information it has. And it's using age correctly while also giving some weight to rxStartYear and rxQuantitiy. (I suspect some information about type of drug / administration method may be contained in the quantity.)</p>
<p>It's not shocking that our recall is much better for False than True. That's just saying it's way easier to predict that someone doesn't have diabetes than that she does. Also, since False is around 3/4 of the dataset, our model's going to benefit more from assuming results will be False.</p>
<p>We can tweak our decision threshold using predict_proba in the RF model, but let's hold off for now. First, we'll engineer our variables a bit more, and then we can think about setting thresholds.</p>
<p>Also one major concern - one of our most important features is 'id'. That implies leakage some sort of data leakage from the ID, as it should be random. In other words, smaller ids, presumably older ones, may be less likely to have diabetes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[90]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y_pred</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Make sure we aren&#39;t using our old logistic y-hats.</span>
<span class="c1"># No need to normalize. </span>
<span class="n">X_train_clean</span><span class="p">,</span> <span class="n">X_test_clean</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">X_test</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_clean</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="c1"># Score it.</span>

<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_clean</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">random_forest</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_clean</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>

<span class="n">probs</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_clean</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;AUC Score:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">probs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogLoss:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">probs</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>0.792664979102
             precision    recall  f1-score   support

      False      0.796     0.980     0.878     25951
       True      0.744     0.186     0.298      8023

avg / total      0.784     0.793     0.741     33974

AUC Score:
0.785238158187

LogLoss:
0.447398101952
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While our AUC has dropped, it wouldn't be proper to let data leak in from our ID column. This is meant to be a useful model, not an attempt to game a Kaggle competition :)</p>
<p>The next order of business is to deal with the rxName column.</p>
<h3 id="Incorporating-prescription-names-into-our-model">Incorporating prescription names into our model<a class="anchor-link" href="#Incorporating-prescription-names-into-our-model">&#182;</a></h3><p>First, let's just try label encoding our prescriptions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">with_rx</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;rxForm&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># I&#39;m just using SKLearn.LabelEncoder because I originally wrote this notebook with it,</span>
<span class="c1">#   but at present, I prefer to convert to pd.categorical dtype and use df.col.cat.codes to train models</span>
<span class="c1">#   as it makes converting between labels and codes very simple.</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">with_rx</span><span class="p">[</span><span class="s1">&#39;rxEnc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">with_rx</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">])</span>

<span class="n">all_x</span> <span class="o">=</span> <span class="n">with_rx</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;rxName&#39;</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">all_y</span> <span class="o">=</span> <span class="n">with_rx</span><span class="p">[</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">]</span>

<span class="n">X_train_enc</span><span class="p">,</span> <span class="n">X_test_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">,</span> <span class="n">Y_test_enc</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">all_x</span><span class="p">,</span> <span class="n">all_y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="p">[</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">X_test_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">,</span> <span class="n">Y_test_enc</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Dataset --&gt; </span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>

Dataset --&gt; 


        age  rxStartYear  rxQuantity  isFemale  d__Amer Indian/Alaska Native  \
18901    51       2011.0        31.0      True                             0   
161166   69       2008.0         9.0     False                             0   
136061   24       2009.0       120.0      True                             0   
36871    66       2011.0        90.0     False                             0   

        d__Asian  d__Black  d__Multiple  d__Native Hawaiian/Pacific Islander  \
18901          0         0            0                                    0   
161166         0         0            0                                    0   
136061         0         0            0                                    0   
36871          0         1            0                                    0   

        d__White  d__DIVORCED  d__DIVORCED IN ROUND  d__MARRIED  \
18901          1            0                     0           0   
161166         1            0                     0           1   
136061         1            0                     0           0   
36871          0            0                     0           0   

        d__MARRIED IN ROUND  d__NEVER MARRIED  d__SEPARATED  \
18901                     0                 1             0   
161166                    0                 0             0   
136061                    1                 0             0   
36871                     0                 0             1   

        d__SEPARATED IN ROUND  d__WIDOWED  d__WIDOWED IN ROUND  rxEnc  
18901                       0           0                    0   5331  
161166                      0           0                    0   5748  
136061                      0           0                    0   3682  
36871                       0           0                    0   3923  


Dataset --&gt; 


        age  rxStartYear  rxQuantity  isFemale  d__Amer Indian/Alaska Native  \
149602   56       2008.0        60.0      True                             0   
57205    61       2009.0        30.0     False                             0   
125708   21       2009.0        15.0     False                             0   
87514    34       2010.0        10.0     False                             0   

        d__Asian  d__Black  d__Multiple  d__Native Hawaiian/Pacific Islander  \
149602         0         0            0                                    0   
57205          0         0            0                                    0   
125708         0         0            0                                    0   
87514          0         0            0                                    0   

        d__White  d__DIVORCED  d__DIVORCED IN ROUND  d__MARRIED  \
149602         1            0                     0           1   
57205          1            0                     0           0   
125708         1            0                     0           0   
87514          1            0                     0           0   

        d__MARRIED IN ROUND  d__NEVER MARRIED  d__SEPARATED  \
149602                    0                 0             0   
57205                     1                 0             0   
125708                    0                 1             0   
87514                     0                 1             0   

        d__SEPARATED IN ROUND  d__WIDOWED  d__WIDOWED IN ROUND  rxEnc  
149602                      0           0                    0   5567  
57205                       0           0                    0   7323  
125708                      0           0                    0   1649  
87514                       0           0                    0   6715  


Dataset --&gt; 


18901     False
161166    False
136061    False
36871     False
Name: diabetesDiagnosed, dtype: bool


Dataset --&gt; 


149602    False
57205      True
125708    False
87514     False
Name: diabetesDiagnosed, dtype: bool
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Let&#39;s also pickle our dataset for easy replication and so we don&#39;t have to re-run the above script every time.</span>

<span class="n">X_train_enc</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_x_train_2017_06_13.pkl.xz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;xz&#39;</span><span class="p">)</span>
<span class="n">Y_train_enc</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_y_train_2017_06_13.pkl.xz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;xz&#39;</span><span class="p">)</span>
<span class="n">X_test_enc</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_x_test_2017_06_13.pkl.xz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;xz&#39;</span><span class="p">)</span>
<span class="n">Y_test_enc</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_y_test_2017_06_13.pkl.xz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;xz&#39;</span><span class="p">)</span>

<span class="c1"># If using pd &lt; 0.20, omit compression.</span>
<span class="c1"># Also, you&#39;ll need backports.lzma to run above. Otherwise, below code will work.</span>
<span class="c1"># X_train_enc.to_pickle(&#39;./input/mep_x_train_2017_06_13.pkl&#39;)</span>
<span class="c1"># Y_train_enc.to_pickle(&#39;./input/mep_y_train_2017_06_13.pkl&#39;)</span>
<span class="c1"># X_test_enc.to_pickle(&#39;./input/mep_x_test_2017_06_13.pkl&#39;)</span>
<span class="c1"># Y_test_enc.to_pickle(&#39;./input/mep_y_test_2017_06_13.pkl&#39;)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train_enc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_x_train_2017_06_13.pkl.xz&#39;</span><span class="p">)</span>
<span class="n">Y_train_enc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_y_train_2017_06_13.pkl.xz&#39;</span><span class="p">)</span>
<span class="n">X_test_enc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_x_test_2017_06_13.pkl.xz&#39;</span><span class="p">)</span>
<span class="n">Y_test_enc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_y_test_2017_06_13.pkl.xz&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Random forest benefits significantly from inclusion of drug labels:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[100]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">max_features</span><span class="o">=.</span><span class="mi">3</span><span class="p">,</span> 
    <span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> 
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">)</span>
<span class="n">Y_pred_enc</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_enc</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">random_forest</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_enc</span><span class="p">,</span> <span class="n">Y_test_enc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">Y_pred_enc</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">print_coef_importances</span><span class="p">(</span><span class="n">random_forest</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">X_train_enc</span><span class="p">)</span>

<span class="n">probs</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_enc</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;AUC Score:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">probs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogLoss:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">probs</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>0.800582798611
             precision    recall  f1-score   support

      False      0.802     0.981     0.883     25951
       True      0.781     0.216     0.339      8023

avg / total      0.797     0.801     0.754     33974


[(&#39;age&#39;, 0.36056278570399025), (&#39;rxEnc&#39;, 0.262825718365092), (&#39;rxQuantity&#39;, 0.12031847313529757), (&#39;rxStartYear&#39;, 0.11745041065500818), (&#39;d__NEVER MARRIED&#39;, 0.024852817359040564), (&#39;isFemale&#39;, 0.020551927606031355), (&#39;d__Black&#39;, 0.018332382124160117), (&#39;d__White&#39;, 0.017191011270011249), (&#39;d__WIDOWED&#39;, 0.011467710559893939), (&#39;d__MARRIED&#39;, 0.0092743445731561618), (&#39;d__DIVORCED&#39;, 0.0065972091731750383), (&#39;d__SEPARATED&#39;, 0.0050327254063759694), (&#39;d__Amer Indian/Alaska Native&#39;, 0.0044432517984435201), (&#39;d__Asian&#39;, 0.004301073009678559), (&#39;d__DIVORCED IN ROUND&#39;, 0.0035001306260339132), (&#39;d__WIDOWED IN ROUND&#39;, 0.003468788582554466), (&#39;d__SEPARATED IN ROUND&#39;, 0.0029908531775029857), (&#39;d__MARRIED IN ROUND&#39;, 0.0026996744238864594), (&#39;d__Multiple&#39;, 0.0025743736179944599), (&#39;d__Native Hawaiian/Pacific Islander&#39;, 0.0015643388326734323)]

AUC Score:
0.794428783614

LogLoss:
0.440975034957
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[101]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Let&#39;s go ahead and try balanced class weights.</span>

<span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">max_features</span><span class="o">=.</span><span class="mi">3</span><span class="p">,</span> 
    <span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> 
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">)</span>
<span class="n">Y_pred_enc</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_enc</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">random_forest</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_enc</span><span class="p">,</span> <span class="n">Y_test_enc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">Y_pred_enc</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">print_coef_importances</span><span class="p">(</span><span class="n">random_forest</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">X_train_enc</span><span class="p">)</span>

<span class="n">probs</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_enc</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;AUC Score:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">probs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogLoss:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">probs</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>0.725643138871
             precision    recall  f1-score   support

      False      0.883     0.739     0.804     25951
       True      0.447     0.683     0.540      8023

avg / total      0.780     0.726     0.742     33974


[(&#39;age&#39;, 0.42200980348967621), (&#39;rxEnc&#39;, 0.22196416618372841), (&#39;rxQuantity&#39;, 0.11296335231539321), (&#39;rxStartYear&#39;, 0.10945869683185157), (&#39;d__NEVER MARRIED&#39;, 0.031287475585166299), (&#39;isFemale&#39;, 0.019323284523234806), (&#39;d__White&#39;, 0.016180200186202105), (&#39;d__Black&#39;, 0.014517297219627578), (&#39;d__WIDOWED&#39;, 0.0097029079282310252), (&#39;d__MARRIED&#39;, 0.0092063148798403067), (&#39;d__DIVORCED&#39;, 0.0068408432582869683), (&#39;d__SEPARATED&#39;, 0.0043443012651767721), (&#39;d__Asian&#39;, 0.0040051568069508674), (&#39;d__Amer Indian/Alaska Native&#39;, 0.0036746526986122696), (&#39;d__DIVORCED IN ROUND&#39;, 0.0036323460229164329), (&#39;d__MARRIED IN ROUND&#39;, 0.0028012895201026196), (&#39;d__WIDOWED IN ROUND&#39;, 0.0025395978832523703), (&#39;d__SEPARATED IN ROUND&#39;, 0.0023741794270886797), (&#39;d__Multiple&#39;, 0.0021776235601624902), (&#39;d__Native Hawaiian/Pacific Islander&#39;, 0.0009965104144989106)]

AUC Score:
0.793821847772

LogLoss:
0.523457754493
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[102]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># And, finally, just add some more trees.</span>
<span class="c1"># Since we&#39;re now tuning parameters, we should really be using CV.</span>
<span class="c1"># Repeated k-fold is the gold standard, so we&#39;ll use it.</span>
<span class="c1"># SKL 20.x implements it but wasn&#39;t available at the time of writing (2017/06),</span>
<span class="c1">#   so use a homebrewed version:</span>


<span class="k">def</span> <span class="nf">get_repeated_k_fold_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">folds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">agg_inner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">agg_outer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">score_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fit_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">print_description</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Do k-fold cv a number of times and aggregate results.</span>

<span class="sd">    :param model: SKLearn model</span>
<span class="sd">    :params xs: pd.DataFrame with cols for x&#39;s, rows for examples</span>
<span class="sd">    :param yx: pd.Series with col of y&#39;s, rows for examples</span>
<span class="sd">    :param folds: int, number of folds in inner k-fold cv</span>
<span class="sd">    :param iters: int, number of times to repeat k-fold cv process</span>
<span class="sd">    :param agg_inner: agg function for the inner loop - each </span>
<span class="sd">      k-fold cross-validation</span>
<span class="sd">    :param agg_outer: agg function for the outer loop - combining</span>
<span class="sd">      the various k-fold cross-validations</span>
<span class="sd">    :param score_func: SKLearn-style scoring metric </span>
<span class="sd">      (e.g. sklearn.metrics.mean_squared_error) </span>
<span class="sd">      that accepts y_true, y_pred as params</span>
<span class="sd">    :param fit_kwargs: kwargs to add to .fit() method of model when called</span>
<span class="sd">    :param print_description: Bool, whether or not to print the cv result&#39;s </span>
<span class="sd">      .describe() method&#39;s result - e.g. count, mean, std, quantiles, min/max</span>
<span class="sd">    :return: aggregated final score of the inner and outer loops</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set defaults</span>
    <span class="n">folds</span> <span class="o">=</span> <span class="n">folds</span> <span class="ow">or</span> <span class="mi">10</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="n">iters</span> <span class="ow">or</span> <span class="mi">10</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span> <span class="ow">or</span> <span class="mi">1</span>
    <span class="n">agg_inner</span> <span class="o">=</span> <span class="n">agg_inner</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span>
    <span class="n">agg_outer</span> <span class="o">=</span> <span class="n">agg_outer</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span>
    <span class="n">fit_kwargs</span> <span class="o">=</span> <span class="n">fit_kwargs</span> <span class="ow">or</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">print_description</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">print_description</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># If supplied a metric, convert to SKLearn scorer object.</span>
    <span class="c1"># Will use greater_is_better=True so no sign flip happens.</span>
    <span class="k">if</span> <span class="n">score_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">score_func</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scorer</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dummy</span> <span class="o">=</span> <span class="n">DummyRegressor</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
    <span class="n">all_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">all_dummy_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># model.fit(xs, ys, **fit_kwargs)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed</span> <span class="o">+</span> <span class="n">iters</span><span class="p">):</span>
        <span class="c1"># Set up CV.</span>
        <span class="n">kfold</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">folds</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
        <span class="c1"># Get CV scores and aggregate over all folds.</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">agg_inner</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">all_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="c1"># Repeat for dummy model.</span>
        <span class="n">dummy_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">dummy</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">)</span>
        <span class="n">dummy_score</span> <span class="o">=</span> <span class="n">agg_inner</span><span class="p">(</span><span class="n">dummy_scores</span><span class="p">)</span>
        <span class="n">all_dummy_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dummy_score</span><span class="p">)</span>
    <span class="n">all_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">all_scores</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">print_description</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">all_scores</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">agg_outer</span><span class="p">(</span><span class="n">all_scores</span><span class="p">)</span>
    <span class="n">dummy_score</span> <span class="o">=</span> <span class="n">agg_outer</span><span class="p">(</span><span class="n">all_dummy_scores</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Performance of model vs. dummy: &#39;</span><span class="p">)</span>
    <span class="n">percent_error</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">dummy_score</span> <span class="o">-</span> <span class="n">score</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">dummy_score</span><span class="p">)</span>
    <span class="n">to_interpolate</span> <span class="o">=</span> <span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">dummy_score</span><span class="p">,</span> <span class="n">percent_error</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error of </span><span class="si">%.3f</span><span class="s1">, </span><span class="si">%.3f</span><span class="s1"> for model, dummy; </span><span class="si">%.3f</span><span class="s1"> percent less error.&#39;</span> <span class="o">%</span> <span class="n">to_interpolate</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>

<span class="n">get_repeated_k_fold_score</span><span class="p">(</span><span class="n">random_forest</span><span class="p">,</span> <span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">,</span> <span class="n">folds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                              <span class="n">agg_inner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">agg_outer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span>
                              <span class="n">score_func</span><span class="o">=</span><span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">fit_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">print_description</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>count    2.000000
mean     0.705857
std      0.000883
min      0.705232
25%      0.705544
50%      0.705857
75%      0.706169
max      0.706481
dtype: float64
Performance of model vs. dummy: 
Error of 0.706, 0.500 for model, dummy; -41.171 percent less error.
</pre>
</div>
</div>

<div class="output_area"><div class="prompt output_prompt">Out[102]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>0.7058565107121059</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[64]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># From here, we could tune our hyperparameters a bit, </span>
<span class="c1">#   but the advantage of random forests is that such a step is largely unnecessary.</span>
<span class="c1"># Test our final RF</span>

<span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> 
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">max_features</span><span class="o">=.</span><span class="mi">3</span><span class="p">,</span> 
    <span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> 
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">)</span>
<span class="n">score_classif_on_test</span><span class="p">(</span><span class="n">random_forest</span><span class="p">,</span> <span class="n">X_test_enc</span><span class="p">,</span> <span class="n">Y_test_enc</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>AUC Score:
0.815120011624

LogLoss:
0.418319933018

             precision    recall  f1-score   support

      False      0.832     0.959     0.891     25951
       True      0.736     0.373     0.495      8023

avg / total      0.809     0.820     0.797     33974

</pre>
</div>
</div>

<div class="output_area"><div class="prompt output_prompt">Out[64]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>array([ 0.25555714,  0.38064172,  0.00166667, ...,  0.00896465,
        0.16441307,  0.26412454])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have CV set up, we could tune our model a bit. But that's not the point of this notebook.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Getting back to our new encoded features, note that logistic regression doesn't benefit too much from encoded drug names without further processing:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[104]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logistic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">)</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">logistic</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_enc</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">logistic</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">Y_pred_enc</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>

<span class="n">print_coef_importances</span><span class="p">(</span><span class="n">logistic</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">X_train_enc</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="n">probs</span> <span class="o">=</span> <span class="n">logistic</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_enc</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;AUC Score:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogLoss:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">probs</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>0.614214019751
             precision    recall  f1-score   support

      False      0.883     0.739     0.804     25951
       True      0.447     0.683     0.540      8023

avg / total      0.780     0.726     0.742     33974

[(&#39;d__Amer Indian/Alaska Native&#39;, 4.6100036502151118), (&#39;d__Native Hawaiian/Pacific Islander&#39;, 4.3882438096225886), (&#39;d__Black&#39;, 4.1946082918323464), (&#39;d__Multiple&#39;, 4.00312068983503), (&#39;d__Asian&#39;, 3.6767788000322703), (&#39;d__White&#39;, 3.6540942163760453), (&#39;d__SEPARATED&#39;, 3.0683279944537447), (&#39;d__SEPARATED IN ROUND&#39;, 2.9380129072322618), (&#39;d__WIDOWED IN ROUND&#39;, 2.8784851462807701), (&#39;d__DIVORCED&#39;, 2.7712634888388572), (&#39;d__MARRIED IN ROUND&#39;, 2.6874017503882723), (&#39;d__MARRIED&#39;, 2.6280901673176009), (&#39;d__DIVORCED IN ROUND&#39;, 2.6196596715053411), (&#39;d__WIDOWED&#39;, 2.5243085519937476), (&#39;d__NEVER MARRIED&#39;, 2.4112997799638709), (&#39;isFemale&#39;, -0.180838939934476), (&#39;age&#39;, 0.036794668758996611), (&#39;rxStartYear&#39;, -0.01639758467133311), (&#39;rxQuantity&#39;, 1.1047039198874027e-05), (&#39;rxEnc&#39;, 6.6698788536737992e-06)]

AUC Score:
0.683131604706

LogLoss:
0.640942677814
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another approach is to one-hot encode. But here it gets a little messy and subjective. There are at least a few way to do this:</p>
<p>1) pick the n most common drugs to use<br>
2) pick the n most common drugs in diabetes patients to use<br>
3) pick the n drugs that have the highest precision or recall when thought of as a "test" for diabetes</p>
<p>I don't really think it's an option to encode every drug - we'd have literally thousands of columns.<br>
I'll start with (1) as the simplest solution, but there's certainly value to the others, and, ideally, we'd try them all.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[72]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Most common drugs overall:</span>
<span class="n">diabetes_df</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt output_prompt">Out[72]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>AZITHROMYCIN                   3358
LISINOPRIL                     2865
AMOXICILLIN                    2687
SIMVASTATIN                    2312
IBUPROFEN                      2243
PREDNISONE                     1712
HYDROCO/APAP                   1703
LIPITOR                        1630
OMEPRAZOLE                     1496
METFORMIN                      1422
HYDROCHLOROTHIAZIDE            1281
NAPROXEN                       1198
ATENOLOL                       1102
FUROSEMIDE                     1082
LEVOTHYROXIN                    954
APAP/HYDROCODONE BITARTRATE     951
AMLODIPINE                      928
HYDROCHLOROT                    852
AMLODIPINE BESYLATE             832
NEXIUM                          829
Name: rxName, dtype: int64</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[73]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Mote that most of these aren&#39;t all that specific for diabetes; </span>
<span class="c1">#   people with type 2 diabetes are often overweight and have other health issues.</span>

<span class="n">have_diabetes</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="p">[</span><span class="n">diabetes_df</span><span class="p">[</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="kc">True</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">have_diabetes</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>METFORMIN                  1158
LISINOPRIL                  975
SIMVASTATIN                 701
LIPITOR                     518
ACTOS                       449
FUROSEMIDE                  447
GLIPIZIDE                   431
METFORMIN HCL               426
ONETOUCH                    366
LANTUS                      350
GLYBURIDE                   342
AZITHROMYCIN                328
OMEPRAZOLE                  315
HYDROCHLOROTHIAZIDE         300
METFORMIN HYDROCHLORIDE     288
GLIMEPIRIDE                 284
INSULIN SYRG                268
AMOXICILLIN                 250
ASPIRIN                     247
HYDROCO/APAP                245
Name: rxName, dtype: int64
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[74]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># confirming that our label encoder worked correctly </span>

<span class="nb">print</span><span class="p">(</span><span class="n">with_rx</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">with_rx</span><span class="p">[</span><span class="s1">&#39;rxEnc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>AZITHROMYCIN    3358
LISINOPRIL      2865
AMOXICILLIN     2687
SIMVASTATIN     2312
IBUPROFEN       2243
PREDNISONE      1712
HYDROCO/APAP    1703
LIPITOR         1630
OMEPRAZOLE      1496
METFORMIN       1422
Name: rxName, dtype: int64
817     3358
3923    2865
486     2687
6295    2312
3443    2243
5632    1712
3295    1703
3906    1630
5028    1496
4235    1422
Name: rxEnc, dtype: int64
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Filtering-prescriptions-by-frequency-(so-we-can-feed-them-into-the-model)">Filtering prescriptions by frequency (so we can feed them into the model)<a class="anchor-link" href="#Filtering-prescriptions-by-frequency-(so-we-can-feed-them-into-the-model)">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[105]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Re-initalize with_rx df so we can run code nonsequentially. </span>
<span class="n">with_rx</span> <span class="o">=</span> <span class="n">diabetes_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;rxForm&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">filtered_rx</span> <span class="o">=</span> <span class="n">with_rx</span>

<span class="n">filter_by_len</span> <span class="o">=</span> <span class="n">with_rx</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;rxName&#39;</span><span class="p">)[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">common_drugs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">filter_by_len</span><span class="p">)</span>
<span class="n">filtered_rx</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">rxName</span> <span class="k">if</span> <span class="n">rxName</span> <span class="ow">in</span> <span class="n">common_drugs</span> <span class="k">else</span> <span class="s1">&#39;OTHER&#39;</span> <span class="k">for</span> <span class="n">rxName</span> <span class="ow">in</span> <span class="n">filtered_rx</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">filtered_rx</span><span class="o">.</span><span class="n">rxName</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">filtered_rx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">with_rx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ATENOLOL&#39;</span> <span class="ow">in</span> <span class="n">common_drugs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;this is not there&#39;</span> <span class="ow">in</span> <span class="n">common_drugs</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>0         ATENOLOL
1     AZITHROMYCIN
2            OTHER
3     HYDROCO/APAP
4     CARISOPRODOL
5            OTHER
6            OTHER
7            OTHER
8          NORVASC
9         SEROQUEL
10       CLONIDINE
11       COMBIVENT
12         DIGOXIN
13       LORAZEPAM
14     SIMVASTATIN
15    HYDROCHLOROT
16      CARVEDILOL
17         LIPITOR
18           OTHER
19           OTHER
Name: rxName, dtype: object
(169868, 22)
(169868, 22)
True
False
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Great, so we have all rx with frequency &gt;=100. Now we have to re-encode to labels and then one-hot encode. If we didn't re-encode, our one-hot encoder would still be based on the entire list of rx's, and we'd have an inordinate number of columns.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[106]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">drop_cols</span><span class="p">(</span><span class="n">filtered_rx</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;rxEnc&#39;</span><span class="p">])</span>
<span class="n">filtered_rx</span><span class="p">[</span><span class="s1">&#39;rxEnc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">filtered_rx</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">filtered_rx</span><span class="p">[</span><span class="s1">&#39;rxName&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">filtered_rx</span><span class="p">[</span><span class="s1">&#39;rxEnc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2"> We&#39;ll have to add columns = &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">filtered_rx</span><span class="o">.</span><span class="n">rxEnc</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>No cols found to drop.
0        ATENOLOL
1    AZITHROMYCIN
2           OTHER
3    HYDROCO/APAP
4    CARISOPRODOL
5           OTHER
6           OTHER
7           OTHER
8         NORVASC
9        SEROQUEL
Name: rxName, dtype: object
0     37
1     43
2    220
3    138
4     56
5    220
6    220
7    220
8    213
9    263
Name: rxEnc, dtype: int64


 We&#39;ll have to add columns = 
329
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Great, our others are correctly encoded and our maxencoding seems reasonable.</p>
<p>Now it's time for</p>
<h3 id="One-hot-encoding">One-hot encoding<a class="anchor-link" href="#One-hot-encoding">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[107]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rx_ohe</span> <span class="o">=</span> <span class="n">filtered_rx</span>

<span class="c1"># we&#39;ll just use the dummies feature in pd</span>
<span class="n">onehot_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">filtered_rx</span><span class="o">.</span><span class="n">rxEnc</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">onehot_df</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>count    169868.000000
mean          0.001189
std           0.034464
min           0.000000
25%           0.000000
50%           0.000000
75%           0.000000
max           1.000000
Name: 0, dtype: float64
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above columns are about 99.9% sparse! We might want to encode them as a sparse matrix.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[108]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># get cols back into the data frame</span>
<span class="n">rx_ohe</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">filtered_rx</span><span class="p">,</span> <span class="n">onehot_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">rx_ohe</span><span class="o">.</span><span class="n">columns</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt output_prompt">Out[108]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>Index([                          u&#39;id&#39;,                          u&#39;age&#39;,
                  u&#39;diabetesDiagnosed&#39;,                  u&#39;rxStartYear&#39;,
                             u&#39;rxName&#39;,                   u&#39;rxQuantity&#39;,
                           u&#39;isFemale&#39;, u&#39;d__Amer Indian/Alaska Native&#39;,
                           u&#39;d__Asian&#39;,                     u&#39;d__Black&#39;,
       ...
                                   320,                             321,
                                   322,                             323,
                                   324,                             325,
                                   326,                             327,
                                   328,                             329],
      dtype=&#39;object&#39;, length=353)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[109]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Split training and test, again.</span>
<span class="n">X_train_ohe</span><span class="p">,</span> <span class="n">X_test_ohe</span><span class="p">,</span> <span class="n">Y_train_ohe</span><span class="p">,</span> <span class="n">Y_test_ohe</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">rx_ohe</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span>
        <span class="p">[</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;rxName&#39;</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">rx_ohe</span><span class="p">[</span><span class="s1">&#39;diabetesDiagnosed&#39;</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's see how our models are affected by one-hot encoding:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[111]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logistic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_ohe</span><span class="p">,</span> <span class="n">Y_train_ohe</span><span class="p">)</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">logistic</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_ohe</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">logistic</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_ohe</span><span class="p">,</span> <span class="n">Y_test_ohe</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test_ohe</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>

<span class="n">probs</span> <span class="o">=</span> <span class="n">logistic</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_ohe</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;AUC Score:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test_ohe</span><span class="p">,</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogLoss:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">Y_test_ohe</span><span class="p">,</span> <span class="n">probs</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>0.675899217048
             precision    recall  f1-score   support

      False      0.861     0.687     0.764     25951
       True      0.387     0.641     0.483      8023

avg / total      0.749     0.676     0.698     33974


AUC Score:
0.742683123944

LogLoss:
0.582382484001
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our logistic couldn't make sense of our encoded prescriptions, but it's gained lots of information from one-hot encoding. At this point, it comes very close to where RF was with a single label-encoded column.</p>
<p>Now we can try RF again. It will be painfully slow with 300+ columns.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[114]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_ohe</span><span class="p">,</span> <span class="n">Y_train_ohe</span><span class="p">)</span>
<span class="n">Y_pred_ohe</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_ohe</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">random_forest</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_ohe</span><span class="p">,</span> <span class="n">Y_test_ohe</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test_ohe</span><span class="p">,</span> <span class="n">Y_pred_ohe</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>

<span class="n">probs</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_ohe</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;AUC Score:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_test_ohe</span><span class="p">,</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogLoss:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">Y_test_ohe</span><span class="p">,</span> <span class="n">probs</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>0.795108023783
             precision    recall  f1-score   support

      False      0.791     0.994     0.881     25951
       True      0.890     0.151     0.258      8023

avg / total      0.814     0.795     0.734     33974


AUC Score:
0.77374064631

LogLoss:
0.454300897467
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this case, RF was able to exploit the encodings effectively, so one-hot encoding was actually harmful (we lost the raw encodings, which actually dropped our AUC by around 2%).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Feature-Engineering">Feature Engineering<a class="anchor-link" href="#Feature-Engineering">&#182;</a></h4><p>While we aren't dealing with very many columns and probably don't have all that many features to extract, let's try making a few.</p>
<p>For instance, we could combine 'd__SEPARATED' with 'd__SEPARATED IN ROUND' and 'd__WIDOWED', 'd__WIDOWED IN ROUND'.
In these cases, the fact that a separation or widowing happened in a particular year probably isn't all that relevant.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="RF-Tuning">RF Tuning<a class="anchor-link" href="#RF-Tuning">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have RF running well, we can tune it using CV.</p>
<p>Interactions between these parameters are pretty minimal, so no need to grid search. 
We could use random search, but I'll do something even simpler and search over each parameter separately.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[124]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Try increasing trees</span>

<span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> 
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
    <span class="n">max_features</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> 
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Optimize params using grid search.</span>


<span class="k">def</span> <span class="nf">test_params</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_value_dict</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run a grid search for a model using given param values and print results.</span>
<span class="sd">    CV is stratified K-Fold in this instance.</span>
<span class="sd">    (SKL is smart enough to know to prefer stratified CV in classification problems.)</span>
<span class="sd">    </span>
<span class="sd">    :param model: SciKit-Learn model, i.e. one that has </span>
<span class="sd">      .fit() and .predict() methods</span>
<span class="sd">    :param param_value_dict: dict, with keys of param names </span>
<span class="sd">      and values of [values] to try for given param</span>
<span class="sd">    :return: None, prints results to stdout</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># First, convert ROC_AUC from SKL metric to scorer API.</span>
    <span class="n">roc_scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">)</span>
    <span class="c1"># Init GridSearchCV instance.</span>
    <span class="n">gsearch</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span> 
        <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_value_dict</span><span class="p">,</span> 
        <span class="n">scoring</span><span class="o">=</span><span class="n">roc_scorer</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># assume model itself is multithreaded - thus only 1 job</span>
        <span class="n">iid</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
        <span class="n">cv</span><span class="o">=</span><span class="mi">10</span>
    <span class="p">)</span>
    <span class="n">gsearch</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Raw grid search scores:&#39;</span><span class="p">)</span>
        <span class="n">gsearch1</span><span class="o">.</span><span class="n">grid_scores_</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best params for this search:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">gsearch</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best AUC for these params:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">gsearch</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Setup param dict.</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">700</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">]</span>
<span class="p">}</span>
<span class="c1"># Use simple 10-fold stratified CV.</span>
<span class="n">test_params</span><span class="p">(</span><span class="n">random_forest</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[125]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="p">}</span>
<span class="n">test_params</span><span class="p">(</span><span class="n">random_forest</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Best params for this search:
{&#39;min_samples_split&#39;: 13}
Best AUC for these params:
0.71458857454
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[122]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>
<span class="p">}</span>
<span class="n">test_params</span><span class="p">(</span><span class="n">random_forest</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="o">.</span><span class="mi">10</span><span class="p">,</span> <span class="o">.</span><span class="mi">20</span><span class="p">,</span> <span class="o">.</span><span class="mi">30</span><span class="p">,</span> <span class="o">.</span><span class="mi">40</span><span class="p">,</span> <span class="o">.</span><span class="mi">50</span><span class="p">,</span> <span class="o">.</span><span class="mi">60</span><span class="p">,</span> <span class="o">.</span><span class="mi">70</span><span class="p">,</span> <span class="o">.</span><span class="mi">80</span><span class="p">,</span> <span class="o">.</span><span class="mi">90</span><span class="p">,</span> <span class="s1">&#39;auto&#39;</span><span class="p">]</span>  <span class="c1"># auto means sqrt here</span>
<span class="p">}</span>
<span class="n">test_params</span><span class="p">(</span><span class="n">random_forest</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Best params for this search:
{&#39;min_samples_leaf&#39;: 5}
Best AUC for these params:
0.712809402922
Best params for this search:
{&#39;max_features&#39;: 0.2}
Best AUC for these params:
0.710624613055
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span> 
    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
    <span class="n">max_features</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">2001</span><span class="p">,</span> 
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[126]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Get final CV score for RF model with different seed</span>

<span class="n">get_repeated_k_fold_score</span><span class="p">(</span><span class="n">random_forest</span><span class="p">,</span> <span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">,</span> <span class="n">folds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                              <span class="n">agg_inner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">agg_outer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2002</span><span class="p">,</span>
                              <span class="n">score_func</span><span class="o">=</span><span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">fit_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">print_description</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Performance of model vs. dummy: 
Error of 0.713, 0.500 for model, dummy; -42.537 percent less error.
</pre>
</div>
</div>

<div class="output_area"><div class="prompt output_prompt">Out[126]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>0.7126871479083617</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># And test performance:</span>
<span class="c1"># TODO!</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>TODO CONCLUSION RF</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bonus">Bonus<a class="anchor-link" href="#Bonus">&#182;</a></h3><h3 id="XGBoost">XGBoost<a class="anchor-link" href="#XGBoost">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[155]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load train and test from pkl.</span>

<span class="n">X_train_enc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_x_train_2017_06_13.pkl.xz&#39;</span><span class="p">)</span>
<span class="n">Y_train_enc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_y_train_2017_06_13.pkl.xz&#39;</span><span class="p">)</span>
<span class="n">X_test_enc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_x_test_2017_06_13.pkl.xz&#39;</span><span class="p">)</span>
<span class="n">Y_test_enc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="s1">&#39;./input/mep_y_test_2017_06_13.pkl.xz&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Cross-Validation">Cross-Validation<a class="anchor-link" href="#Cross-Validation">&#182;</a></h4><p>1) We'll use cross-validation to establish num_rounds, 
2) then tune hyperparameters on that num_rounds, 
3) and finally fit our resulting model using test set validation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[127]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Build xgboost datamatrix object for further operations.</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[150]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Cross validate to get n_rounds for our simple starter gradient-boosted tree model.</span>
<span class="c1"># We&#39;ll decrease the learning rate later to improve performance, but use this one</span>
<span class="c1">#   to set parameters w/ CV.</span>
<span class="n">xgb_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span><span class="mi">9</span><span class="p">,</span> 
    <span class="s1">&#39;eta&#39;</span><span class="p">:</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> 
    <span class="s1">&#39;silent&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> 
    <span class="s1">&#39;objective&#39;</span><span class="p">:</span><span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span>
    <span class="s1">&#39;eval_metric&#39;</span><span class="p">:</span><span class="s1">&#39;auc&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;subsample&#39;</span><span class="p">:</span><span class="mf">0.80</span><span class="p">,</span> 
    <span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:</span><span class="mf">0.80</span>
<span class="p">}</span>

<span class="c1"># It would be better to use repeated, stratified k-fold validation rather than </span>
<span class="c1">#   simple stratified k-fold, </span>
<span class="c1">#   but given our low number of columns and large number of training examples,</span>
<span class="c1">#   I&#39;m not very worried about overfitting and 1x10-fold CV should be adequate.</span>
<span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
    <span class="n">xgb_params</span><span class="p">,</span> 
    <span class="n">dtrain</span><span class="p">,</span> 
    <span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">stratified</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> 
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;logloss&#39;</span><span class="p">,</span> <span class="s1">&#39;auc&#39;</span><span class="p">},</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="mi">2001</span><span class="p">,</span> 
    <span class="n">verbose_eval</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[0]	train-auc:0.736994+0.00730653	train-logloss:0.657191+0.00104062	test-auc:0.725014+0.00916592	test-logloss:0.657793+0.00106194
[20]	train-auc:0.814671+0.00134697	train-logloss:0.439679+0.00094271	test-auc:0.792183+0.00467564	test-logloss:0.449996+0.00181048
[40]	train-auc:0.835481+0.00118035	train-logloss:0.405057+0.00121355	test-auc:0.806548+0.0043459	test-logloss:0.422157+0.0027617
[60]	train-auc:0.848699+0.00126471	train-logloss:0.389739+0.00142101	test-auc:0.814377+0.00439097	test-logloss:0.411935+0.00314276
[80]	train-auc:0.858763+0.00122946	train-logloss:0.379272+0.000966371	test-auc:0.818938+0.00364885	test-logloss:0.406244+0.00262638
[100]	train-auc:0.867795+0.00111532	train-logloss:0.370414+0.000832694	test-auc:0.822548+0.00332232	test-logloss:0.402015+0.0025701
[120]	train-auc:0.875949+0.00105389	train-logloss:0.362421+0.000790587	test-auc:0.825392+0.0027878	test-logloss:0.398723+0.0022755
[140]	train-auc:0.882934+0.000993645	train-logloss:0.355472+0.000809815	test-auc:0.827775+0.00294327	test-logloss:0.396065+0.00236317
[160]	train-auc:0.889095+0.00105758	train-logloss:0.349056+0.00109076	test-auc:0.829621+0.00263039	test-logloss:0.393816+0.00215368
[180]	train-auc:0.894988+0.00105705	train-logloss:0.342835+0.00119382	test-auc:0.831563+0.00274183	test-logloss:0.391745+0.00237056
[200]	train-auc:0.899987+0.00119759	train-logloss:0.337376+0.00128627	test-auc:0.832837+0.00273348	test-logloss:0.390252+0.00233655
[220]	train-auc:0.904653+0.00099134	train-logloss:0.332038+0.00103194	test-auc:0.834171+0.00288882	test-logloss:0.388763+0.00251629
[240]	train-auc:0.908966+0.000713698	train-logloss:0.327142+0.000825729	test-auc:0.834938+0.00328169	test-logloss:0.38776+0.00283505
[260]	train-auc:0.913047+0.000777197	train-logloss:0.322338+0.000942822	test-auc:0.83581+0.0032644	test-logloss:0.386776+0.0028029
[280]	train-auc:0.916867+0.000836348	train-logloss:0.317715+0.000957122	test-auc:0.836375+0.00343579	test-logloss:0.385985+0.00304294
[300]	train-auc:0.920103+0.000756778	train-logloss:0.313565+0.000865947	test-auc:0.837073+0.00348187	test-logloss:0.385215+0.00310886
[320]	train-auc:0.923314+0.000864092	train-logloss:0.309422+0.000978745	test-auc:0.837599+0.0034479	test-logloss:0.384565+0.00309501
[340]	train-auc:0.926441+0.000764257	train-logloss:0.305276+0.000947671	test-auc:0.838243+0.00357501	test-logloss:0.383861+0.00326165
[360]	train-auc:0.9293+0.000639208	train-logloss:0.301396+0.000774953	test-auc:0.838554+0.00358548	test-logloss:0.38337+0.0033492
[380]	train-auc:0.931946+0.000685413	train-logloss:0.297697+0.000929908	test-auc:0.838731+0.00352917	test-logloss:0.383062+0.00328209
</pre>
</div>
</div>

<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-150-08f92544928a&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>     metrics<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">{</span><span class="ansi-blue-fg">&#39;logloss&#39;</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#39;auc&#39;</span><span class="ansi-blue-fg">}</span><span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>     seed <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">2001</span><span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">---&gt; 27</span><span class="ansi-red-fg">     </span>verbose_eval<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">20</span><span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">     28</span>     <span class="ansi-red-fg"># callbacks=[xgb.callback.print_evaluation(show_stdv=True)]</span>
<span class="ansi-green-intense-fg ansi-bold">     29</span> )

<span class="ansi-green-fg">/Users/jan/anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc</span> in <span class="ansi-cyan-fg">cv</span><span class="ansi-blue-fg">(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)</span>
<span class="ansi-green-intense-fg ansi-bold">    405</span>                            evaluation_result_list=None))
<span class="ansi-green-intense-fg ansi-bold">    406</span>         <span class="ansi-green-fg">for</span> fold <span class="ansi-green-fg">in</span> cvfolds<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 407</span><span class="ansi-red-fg">             </span>fold<span class="ansi-blue-fg">.</span>update<span class="ansi-blue-fg">(</span>i<span class="ansi-blue-fg">,</span> obj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    408</span>         res <span class="ansi-blue-fg">=</span> aggcv<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">[</span>f<span class="ansi-blue-fg">.</span>eval<span class="ansi-blue-fg">(</span>i<span class="ansi-blue-fg">,</span> feval<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> f <span class="ansi-green-fg">in</span> cvfolds<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    409</span> 

<span class="ansi-green-fg">/Users/jan/anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/training.pyc</span> in <span class="ansi-cyan-fg">update</span><span class="ansi-blue-fg">(self, iteration, fobj)</span>
<span class="ansi-green-intense-fg ansi-bold">    216</span>     <span class="ansi-green-fg">def</span> update<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> iteration<span class="ansi-blue-fg">,</span> fobj<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    217</span>         <span class="ansi-blue-fg">&#34;&#34;&#34;&#34;Update the boosters for one iteration&#34;&#34;&#34;</span>
<span class="ansi-green-fg">--&gt; 218</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>bst<span class="ansi-blue-fg">.</span>update<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>dtrain<span class="ansi-blue-fg">,</span> iteration<span class="ansi-blue-fg">,</span> fobj<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    219</span> 
<span class="ansi-green-intense-fg ansi-bold">    220</span>     <span class="ansi-green-fg">def</span> eval<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> iteration<span class="ansi-blue-fg">,</span> feval<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/Users/jan/anaconda/lib/python2.7/site-packages/xgboost-0.6-py2.7.egg/xgboost/core.pyc</span> in <span class="ansi-cyan-fg">update</span><span class="ansi-blue-fg">(self, dtrain, iteration, fobj)</span>
<span class="ansi-green-intense-fg ansi-bold">    817</span> 
<span class="ansi-green-intense-fg ansi-bold">    818</span>         <span class="ansi-green-fg">if</span> fobj <span class="ansi-green-fg">is</span> None<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 819</span><span class="ansi-red-fg">             </span>_check_call<span class="ansi-blue-fg">(</span>_LIB<span class="ansi-blue-fg">.</span>XGBoosterUpdateOneIter<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">.</span>handle<span class="ansi-blue-fg">,</span> iteration<span class="ansi-blue-fg">,</span> dtrain<span class="ansi-blue-fg">.</span>handle<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    820</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    821</span>             pred <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>predict<span class="ansi-blue-fg">(</span>dtrain<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[129]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">classifier_for_gridsearch</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="n">learning_rate</span> <span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">339</span><span class="p">,</span> 
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> 
    <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">objective</span><span class="o">=</span> <span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span> 
    <span class="n">nthread</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">seed</span><span class="o">=</span><span class="mi">2001</span>
<span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[69]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Optimize with grid search.</span>

<span class="n">params1</span> <span class="o">=</span> <span class="p">{</span>
 <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
 <span class="s1">&#39;min_child_weight&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">test_params</span><span class="p">(</span><span class="n">classifier_for_gridsearch</span><span class="p">,</span> <span class="n">params1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt output_prompt">Out[69]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>([mean: 0.79871, std: 0.00252, params: {&#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 1},
  mean: 0.79881, std: 0.00254, params: {&#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 2},
  mean: 0.81963, std: 0.00211, params: {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 1},
  mean: 0.81897, std: 0.00159, params: {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 2},
  mean: 0.83228, std: 0.00208, params: {&#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 1},
  mean: 0.83066, std: 0.00199, params: {&#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 2},
  mean: 0.83755, std: 0.00192, params: {&#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 1},
  mean: 0.83562, std: 0.00221, params: {&#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 2},
  mean: 0.83746, std: 0.00229, params: {&#39;max_depth&#39;: 11, &#39;min_child_weight&#39;: 1},
  mean: 0.83510, std: 0.00283, params: {&#39;max_depth&#39;: 11, &#39;min_child_weight&#39;: 2},
  mean: 0.83429, std: 0.00258, params: {&#39;max_depth&#39;: 13, &#39;min_child_weight&#39;: 1},
  mean: 0.83287, std: 0.00275, params: {&#39;max_depth&#39;: 13, &#39;min_child_weight&#39;: 2}],
 {&#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 1},
 0.8375502124420201)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">params2</span> <span class="o">=</span> <span class="p">{</span>
 <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
 <span class="s1">&#39;gamma&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">i</span><span class="o">/</span><span class="mf">10.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)]</span>
<span class="p">}</span>

<span class="n">test_params</span><span class="p">(</span><span class="n">classifier_for_gridsearch</span><span class="p">,</span> <span class="n">params2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[78]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">params3</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;subsample&#39;</span><span class="p">:[</span><span class="n">i</span> <span class="o">/</span> <span class="mf">10.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">)],</span>
    <span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:[</span><span class="n">i</span> <span class="o">/</span> <span class="mf">10.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">)]</span>
<span class="p">}</span>

<span class="n">test_params</span><span class="p">(</span><span class="n">classifier_for_gridsearch</span><span class="p">,</span> <span class="n">params3</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>([mean: 0.83789, std: 0.00229, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.7}, mean: 0.83984, std: 0.00215, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.7}, mean: 0.84272, std: 0.00246, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.7}, mean: 0.84001, std: 0.00202, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 0.7}, mean: 0.83446, std: 0.00193, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83755, std: 0.00192, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.8}, mean: 0.84020, std: 0.00204, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83908, std: 0.00218, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83090, std: 0.00167, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83495, std: 0.00179, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83721, std: 0.00253, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83773, std: 0.00220, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 0.9}, mean: 0.82819, std: 0.00174, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 1.0}, mean: 0.83353, std: 0.00225, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 1.0}, mean: 0.83679, std: 0.00197, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 1.0}, mean: 0.83684, std: 0.00271, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 1.0}], [mean: 0.83789, std: 0.00229, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.7}, mean: 0.83984, std: 0.00215, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.7}, mean: 0.84272, std: 0.00246, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.7}, mean: 0.84001, std: 0.00202, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 0.7}, mean: 0.83446, std: 0.00193, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83755, std: 0.00192, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.8}, mean: 0.84020, std: 0.00204, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83908, std: 0.00218, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83090, std: 0.00167, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83495, std: 0.00179, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83721, std: 0.00253, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83773, std: 0.00220, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 0.9}, mean: 0.82819, std: 0.00174, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 1.0}, mean: 0.83353, std: 0.00225, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 1.0}, mean: 0.83679, std: 0.00197, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 1.0}, mean: 0.83684, std: 0.00271, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 1.0}], [mean: 0.83789, std: 0.00229, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.7}, mean: 0.83984, std: 0.00215, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.7}, mean: 0.84272, std: 0.00246, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.7}, mean: 0.84001, std: 0.00202, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 0.7}, mean: 0.83446, std: 0.00193, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83755, std: 0.00192, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.8}, mean: 0.84020, std: 0.00204, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83908, std: 0.00218, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83090, std: 0.00167, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83495, std: 0.00179, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83721, std: 0.00253, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83773, std: 0.00220, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 0.9}, mean: 0.82819, std: 0.00174, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 1.0}, mean: 0.83353, std: 0.00225, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 1.0}, mean: 0.83679, std: 0.00197, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 1.0}, mean: 0.83684, std: 0.00271, params: {&#39;subsample&#39;: 1.0, &#39;colsample_bytree&#39;: 1.0}])
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Model-Training-Using-SKL-Wrapper">Model Training Using SKL Wrapper<a class="anchor-link" href="#Model-Training-Using-SKL-Wrapper">&#182;</a></h4><p>XGBoost provides a convenient SKL wrapper so you can use it as you would the other models in SKL.</p>
<p>While XGBoost's native models tend to be a bit faster and let you access more of XGB's inteface,
  we'll use the SKL wrapper here.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[147]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">skl_boost</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="n">missing</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> 
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">2400</span><span class="p">,</span> 
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> 
    <span class="n">silent</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>        
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> 
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> 
    <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>            
    <span class="n">nthread</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">seed</span><span class="o">=</span><span class="mi">2016</span><span class="p">,</span> 
    <span class="n">objective</span><span class="o">=</span><span class="s1">&#39;binary:logistic&#39;</span>
<span class="p">)</span>

<span class="c1"># Here I&#39;m using performance on the test set for early stopping.</span>
<span class="c1">#   Given the large number of training examples and few columns, </span>
<span class="c1">#   I think this is acceptable. But you might prefer to set the</span>
<span class="c1">#   number of training rounds purely with CV.</span>
<span class="n">skl_boost</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
            <span class="n">eval_metric</span><span class="o">=</span><span class="s2">&quot;auc&quot;</span><span class="p">,</span> <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">X_test_enc</span><span class="p">,</span> <span class="n">Y_test_enc</span><span class="p">)])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[0]	validation_0-auc:0.743262
Will train until validation_0-auc hasn&#39;t improved in 40 rounds.
[40]	validation_0-auc:0.784491
[80]	validation_0-auc:0.795197
[120]	validation_0-auc:0.80188
[160]	validation_0-auc:0.807302
[200]	validation_0-auc:0.811926
[240]	validation_0-auc:0.815163
[280]	validation_0-auc:0.817509
[320]	validation_0-auc:0.819837
[360]	validation_0-auc:0.821489
[400]	validation_0-auc:0.822823
[440]	validation_0-auc:0.824378
[480]	validation_0-auc:0.825412
[520]	validation_0-auc:0.827066
[560]	validation_0-auc:0.828169
[600]	validation_0-auc:0.829378
[640]	validation_0-auc:0.830374
[680]	validation_0-auc:0.831341
[720]	validation_0-auc:0.832451
[760]	validation_0-auc:0.833484
[800]	validation_0-auc:0.833997
[840]	validation_0-auc:0.834512
[880]	validation_0-auc:0.835258
[920]	validation_0-auc:0.835846
[960]	validation_0-auc:0.836545
[1000]	validation_0-auc:0.836908
[1040]	validation_0-auc:0.837567
[1080]	validation_0-auc:0.837881
[1120]	validation_0-auc:0.838382
[1160]	validation_0-auc:0.838929
[1200]	validation_0-auc:0.839505
[1240]	validation_0-auc:0.839874
[1280]	validation_0-auc:0.840457
[1320]	validation_0-auc:0.840996
[1360]	validation_0-auc:0.841456
[1400]	validation_0-auc:0.841561
[1440]	validation_0-auc:0.841935
[1480]	validation_0-auc:0.842136
[1520]	validation_0-auc:0.842423
[1560]	validation_0-auc:0.842739
[1600]	validation_0-auc:0.842872
[1640]	validation_0-auc:0.843259
[1680]	validation_0-auc:0.843551
[1720]	validation_0-auc:0.843683
[1760]	validation_0-auc:0.843946
[1800]	validation_0-auc:0.844139
[1840]	validation_0-auc:0.844333
[1880]	validation_0-auc:0.844585
[1920]	validation_0-auc:0.845013
[1960]	validation_0-auc:0.845237
[1999]	validation_0-auc:0.845424
</pre>
</div>
</div>

<div class="output_area"><div class="prompt output_prompt">Out[147]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,
       gamma=0, learning_rate=0.02, max_delta_step=0, max_depth=9,
       min_child_weight=1, missing=None, n_estimators=2000, nthread=-1,
       objective=&#39;binary:logistic&#39;, reg_alpha=0, reg_lambda=1,
       scale_pos_weight=1, seed=2016, silent=True, subsample=0.9)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[139]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">skl_boost</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="n">missing</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> 
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1686</span><span class="p">,</span> 
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> 
    <span class="n">silent</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>        
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> 
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> 
    <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>            
    <span class="n">nthread</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">seed</span><span class="o">=</span><span class="mi">2016</span><span class="p">,</span> 
    <span class="n">objective</span><span class="o">=</span><span class="s1">&#39;binary:logistic&#39;</span>
<span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Model-Scores-&amp;-Features">Model Scores &amp; Features<a class="anchor-link" href="#Model-Scores-&amp;-Features">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[142]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Verify above w/ k-fold CV &amp; different seed (so we&#39;re not overfitting to those particular cv-splits)</span>

<span class="n">get_repeated_k_fold_score</span><span class="p">(</span><span class="n">skl_boost</span><span class="p">,</span> <span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">,</span> <span class="n">folds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                              <span class="n">agg_inner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">agg_outer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2002</span><span class="p">,</span>
                              <span class="n">score_func</span><span class="o">=</span><span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">fit_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">print_description</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Performance of model vs. dummy: 
Error of 0.679, 0.500 for model, dummy; -35.791 percent less error.
</pre>
</div>
</div>

<div class="output_area"><div class="prompt output_prompt">Out[142]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>0.6789536671988217</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[144]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># CV perf with a different seed is far worse than test performance, </span>
<span class="c1">#   suggesting we might be overfitting to the test set. </span>
<span class="c1"># Let&#39;s try checking how many rds we get with CV and lower lrate.</span>

<span class="n">xgb_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span><span class="mi">9</span><span class="p">,</span> 
    <span class="s1">&#39;eta&#39;</span><span class="p">:</span><span class="o">.</span><span class="mi">02</span><span class="p">,</span> 
    <span class="s1">&#39;silent&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> 
    <span class="s1">&#39;objective&#39;</span><span class="p">:</span><span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span>
    <span class="s1">&#39;eval_metric&#39;</span><span class="p">:</span><span class="s1">&#39;auc&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;subsample&#39;</span><span class="p">:</span><span class="mf">0.90</span><span class="p">,</span> 
    <span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:</span><span class="mf">0.70</span>
<span class="p">}</span>

<span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
    <span class="n">xgb_params</span><span class="p">,</span> 
    <span class="n">dtrain</span><span class="p">,</span> 
    <span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
    <span class="n">stratified</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">2500</span><span class="p">,</span> 
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;error&#39;</span><span class="p">},</span> <span class="c1"># try metrics={&#39;logloss&#39;, &#39;auc&#39;}</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="mi">2003</span><span class="p">,</span> 
    <span class="n">verbose_eval</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[0]	train-error:0.221061+0.00489284	test-error:0.224403+0.00524578
[20]	train-error:0.198558+0.00137553	test-error:0.202172+0.00189445
[40]	train-error:0.194455+0.00110733	test-error:0.199185+0.00156989
[60]	train-error:0.191194+0.00107074	test-error:0.196616+0.00156405
[80]	train-error:0.188156+0.000778677	test-error:0.193857+0.00147352
[100]	train-error:0.185744+0.000766213	test-error:0.19173+0.00125641
[120]	train-error:0.183226+0.000649301	test-error:0.189435+0.00141424
[140]	train-error:0.180722+0.000449606	test-error:0.187506+0.00126284
[160]	train-error:0.178492+0.000570488	test-error:0.185395+0.00135194
[180]	train-error:0.176483+0.00060382	test-error:0.183459+0.00143589
[200]	train-error:0.174496+0.000608873	test-error:0.181627+0.0015561
[220]	train-error:0.172856+0.00057987	test-error:0.180427+0.00166718
[240]	train-error:0.171551+0.000562294	test-error:0.179522+0.00160945
[260]	train-error:0.170408+0.000572816	test-error:0.178551+0.00161733
[280]	train-error:0.169335+0.000461947	test-error:0.17766+0.0014494
[300]	train-error:0.168436+0.000456619	test-error:0.176888+0.00131774
[320]	train-error:0.167581+0.000426162	test-error:0.176255+0.00108376
[340]	train-error:0.166802+0.000447369	test-error:0.175563+0.00106874
[360]	train-error:0.166034+0.00040892	test-error:0.175173+0.00101594
[380]	train-error:0.165305+0.000412802	test-error:0.174725+0.000956793
[400]	train-error:0.164639+0.00040259	test-error:0.174305+0.000870114
[420]	train-error:0.16399+0.000442411	test-error:0.173716+0.000998041
[440]	train-error:0.163335+0.000419896	test-error:0.17329+0.000943744
[460]	train-error:0.162563+0.000321566	test-error:0.172848+0.000934658
[480]	train-error:0.162019+0.000289289	test-error:0.172495+0.000804425
[500]	train-error:0.161368+0.000342763	test-error:0.172325+0.000732776
[520]	train-error:0.160639+0.000299701	test-error:0.171921+0.00082517
[540]	train-error:0.160086+0.000344351	test-error:0.17159+0.000672181
[560]	train-error:0.159527+0.000309463	test-error:0.171523+0.000815865
[580]	train-error:0.158954+0.000359277	test-error:0.17117+0.00092372
[600]	train-error:0.15833+0.000420418	test-error:0.170971+0.000878218
[620]	train-error:0.157786+0.000430296	test-error:0.170788+0.000895049
[640]	train-error:0.157118+0.000457291	test-error:0.170515+0.0008054
[660]	train-error:0.15653+0.000401612	test-error:0.170162+0.000855623
[680]	train-error:0.156005+0.0004078	test-error:0.169927+0.000939594
[700]	train-error:0.155495+0.000433354	test-error:0.169765+0.000950004
[720]	train-error:0.154956+0.000464642	test-error:0.169404+0.000939624
[740]	train-error:0.154428+0.00048018	test-error:0.16922+0.00109687
[760]	train-error:0.153933+0.000468627	test-error:0.169029+0.00101068
[780]	train-error:0.153456+0.000463019	test-error:0.168852+0.000923257
[800]	train-error:0.152862+0.000469569	test-error:0.168595+0.000928567
[820]	train-error:0.152376+0.000504465	test-error:0.168469+0.000916096
[840]	train-error:0.151831+0.000480865	test-error:0.168271+0.00095934
[860]	train-error:0.151347+0.000482976	test-error:0.168175+0.00104777
[880]	train-error:0.150881+0.000454849	test-error:0.167962+0.0010594
[900]	train-error:0.1504+0.000488052	test-error:0.167866+0.000955878
[920]	train-error:0.149924+0.000471824	test-error:0.167653+0.00106416
[940]	train-error:0.149337+0.000535097	test-error:0.167601+0.00106566
[960]	train-error:0.148865+0.000464816	test-error:0.167322+0.00109959
[980]	train-error:0.148348+0.000495603	test-error:0.167145+0.00102648
[1000]	train-error:0.147854+0.000497573	test-error:0.16699+0.00110779
[1020]	train-error:0.147366+0.000451596	test-error:0.166902+0.00123188
[1040]	train-error:0.146884+0.000429251	test-error:0.166645+0.00130581
[1060]	train-error:0.14647+0.00034647	test-error:0.16649+0.00129708
[1080]	train-error:0.146049+0.00039342	test-error:0.166431+0.00123885
[1100]	train-error:0.145537+0.00037747	test-error:0.166284+0.00130002
[1120]	train-error:0.145126+0.000376342	test-error:0.166181+0.00125539
[1140]	train-error:0.144686+0.000363335	test-error:0.16599+0.00125768
[1160]	train-error:0.144204+0.00038557	test-error:0.165872+0.00124569
[1180]	train-error:0.143735+0.000347527	test-error:0.165585+0.00120539
[1200]	train-error:0.143311+0.000390592	test-error:0.165482+0.00119063
[1220]	train-error:0.142865+0.000417379	test-error:0.165393+0.00125919
[1240]	train-error:0.142338+0.000432796	test-error:0.165305+0.00130093
[1260]	train-error:0.141932+0.000441362	test-error:0.165107+0.0012574
[1280]	train-error:0.141469+0.000468851	test-error:0.164982+0.00121969
</pre>
</div>
</div>

<div class="output_area"><div class="prompt output_prompt">Out[144]:</div>

<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>test-error-mean</th>
      <th>test-error-std</th>
      <th>train-error-mean</th>
      <th>train-error-std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.224403</td>
      <td>0.005246</td>
      <td>0.221061</td>
      <td>0.004893</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.216853</td>
      <td>0.008242</td>
      <td>0.213367</td>
      <td>0.008318</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.214734</td>
      <td>0.007105</td>
      <td>0.211057</td>
      <td>0.007357</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.211408</td>
      <td>0.006326</td>
      <td>0.208266</td>
      <td>0.006403</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.210627</td>
      <td>0.007650</td>
      <td>0.207371</td>
      <td>0.007425</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.208913</td>
      <td>0.005325</td>
      <td>0.205374</td>
      <td>0.005211</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.207154</td>
      <td>0.005072</td>
      <td>0.203516</td>
      <td>0.005004</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.206992</td>
      <td>0.005207</td>
      <td>0.203350</td>
      <td>0.005254</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.206455</td>
      <td>0.005200</td>
      <td>0.202681</td>
      <td>0.005355</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.206139</td>
      <td>0.005536</td>
      <td>0.202377</td>
      <td>0.005639</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.205087</td>
      <td>0.005154</td>
      <td>0.201184</td>
      <td>0.004917</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.204387</td>
      <td>0.003712</td>
      <td>0.200678</td>
      <td>0.003372</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.204292</td>
      <td>0.003450</td>
      <td>0.200328</td>
      <td>0.003036</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.203593</td>
      <td>0.002831</td>
      <td>0.199644</td>
      <td>0.002334</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.203431</td>
      <td>0.002784</td>
      <td>0.199418</td>
      <td>0.002300</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.203019</td>
      <td>0.002364</td>
      <td>0.199287</td>
      <td>0.001973</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.203203</td>
      <td>0.002942</td>
      <td>0.199635</td>
      <td>0.002185</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.202702</td>
      <td>0.002283</td>
      <td>0.199276</td>
      <td>0.001597</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.202570</td>
      <td>0.002127</td>
      <td>0.199218</td>
      <td>0.001326</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.202415</td>
      <td>0.002187</td>
      <td>0.198885</td>
      <td>0.001554</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.202172</td>
      <td>0.001894</td>
      <td>0.198558</td>
      <td>0.001376</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.201922</td>
      <td>0.001569</td>
      <td>0.198316</td>
      <td>0.001318</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.201768</td>
      <td>0.001436</td>
      <td>0.197959</td>
      <td>0.001290</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.201554</td>
      <td>0.001431</td>
      <td>0.197817</td>
      <td>0.001384</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.201569</td>
      <td>0.001462</td>
      <td>0.197651</td>
      <td>0.001309</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.201429</td>
      <td>0.001360</td>
      <td>0.197428</td>
      <td>0.001421</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.201172</td>
      <td>0.001483</td>
      <td>0.197253</td>
      <td>0.001270</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.201311</td>
      <td>0.001323</td>
      <td>0.197375</td>
      <td>0.001487</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.201363</td>
      <td>0.001402</td>
      <td>0.197284</td>
      <td>0.001568</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.200627</td>
      <td>0.001628</td>
      <td>0.196761</td>
      <td>0.001578</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1251</th>
      <td>0.165107</td>
      <td>0.001247</td>
      <td>0.142096</td>
      <td>0.000413</td>
    </tr>
    <tr>
      <th>1252</th>
      <td>0.165099</td>
      <td>0.001264</td>
      <td>0.142076</td>
      <td>0.000419</td>
    </tr>
    <tr>
      <th>1253</th>
      <td>0.165099</td>
      <td>0.001291</td>
      <td>0.142054</td>
      <td>0.000430</td>
    </tr>
    <tr>
      <th>1254</th>
      <td>0.165107</td>
      <td>0.001285</td>
      <td>0.142042</td>
      <td>0.000441</td>
    </tr>
    <tr>
      <th>1255</th>
      <td>0.165077</td>
      <td>0.001261</td>
      <td>0.142024</td>
      <td>0.000437</td>
    </tr>
    <tr>
      <th>1256</th>
      <td>0.165019</td>
      <td>0.001232</td>
      <td>0.142010</td>
      <td>0.000436</td>
    </tr>
    <tr>
      <th>1257</th>
      <td>0.165085</td>
      <td>0.001311</td>
      <td>0.141986</td>
      <td>0.000434</td>
    </tr>
    <tr>
      <th>1258</th>
      <td>0.165092</td>
      <td>0.001309</td>
      <td>0.141966</td>
      <td>0.000433</td>
    </tr>
    <tr>
      <th>1259</th>
      <td>0.165063</td>
      <td>0.001269</td>
      <td>0.141953</td>
      <td>0.000442</td>
    </tr>
    <tr>
      <th>1260</th>
      <td>0.165107</td>
      <td>0.001257</td>
      <td>0.141932</td>
      <td>0.000441</td>
    </tr>
    <tr>
      <th>1261</th>
      <td>0.165085</td>
      <td>0.001251</td>
      <td>0.141915</td>
      <td>0.000445</td>
    </tr>
    <tr>
      <th>1262</th>
      <td>0.165077</td>
      <td>0.001307</td>
      <td>0.141890</td>
      <td>0.000430</td>
    </tr>
    <tr>
      <th>1263</th>
      <td>0.165063</td>
      <td>0.001304</td>
      <td>0.141861</td>
      <td>0.000455</td>
    </tr>
    <tr>
      <th>1264</th>
      <td>0.165077</td>
      <td>0.001315</td>
      <td>0.141830</td>
      <td>0.000444</td>
    </tr>
    <tr>
      <th>1265</th>
      <td>0.165077</td>
      <td>0.001333</td>
      <td>0.141803</td>
      <td>0.000438</td>
    </tr>
    <tr>
      <th>1266</th>
      <td>0.165077</td>
      <td>0.001387</td>
      <td>0.141770</td>
      <td>0.000444</td>
    </tr>
    <tr>
      <th>1267</th>
      <td>0.165055</td>
      <td>0.001346</td>
      <td>0.141761</td>
      <td>0.000451</td>
    </tr>
    <tr>
      <th>1268</th>
      <td>0.165018</td>
      <td>0.001323</td>
      <td>0.141744</td>
      <td>0.000454</td>
    </tr>
    <tr>
      <th>1269</th>
      <td>0.165041</td>
      <td>0.001328</td>
      <td>0.141725</td>
      <td>0.000462</td>
    </tr>
    <tr>
      <th>1270</th>
      <td>0.165048</td>
      <td>0.001345</td>
      <td>0.141704</td>
      <td>0.000469</td>
    </tr>
    <tr>
      <th>1271</th>
      <td>0.165048</td>
      <td>0.001353</td>
      <td>0.141686</td>
      <td>0.000475</td>
    </tr>
    <tr>
      <th>1272</th>
      <td>0.165004</td>
      <td>0.001314</td>
      <td>0.141670</td>
      <td>0.000462</td>
    </tr>
    <tr>
      <th>1273</th>
      <td>0.164989</td>
      <td>0.001291</td>
      <td>0.141642</td>
      <td>0.000462</td>
    </tr>
    <tr>
      <th>1274</th>
      <td>0.164996</td>
      <td>0.001296</td>
      <td>0.141614</td>
      <td>0.000462</td>
    </tr>
    <tr>
      <th>1275</th>
      <td>0.165026</td>
      <td>0.001256</td>
      <td>0.141595</td>
      <td>0.000460</td>
    </tr>
    <tr>
      <th>1276</th>
      <td>0.165026</td>
      <td>0.001280</td>
      <td>0.141581</td>
      <td>0.000467</td>
    </tr>
    <tr>
      <th>1277</th>
      <td>0.165011</td>
      <td>0.001272</td>
      <td>0.141558</td>
      <td>0.000465</td>
    </tr>
    <tr>
      <th>1278</th>
      <td>0.165004</td>
      <td>0.001237</td>
      <td>0.141536</td>
      <td>0.000470</td>
    </tr>
    <tr>
      <th>1279</th>
      <td>0.165004</td>
      <td>0.001216</td>
      <td>0.141503</td>
      <td>0.000462</td>
    </tr>
    <tr>
      <th>1280</th>
      <td>0.164982</td>
      <td>0.001220</td>
      <td>0.141469</td>
      <td>0.000469</td>
    </tr>
  </tbody>
</table>
<p>1281 rows × 4 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Now try using cv rounds instead of test-early-stopping rounds (1280 instead of 1686)</span>

<span class="n">skl_boost</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span>
    <span class="n">missing</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> 
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">1280</span><span class="p">,</span> 
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> 
    <span class="n">silent</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>        
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> 
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">gamma</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
    <span class="n">subsample</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> 
    <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>            
    <span class="n">nthread</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">seed</span><span class="o">=</span><span class="mi">2016</span><span class="p">,</span> 
    <span class="n">objective</span><span class="o">=</span><span class="s1">&#39;binary:logistic&#39;</span>
<span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Get CV error.</span>

<span class="n">get_repeated_k_fold_score</span><span class="p">(</span><span class="n">skl_boost</span><span class="p">,</span> <span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">,</span> <span class="n">folds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                              <span class="n">agg_inner</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">agg_outer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">2002</span><span class="p">,</span>
                              <span class="n">score_func</span><span class="o">=</span><span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">fit_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                              <span class="n">print_description</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">score_classif_on_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get classifier accuracy on the test set in terms of accuracy of its</span>
<span class="sd">      probabilities (AUC and log-loss) and predictions (precision, recall, and F1).</span>
<span class="sd">    </span>
<span class="sd">    :model: sklearn model (i.e. with .fit() and .predict_proba() methods)</span>
<span class="sd">    :param x_test: np array of x-values for test set</span>
<span class="sd">    :param y_test: np array of y-values for test set</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;AUC Score:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">probs</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;LogLoss:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">log_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">probs</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

    <span class="n">Y_pred_enc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">Y_pred_enc</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">probs</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Score on test set.</span>
<span class="n">skl_boost</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">)</span>
<span class="n">score_classif_on_test</span><span class="p">(</span><span class="n">skl_boost</span><span class="p">,</span> <span class="n">X_test_enc</span><span class="p">,</span> <span class="n">Y_test_enc</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>AUC Score:
0.840451837071

LogLoss:
0.383405132526

             precision    recall  f1-score   support

      False      0.834     0.979     0.901     25951
       True      0.848     0.370     0.515      8023

avg / total      0.837     0.836     0.810     33974

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># XGBoost provides a convenient function to show feature importances.</span>
<span class="c1"># It&#39;s even more useful than with RF, since gradient-boosted trees tend to </span>
<span class="c1">#   &quot;pick&quot; between highly correlated features, choosing just one feature to represent the trend.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2"> Feature importances: &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xgb</span><span class="o">.</span><span class="n">plot_importance</span><span class="p">(</span><span class="n">skl_boost</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So we got an extra 1%+ accuracy from xgboost vs. rf (.823 vs .836). We lost some recall for the true cases.</p>
<h3 id="Tweaking-the-decision-threshold">Tweaking the decision threshold<a class="anchor-link" href="#Tweaking-the-decision-threshold">&#182;</a></h3><p>We wanted to be able to correctly recall instances of diabetes - so here we'll tweak the decision threshold up to trade precision for recall (and, specifically, recall for the positive cases).</p>
<p>We'll graph AUC to get a better idea of the trade-offs involved.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[43]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#from @motuai10</span>
<span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">predicted_probs</span><span class="p">)</span>
<span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Receiver Operating Characteristic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span>
<span class="n">label</span><span class="o">=</span><span class="s1">&#39;AUC = </span><span class="si">%0.2f</span><span class="s1">&#39;</span><span class="o">%</span> <span class="n">roc_auc</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;r--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>


<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAhcAAAGJCAYAAAA5XRHmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[46]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Y_pred_30</span> <span class="o">=</span> <span class="n">predicted_probs</span> <span class="o">&gt;=</span> <span class="o">.</span><span class="mi">30</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">Y_pred_30</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>             precision    recall  f1-score   support

      False      0.881     0.864     0.872     25968
       True      0.584     0.621     0.602      8006

avg / total      0.811     0.806     0.808     33974

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now recall 62% of true positives - much better than 37%!</p>
<p>Only 58% of our positives actually have diabetes, but that seems like an acceptable trade-off from a medical standpoint; it's OK to spend a little extra time checking on these people if they more likely than not have diabetes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Bonus-2---LightGBM">Bonus 2 - LightGBM<a class="anchor-link" href="#Bonus-2---LightGBM">&#182;</a></h3><p>I originally wrote this notebook in Summer 2016, and since then, LightGBM has risen to prominence as a faster and often more accurate alternative to XGBoost. By using binning, it can typically run 5-15x faster, and XGBoost is already quite fast.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Start by using CV to get rounds for a quicker-training model.</span>
<span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>

<span class="n">lgb_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s1">&#39;task&#39;</span><span class="p">:</span> <span class="s1">&#39;train&#39;</span><span class="p">,</span>
    <span class="s1">&#39;boosting_type&#39;</span><span class="p">:</span> <span class="s1">&#39;gbdt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;objective&#39;</span><span class="p">:</span> <span class="s1">&#39;binary&#39;</span><span class="p">,</span>
    <span class="s1">&#39;metric&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;rmse&#39;</span><span class="p">,</span> <span class="s1">&#39;auc&#39;</span><span class="p">],</span>
    <span class="s1">&#39;num_leaves&#39;</span><span class="p">:</span> <span class="mi">31</span><span class="p">,</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;max_bin&#39;</span><span class="p">:</span><span class="mi">255</span><span class="p">,</span>
    <span class="s1">&#39;min_split_gain&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;min_child_weight&#39;</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span>
    <span class="s1">&#39;min_child_samples&#39;</span><span class="p">:</span><span class="mi">30</span><span class="p">,</span>
    <span class="s1">&#39;subsample&#39;</span><span class="p">:</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;subsample_freq&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;reg_alpha&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;reg_lambda&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;seed&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;nthread&#39;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;verbose&#39;</span><span class="p">:</span> <span class="mi">20</span>
<span class="p">}</span>

<span class="c1"># Set up training dataset as lgb.Dataset object. </span>
<span class="c1"># Using LGB&#39;s built in cv() method is faster than </span>
<span class="c1">#   relying on SKLearn wrapper (e.g. because it parallelizes better), </span>
<span class="c1">#   but we&#39;ll have to fall back to SKLearn for some tuning. </span>

<span class="n">lgb_train</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># CV for lgbm</span>
<span class="c1"># Using log vals to simulate eval metric</span>
<span class="n">cv_result</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
<span class="n">lgb_params</span><span class="p">,</span> 
<span class="n">lgb_train</span><span class="p">,</span> 
<span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">num_boost_round</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> 
<span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
<span class="n">verbose_eval</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
<span class="n">show_stdv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="n">seed</span><span class="o">=</span><span class="mi">2001</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Num rds from 10-fold CV, lrate =.01 == </span><span class="si">%i</span><span class="s1">.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv_result</span><span class="p">[</span><span class="s1">&#39;auc-mean&#39;</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[1]	cv_agg&#39;s auc: 0.706636	cv_agg&#39;s rmse: 0.482602
[2]	cv_agg&#39;s auc: 0.721756	cv_agg&#39;s rmse: 0.467919
[3]	cv_agg&#39;s auc: 0.724896	cv_agg&#39;s rmse: 0.456002
[4]	cv_agg&#39;s auc: 0.733995	cv_agg&#39;s rmse: 0.44575
[5]	cv_agg&#39;s auc: 0.73887	cv_agg&#39;s rmse: 0.436724
[6]	cv_agg&#39;s auc: 0.741884	cv_agg&#39;s rmse: 0.429615
[7]	cv_agg&#39;s auc: 0.744562	cv_agg&#39;s rmse: 0.423175
[8]	cv_agg&#39;s auc: 0.74603	cv_agg&#39;s rmse: 0.417762
[9]	cv_agg&#39;s auc: 0.747966	cv_agg&#39;s rmse: 0.413048
[10]	cv_agg&#39;s auc: 0.749728	cv_agg&#39;s rmse: 0.408965
[11]	cv_agg&#39;s auc: 0.750832	cv_agg&#39;s rmse: 0.405894
[12]	cv_agg&#39;s auc: 0.75228	cv_agg&#39;s rmse: 0.403231
[13]	cv_agg&#39;s auc: 0.753666	cv_agg&#39;s rmse: 0.400536
[14]	cv_agg&#39;s auc: 0.753817	cv_agg&#39;s rmse: 0.399093
[15]	cv_agg&#39;s auc: 0.755498	cv_agg&#39;s rmse: 0.396625
[16]	cv_agg&#39;s auc: 0.756704	cv_agg&#39;s rmse: 0.394863
[17]	cv_agg&#39;s auc: 0.757842	cv_agg&#39;s rmse: 0.393304
[18]	cv_agg&#39;s auc: 0.758938	cv_agg&#39;s rmse: 0.392189
[19]	cv_agg&#39;s auc: 0.760357	cv_agg&#39;s rmse: 0.390692
[20]	cv_agg&#39;s auc: 0.761433	cv_agg&#39;s rmse: 0.389428
[21]	cv_agg&#39;s auc: 0.762516	cv_agg&#39;s rmse: 0.388562
[22]	cv_agg&#39;s auc: 0.763137	cv_agg&#39;s rmse: 0.387735
[23]	cv_agg&#39;s auc: 0.763945	cv_agg&#39;s rmse: 0.387016
[24]	cv_agg&#39;s auc: 0.76466	cv_agg&#39;s rmse: 0.386242
[25]	cv_agg&#39;s auc: 0.765805	cv_agg&#39;s rmse: 0.385406
[26]	cv_agg&#39;s auc: 0.766803	cv_agg&#39;s rmse: 0.384787
[27]	cv_agg&#39;s auc: 0.767782	cv_agg&#39;s rmse: 0.384108
[28]	cv_agg&#39;s auc: 0.768349	cv_agg&#39;s rmse: 0.38362
[29]	cv_agg&#39;s auc: 0.769075	cv_agg&#39;s rmse: 0.383164
[30]	cv_agg&#39;s auc: 0.769637	cv_agg&#39;s rmse: 0.382819
[31]	cv_agg&#39;s auc: 0.770174	cv_agg&#39;s rmse: 0.382412
[32]	cv_agg&#39;s auc: 0.770672	cv_agg&#39;s rmse: 0.382141
[33]	cv_agg&#39;s auc: 0.771168	cv_agg&#39;s rmse: 0.38191
[34]	cv_agg&#39;s auc: 0.771734	cv_agg&#39;s rmse: 0.381418
[35]	cv_agg&#39;s auc: 0.772372	cv_agg&#39;s rmse: 0.380968
[36]	cv_agg&#39;s auc: 0.772933	cv_agg&#39;s rmse: 0.380598
[37]	cv_agg&#39;s auc: 0.773702	cv_agg&#39;s rmse: 0.380234
[38]	cv_agg&#39;s auc: 0.774197	cv_agg&#39;s rmse: 0.379966
[39]	cv_agg&#39;s auc: 0.775126	cv_agg&#39;s rmse: 0.379436
[40]	cv_agg&#39;s auc: 0.775898	cv_agg&#39;s rmse: 0.379144
[41]	cv_agg&#39;s auc: 0.776404	cv_agg&#39;s rmse: 0.378861
[42]	cv_agg&#39;s auc: 0.777303	cv_agg&#39;s rmse: 0.378297
[43]	cv_agg&#39;s auc: 0.777901	cv_agg&#39;s rmse: 0.377999
[44]	cv_agg&#39;s auc: 0.778485	cv_agg&#39;s rmse: 0.377758
[45]	cv_agg&#39;s auc: 0.778972	cv_agg&#39;s rmse: 0.377476
[46]	cv_agg&#39;s auc: 0.779533	cv_agg&#39;s rmse: 0.377149
[47]	cv_agg&#39;s auc: 0.780111	cv_agg&#39;s rmse: 0.376855
[48]	cv_agg&#39;s auc: 0.780607	cv_agg&#39;s rmse: 0.376658
[49]	cv_agg&#39;s auc: 0.781129	cv_agg&#39;s rmse: 0.37643
[50]	cv_agg&#39;s auc: 0.781708	cv_agg&#39;s rmse: 0.37618
[51]	cv_agg&#39;s auc: 0.782204	cv_agg&#39;s rmse: 0.376029
[52]	cv_agg&#39;s auc: 0.782792	cv_agg&#39;s rmse: 0.375808
[53]	cv_agg&#39;s auc: 0.783279	cv_agg&#39;s rmse: 0.375592
[54]	cv_agg&#39;s auc: 0.783579	cv_agg&#39;s rmse: 0.375459
[55]	cv_agg&#39;s auc: 0.783895	cv_agg&#39;s rmse: 0.375272
[56]	cv_agg&#39;s auc: 0.784343	cv_agg&#39;s rmse: 0.375031
[57]	cv_agg&#39;s auc: 0.784657	cv_agg&#39;s rmse: 0.3749
[58]	cv_agg&#39;s auc: 0.785066	cv_agg&#39;s rmse: 0.374682
[59]	cv_agg&#39;s auc: 0.785374	cv_agg&#39;s rmse: 0.374529
[60]	cv_agg&#39;s auc: 0.785548	cv_agg&#39;s rmse: 0.374442
[61]	cv_agg&#39;s auc: 0.785933	cv_agg&#39;s rmse: 0.374271
[62]	cv_agg&#39;s auc: 0.786293	cv_agg&#39;s rmse: 0.374138
[63]	cv_agg&#39;s auc: 0.786656	cv_agg&#39;s rmse: 0.373969
[64]	cv_agg&#39;s auc: 0.787033	cv_agg&#39;s rmse: 0.373728
[65]	cv_agg&#39;s auc: 0.787383	cv_agg&#39;s rmse: 0.373597
[66]	cv_agg&#39;s auc: 0.787515	cv_agg&#39;s rmse: 0.373538
[67]	cv_agg&#39;s auc: 0.78791	cv_agg&#39;s rmse: 0.37333
[68]	cv_agg&#39;s auc: 0.788462	cv_agg&#39;s rmse: 0.37314
[69]	cv_agg&#39;s auc: 0.788757	cv_agg&#39;s rmse: 0.37296
[70]	cv_agg&#39;s auc: 0.78905	cv_agg&#39;s rmse: 0.372851
[71]	cv_agg&#39;s auc: 0.789403	cv_agg&#39;s rmse: 0.372699
[72]	cv_agg&#39;s auc: 0.789711	cv_agg&#39;s rmse: 0.372573
[73]	cv_agg&#39;s auc: 0.789965	cv_agg&#39;s rmse: 0.372417
[74]	cv_agg&#39;s auc: 0.790219	cv_agg&#39;s rmse: 0.372296
[75]	cv_agg&#39;s auc: 0.790561	cv_agg&#39;s rmse: 0.372116
[76]	cv_agg&#39;s auc: 0.790792	cv_agg&#39;s rmse: 0.372034
[77]	cv_agg&#39;s auc: 0.791113	cv_agg&#39;s rmse: 0.371909
[78]	cv_agg&#39;s auc: 0.791382	cv_agg&#39;s rmse: 0.371779
[79]	cv_agg&#39;s auc: 0.791567	cv_agg&#39;s rmse: 0.371675
[80]	cv_agg&#39;s auc: 0.791878	cv_agg&#39;s rmse: 0.371547
[81]	cv_agg&#39;s auc: 0.792025	cv_agg&#39;s rmse: 0.371487
[82]	cv_agg&#39;s auc: 0.792237	cv_agg&#39;s rmse: 0.37141
[83]	cv_agg&#39;s auc: 0.792407	cv_agg&#39;s rmse: 0.371319
[84]	cv_agg&#39;s auc: 0.792549	cv_agg&#39;s rmse: 0.371167
[85]	cv_agg&#39;s auc: 0.792841	cv_agg&#39;s rmse: 0.371054
[86]	cv_agg&#39;s auc: 0.793078	cv_agg&#39;s rmse: 0.370964
[87]	cv_agg&#39;s auc: 0.793312	cv_agg&#39;s rmse: 0.370869
[88]	cv_agg&#39;s auc: 0.793586	cv_agg&#39;s rmse: 0.370717
[89]	cv_agg&#39;s auc: 0.793801	cv_agg&#39;s rmse: 0.370613
[90]	cv_agg&#39;s auc: 0.794019	cv_agg&#39;s rmse: 0.370527
[91]	cv_agg&#39;s auc: 0.794342	cv_agg&#39;s rmse: 0.370419
[92]	cv_agg&#39;s auc: 0.794566	cv_agg&#39;s rmse: 0.370317
[93]	cv_agg&#39;s auc: 0.794892	cv_agg&#39;s rmse: 0.370204
[94]	cv_agg&#39;s auc: 0.795034	cv_agg&#39;s rmse: 0.370132
[95]	cv_agg&#39;s auc: 0.795164	cv_agg&#39;s rmse: 0.370058
[96]	cv_agg&#39;s auc: 0.795209	cv_agg&#39;s rmse: 0.370032
[97]	cv_agg&#39;s auc: 0.795339	cv_agg&#39;s rmse: 0.369967
[98]	cv_agg&#39;s auc: 0.795582	cv_agg&#39;s rmse: 0.369867
[99]	cv_agg&#39;s auc: 0.79573	cv_agg&#39;s rmse: 0.369765
[100]	cv_agg&#39;s auc: 0.795907	cv_agg&#39;s rmse: 0.369703
[101]	cv_agg&#39;s auc: 0.796069	cv_agg&#39;s rmse: 0.369612
[102]	cv_agg&#39;s auc: 0.796212	cv_agg&#39;s rmse: 0.369532
[103]	cv_agg&#39;s auc: 0.796255	cv_agg&#39;s rmse: 0.369498
[104]	cv_agg&#39;s auc: 0.796389	cv_agg&#39;s rmse: 0.369409
[105]	cv_agg&#39;s auc: 0.796561	cv_agg&#39;s rmse: 0.369331
[106]	cv_agg&#39;s auc: 0.796787	cv_agg&#39;s rmse: 0.369239
[107]	cv_agg&#39;s auc: 0.796815	cv_agg&#39;s rmse: 0.369222
[108]	cv_agg&#39;s auc: 0.797003	cv_agg&#39;s rmse: 0.369149
[109]	cv_agg&#39;s auc: 0.7973	cv_agg&#39;s rmse: 0.36905
[110]	cv_agg&#39;s auc: 0.797453	cv_agg&#39;s rmse: 0.368987
[111]	cv_agg&#39;s auc: 0.797695	cv_agg&#39;s rmse: 0.368898
[112]	cv_agg&#39;s auc: 0.797785	cv_agg&#39;s rmse: 0.368844
[113]	cv_agg&#39;s auc: 0.797859	cv_agg&#39;s rmse: 0.368807
[114]	cv_agg&#39;s auc: 0.798058	cv_agg&#39;s rmse: 0.368718
[115]	cv_agg&#39;s auc: 0.798193	cv_agg&#39;s rmse: 0.368645
[116]	cv_agg&#39;s auc: 0.798229	cv_agg&#39;s rmse: 0.368601
[117]	cv_agg&#39;s auc: 0.798295	cv_agg&#39;s rmse: 0.368546
[118]	cv_agg&#39;s auc: 0.798386	cv_agg&#39;s rmse: 0.3685
[119]	cv_agg&#39;s auc: 0.798514	cv_agg&#39;s rmse: 0.368455
[120]	cv_agg&#39;s auc: 0.798611	cv_agg&#39;s rmse: 0.368399
[121]	cv_agg&#39;s auc: 0.798745	cv_agg&#39;s rmse: 0.368344
[122]	cv_agg&#39;s auc: 0.798972	cv_agg&#39;s rmse: 0.368255
[123]	cv_agg&#39;s auc: 0.799037	cv_agg&#39;s rmse: 0.368224
[124]	cv_agg&#39;s auc: 0.799182	cv_agg&#39;s rmse: 0.368156
[125]	cv_agg&#39;s auc: 0.799326	cv_agg&#39;s rmse: 0.368084
[126]	cv_agg&#39;s auc: 0.799437	cv_agg&#39;s rmse: 0.368017
[127]	cv_agg&#39;s auc: 0.799571	cv_agg&#39;s rmse: 0.367965
[128]	cv_agg&#39;s auc: 0.79963	cv_agg&#39;s rmse: 0.367912
[129]	cv_agg&#39;s auc: 0.799787	cv_agg&#39;s rmse: 0.367844
[130]	cv_agg&#39;s auc: 0.799936	cv_agg&#39;s rmse: 0.367787
[131]	cv_agg&#39;s auc: 0.800075	cv_agg&#39;s rmse: 0.36773
[132]	cv_agg&#39;s auc: 0.800274	cv_agg&#39;s rmse: 0.367666
[133]	cv_agg&#39;s auc: 0.800474	cv_agg&#39;s rmse: 0.367599
[134]	cv_agg&#39;s auc: 0.800573	cv_agg&#39;s rmse: 0.367543
[135]	cv_agg&#39;s auc: 0.800722	cv_agg&#39;s rmse: 0.367473
[136]	cv_agg&#39;s auc: 0.800779	cv_agg&#39;s rmse: 0.367451
[137]	cv_agg&#39;s auc: 0.800903	cv_agg&#39;s rmse: 0.367406
[138]	cv_agg&#39;s auc: 0.801071	cv_agg&#39;s rmse: 0.367348
[139]	cv_agg&#39;s auc: 0.80133	cv_agg&#39;s rmse: 0.36725
[140]	cv_agg&#39;s auc: 0.80139	cv_agg&#39;s rmse: 0.36722
[141]	cv_agg&#39;s auc: 0.801524	cv_agg&#39;s rmse: 0.367144
[142]	cv_agg&#39;s auc: 0.801612	cv_agg&#39;s rmse: 0.367094
[143]	cv_agg&#39;s auc: 0.801717	cv_agg&#39;s rmse: 0.367043
[144]	cv_agg&#39;s auc: 0.801771	cv_agg&#39;s rmse: 0.367008
[145]	cv_agg&#39;s auc: 0.801803	cv_agg&#39;s rmse: 0.366991
[146]	cv_agg&#39;s auc: 0.801889	cv_agg&#39;s rmse: 0.366947
[147]	cv_agg&#39;s auc: 0.802005	cv_agg&#39;s rmse: 0.366917
[148]	cv_agg&#39;s auc: 0.802328	cv_agg&#39;s rmse: 0.366817
[149]	cv_agg&#39;s auc: 0.802525	cv_agg&#39;s rmse: 0.36675
[150]	cv_agg&#39;s auc: 0.802634	cv_agg&#39;s rmse: 0.366708
[151]	cv_agg&#39;s auc: 0.802717	cv_agg&#39;s rmse: 0.366681
[152]	cv_agg&#39;s auc: 0.802796	cv_agg&#39;s rmse: 0.366646
[153]	cv_agg&#39;s auc: 0.802924	cv_agg&#39;s rmse: 0.366607
[154]	cv_agg&#39;s auc: 0.803044	cv_agg&#39;s rmse: 0.366562
[155]	cv_agg&#39;s auc: 0.803051	cv_agg&#39;s rmse: 0.366546
[156]	cv_agg&#39;s auc: 0.803145	cv_agg&#39;s rmse: 0.366496
[157]	cv_agg&#39;s auc: 0.803228	cv_agg&#39;s rmse: 0.366457
[158]	cv_agg&#39;s auc: 0.803319	cv_agg&#39;s rmse: 0.366415
[159]	cv_agg&#39;s auc: 0.803387	cv_agg&#39;s rmse: 0.366397
[160]	cv_agg&#39;s auc: 0.803493	cv_agg&#39;s rmse: 0.366354
[161]	cv_agg&#39;s auc: 0.803513	cv_agg&#39;s rmse: 0.366341
[162]	cv_agg&#39;s auc: 0.80356	cv_agg&#39;s rmse: 0.366317
[163]	cv_agg&#39;s auc: 0.803674	cv_agg&#39;s rmse: 0.366276
[164]	cv_agg&#39;s auc: 0.803894	cv_agg&#39;s rmse: 0.366207
[165]	cv_agg&#39;s auc: 0.803944	cv_agg&#39;s rmse: 0.366172
[166]	cv_agg&#39;s auc: 0.803972	cv_agg&#39;s rmse: 0.366146
[167]	cv_agg&#39;s auc: 0.804037	cv_agg&#39;s rmse: 0.366124
[168]	cv_agg&#39;s auc: 0.804087	cv_agg&#39;s rmse: 0.3661
[169]	cv_agg&#39;s auc: 0.804179	cv_agg&#39;s rmse: 0.36606
[170]	cv_agg&#39;s auc: 0.804234	cv_agg&#39;s rmse: 0.366039
[171]	cv_agg&#39;s auc: 0.804386	cv_agg&#39;s rmse: 0.365986
[172]	cv_agg&#39;s auc: 0.804468	cv_agg&#39;s rmse: 0.365949
[173]	cv_agg&#39;s auc: 0.804656	cv_agg&#39;s rmse: 0.365894
[174]	cv_agg&#39;s auc: 0.804814	cv_agg&#39;s rmse: 0.365829
[175]	cv_agg&#39;s auc: 0.804877	cv_agg&#39;s rmse: 0.365797
[176]	cv_agg&#39;s auc: 0.804953	cv_agg&#39;s rmse: 0.365769
[177]	cv_agg&#39;s auc: 0.805221	cv_agg&#39;s rmse: 0.36567
[178]	cv_agg&#39;s auc: 0.805367	cv_agg&#39;s rmse: 0.365613
[179]	cv_agg&#39;s auc: 0.805386	cv_agg&#39;s rmse: 0.365593
[180]	cv_agg&#39;s auc: 0.805466	cv_agg&#39;s rmse: 0.36556
[181]	cv_agg&#39;s auc: 0.805521	cv_agg&#39;s rmse: 0.365521
[182]	cv_agg&#39;s auc: 0.805571	cv_agg&#39;s rmse: 0.365494
[183]	cv_agg&#39;s auc: 0.805692	cv_agg&#39;s rmse: 0.365429
[184]	cv_agg&#39;s auc: 0.805756	cv_agg&#39;s rmse: 0.365401
[185]	cv_agg&#39;s auc: 0.805851	cv_agg&#39;s rmse: 0.365357
[186]	cv_agg&#39;s auc: 0.805887	cv_agg&#39;s rmse: 0.365332
[187]	cv_agg&#39;s auc: 0.806095	cv_agg&#39;s rmse: 0.365273
[188]	cv_agg&#39;s auc: 0.806354	cv_agg&#39;s rmse: 0.365189
[189]	cv_agg&#39;s auc: 0.806401	cv_agg&#39;s rmse: 0.365165
[190]	cv_agg&#39;s auc: 0.80645	cv_agg&#39;s rmse: 0.365147
[191]	cv_agg&#39;s auc: 0.80651	cv_agg&#39;s rmse: 0.365123
[192]	cv_agg&#39;s auc: 0.806501	cv_agg&#39;s rmse: 0.365111
[193]	cv_agg&#39;s auc: 0.806502	cv_agg&#39;s rmse: 0.365105
[194]	cv_agg&#39;s auc: 0.806559	cv_agg&#39;s rmse: 0.365072
[195]	cv_agg&#39;s auc: 0.806663	cv_agg&#39;s rmse: 0.365028
[196]	cv_agg&#39;s auc: 0.806724	cv_agg&#39;s rmse: 0.364995
[197]	cv_agg&#39;s auc: 0.806854	cv_agg&#39;s rmse: 0.364952
[198]	cv_agg&#39;s auc: 0.806888	cv_agg&#39;s rmse: 0.364938
[199]	cv_agg&#39;s auc: 0.807022	cv_agg&#39;s rmse: 0.364894
[200]	cv_agg&#39;s auc: 0.80707	cv_agg&#39;s rmse: 0.364862
[201]	cv_agg&#39;s auc: 0.807148	cv_agg&#39;s rmse: 0.364812
[202]	cv_agg&#39;s auc: 0.807342	cv_agg&#39;s rmse: 0.364758
[203]	cv_agg&#39;s auc: 0.807361	cv_agg&#39;s rmse: 0.36474
[204]	cv_agg&#39;s auc: 0.8074	cv_agg&#39;s rmse: 0.364718
[205]	cv_agg&#39;s auc: 0.807459	cv_agg&#39;s rmse: 0.364687
[206]	cv_agg&#39;s auc: 0.807476	cv_agg&#39;s rmse: 0.36468
[207]	cv_agg&#39;s auc: 0.807624	cv_agg&#39;s rmse: 0.364627
[208]	cv_agg&#39;s auc: 0.807794	cv_agg&#39;s rmse: 0.364574
[209]	cv_agg&#39;s auc: 0.807831	cv_agg&#39;s rmse: 0.364552
[210]	cv_agg&#39;s auc: 0.807896	cv_agg&#39;s rmse: 0.36452
[211]	cv_agg&#39;s auc: 0.807955	cv_agg&#39;s rmse: 0.364498
[212]	cv_agg&#39;s auc: 0.808007	cv_agg&#39;s rmse: 0.364479
[213]	cv_agg&#39;s auc: 0.808072	cv_agg&#39;s rmse: 0.364449
[214]	cv_agg&#39;s auc: 0.808164	cv_agg&#39;s rmse: 0.364411
[215]	cv_agg&#39;s auc: 0.808247	cv_agg&#39;s rmse: 0.36438
[216]	cv_agg&#39;s auc: 0.808306	cv_agg&#39;s rmse: 0.364355
[217]	cv_agg&#39;s auc: 0.808333	cv_agg&#39;s rmse: 0.364342
[218]	cv_agg&#39;s auc: 0.808319	cv_agg&#39;s rmse: 0.36434
[219]	cv_agg&#39;s auc: 0.808417	cv_agg&#39;s rmse: 0.364282
[220]	cv_agg&#39;s auc: 0.808439	cv_agg&#39;s rmse: 0.36427
[221]	cv_agg&#39;s auc: 0.80848	cv_agg&#39;s rmse: 0.364245
[222]	cv_agg&#39;s auc: 0.808553	cv_agg&#39;s rmse: 0.36422
[223]	cv_agg&#39;s auc: 0.808649	cv_agg&#39;s rmse: 0.364168
[224]	cv_agg&#39;s auc: 0.808698	cv_agg&#39;s rmse: 0.36415
[225]	cv_agg&#39;s auc: 0.808813	cv_agg&#39;s rmse: 0.364102
[226]	cv_agg&#39;s auc: 0.808831	cv_agg&#39;s rmse: 0.364088
[227]	cv_agg&#39;s auc: 0.808934	cv_agg&#39;s rmse: 0.364053
[228]	cv_agg&#39;s auc: 0.809017	cv_agg&#39;s rmse: 0.364021
[229]	cv_agg&#39;s auc: 0.809086	cv_agg&#39;s rmse: 0.363994
[230]	cv_agg&#39;s auc: 0.809114	cv_agg&#39;s rmse: 0.363978
[231]	cv_agg&#39;s auc: 0.809183	cv_agg&#39;s rmse: 0.363953
[232]	cv_agg&#39;s auc: 0.809195	cv_agg&#39;s rmse: 0.363931
[233]	cv_agg&#39;s auc: 0.809227	cv_agg&#39;s rmse: 0.363914
[234]	cv_agg&#39;s auc: 0.809272	cv_agg&#39;s rmse: 0.363889
[235]	cv_agg&#39;s auc: 0.80938	cv_agg&#39;s rmse: 0.363848
[236]	cv_agg&#39;s auc: 0.809487	cv_agg&#39;s rmse: 0.363806
[237]	cv_agg&#39;s auc: 0.80949	cv_agg&#39;s rmse: 0.363797
[238]	cv_agg&#39;s auc: 0.809555	cv_agg&#39;s rmse: 0.363761
[239]	cv_agg&#39;s auc: 0.809594	cv_agg&#39;s rmse: 0.363744
[240]	cv_agg&#39;s auc: 0.809595	cv_agg&#39;s rmse: 0.363733
[241]	cv_agg&#39;s auc: 0.809631	cv_agg&#39;s rmse: 0.363717
[242]	cv_agg&#39;s auc: 0.809694	cv_agg&#39;s rmse: 0.363677
[243]	cv_agg&#39;s auc: 0.809739	cv_agg&#39;s rmse: 0.363655
[244]	cv_agg&#39;s auc: 0.809772	cv_agg&#39;s rmse: 0.363642
[245]	cv_agg&#39;s auc: 0.809814	cv_agg&#39;s rmse: 0.36362
[246]	cv_agg&#39;s auc: 0.809902	cv_agg&#39;s rmse: 0.363591
[247]	cv_agg&#39;s auc: 0.809923	cv_agg&#39;s rmse: 0.363576
[248]	cv_agg&#39;s auc: 0.809979	cv_agg&#39;s rmse: 0.363553
[249]	cv_agg&#39;s auc: 0.810002	cv_agg&#39;s rmse: 0.363543
[250]	cv_agg&#39;s auc: 0.810007	cv_agg&#39;s rmse: 0.363536
[251]	cv_agg&#39;s auc: 0.810044	cv_agg&#39;s rmse: 0.363525
[252]	cv_agg&#39;s auc: 0.810049	cv_agg&#39;s rmse: 0.363517
[253]	cv_agg&#39;s auc: 0.810077	cv_agg&#39;s rmse: 0.363499
[254]	cv_agg&#39;s auc: 0.810144	cv_agg&#39;s rmse: 0.363467
[255]	cv_agg&#39;s auc: 0.810201	cv_agg&#39;s rmse: 0.363445
[256]	cv_agg&#39;s auc: 0.810369	cv_agg&#39;s rmse: 0.363379
[257]	cv_agg&#39;s auc: 0.810402	cv_agg&#39;s rmse: 0.363362
[258]	cv_agg&#39;s auc: 0.810411	cv_agg&#39;s rmse: 0.363348
[259]	cv_agg&#39;s auc: 0.81056	cv_agg&#39;s rmse: 0.363286
[260]	cv_agg&#39;s auc: 0.810681	cv_agg&#39;s rmse: 0.363239
[261]	cv_agg&#39;s auc: 0.810794	cv_agg&#39;s rmse: 0.36319
[262]	cv_agg&#39;s auc: 0.810908	cv_agg&#39;s rmse: 0.363147
[263]	cv_agg&#39;s auc: 0.810959	cv_agg&#39;s rmse: 0.363122
[264]	cv_agg&#39;s auc: 0.81105	cv_agg&#39;s rmse: 0.363091
[265]	cv_agg&#39;s auc: 0.811152	cv_agg&#39;s rmse: 0.363047
[266]	cv_agg&#39;s auc: 0.8112	cv_agg&#39;s rmse: 0.363022
[267]	cv_agg&#39;s auc: 0.811218	cv_agg&#39;s rmse: 0.363005
[268]	cv_agg&#39;s auc: 0.811263	cv_agg&#39;s rmse: 0.362983
[269]	cv_agg&#39;s auc: 0.811328	cv_agg&#39;s rmse: 0.36296
[270]	cv_agg&#39;s auc: 0.811507	cv_agg&#39;s rmse: 0.362879
[271]	cv_agg&#39;s auc: 0.8117	cv_agg&#39;s rmse: 0.3628
[272]	cv_agg&#39;s auc: 0.811727	cv_agg&#39;s rmse: 0.36279
[273]	cv_agg&#39;s auc: 0.811817	cv_agg&#39;s rmse: 0.362745
[274]	cv_agg&#39;s auc: 0.811864	cv_agg&#39;s rmse: 0.362724
[275]	cv_agg&#39;s auc: 0.811917	cv_agg&#39;s rmse: 0.362703
[276]	cv_agg&#39;s auc: 0.811983	cv_agg&#39;s rmse: 0.362674
[277]	cv_agg&#39;s auc: 0.812034	cv_agg&#39;s rmse: 0.362655
[278]	cv_agg&#39;s auc: 0.812093	cv_agg&#39;s rmse: 0.362628
[279]	cv_agg&#39;s auc: 0.812105	cv_agg&#39;s rmse: 0.362615
[280]	cv_agg&#39;s auc: 0.81216	cv_agg&#39;s rmse: 0.362589
[281]	cv_agg&#39;s auc: 0.812217	cv_agg&#39;s rmse: 0.362568
[282]	cv_agg&#39;s auc: 0.812198	cv_agg&#39;s rmse: 0.362567
[283]	cv_agg&#39;s auc: 0.812226	cv_agg&#39;s rmse: 0.362553
[284]	cv_agg&#39;s auc: 0.812212	cv_agg&#39;s rmse: 0.362554
[285]	cv_agg&#39;s auc: 0.812246	cv_agg&#39;s rmse: 0.362527
[286]	cv_agg&#39;s auc: 0.812456	cv_agg&#39;s rmse: 0.362436
[287]	cv_agg&#39;s auc: 0.812466	cv_agg&#39;s rmse: 0.362433
[288]	cv_agg&#39;s auc: 0.8125	cv_agg&#39;s rmse: 0.362408
[289]	cv_agg&#39;s auc: 0.812552	cv_agg&#39;s rmse: 0.362383
[290]	cv_agg&#39;s auc: 0.812547	cv_agg&#39;s rmse: 0.362382
[291]	cv_agg&#39;s auc: 0.812576	cv_agg&#39;s rmse: 0.362367
[292]	cv_agg&#39;s auc: 0.812619	cv_agg&#39;s rmse: 0.362356
[293]	cv_agg&#39;s auc: 0.812621	cv_agg&#39;s rmse: 0.362354
[294]	cv_agg&#39;s auc: 0.812631	cv_agg&#39;s rmse: 0.362344
[295]	cv_agg&#39;s auc: 0.812725	cv_agg&#39;s rmse: 0.362311
[296]	cv_agg&#39;s auc: 0.812754	cv_agg&#39;s rmse: 0.362296
[297]	cv_agg&#39;s auc: 0.812819	cv_agg&#39;s rmse: 0.362268
[298]	cv_agg&#39;s auc: 0.812864	cv_agg&#39;s rmse: 0.362249
[299]	cv_agg&#39;s auc: 0.812883	cv_agg&#39;s rmse: 0.362231
[300]	cv_agg&#39;s auc: 0.812937	cv_agg&#39;s rmse: 0.362215
[301]	cv_agg&#39;s auc: 0.813003	cv_agg&#39;s rmse: 0.362191
[302]	cv_agg&#39;s auc: 0.813037	cv_agg&#39;s rmse: 0.362169
[303]	cv_agg&#39;s auc: 0.813104	cv_agg&#39;s rmse: 0.362135
[304]	cv_agg&#39;s auc: 0.813176	cv_agg&#39;s rmse: 0.362103
[305]	cv_agg&#39;s auc: 0.813194	cv_agg&#39;s rmse: 0.362089
[306]	cv_agg&#39;s auc: 0.813182	cv_agg&#39;s rmse: 0.362089
[307]	cv_agg&#39;s auc: 0.813225	cv_agg&#39;s rmse: 0.362065
[308]	cv_agg&#39;s auc: 0.813226	cv_agg&#39;s rmse: 0.362062
[309]	cv_agg&#39;s auc: 0.813252	cv_agg&#39;s rmse: 0.362048
[310]	cv_agg&#39;s auc: 0.813272	cv_agg&#39;s rmse: 0.362032
[311]	cv_agg&#39;s auc: 0.81328	cv_agg&#39;s rmse: 0.36203
[312]	cv_agg&#39;s auc: 0.813277	cv_agg&#39;s rmse: 0.362024
[313]	cv_agg&#39;s auc: 0.813336	cv_agg&#39;s rmse: 0.361982
[314]	cv_agg&#39;s auc: 0.813349	cv_agg&#39;s rmse: 0.361971
[315]	cv_agg&#39;s auc: 0.8134	cv_agg&#39;s rmse: 0.361947
[316]	cv_agg&#39;s auc: 0.813401	cv_agg&#39;s rmse: 0.361946
[317]	cv_agg&#39;s auc: 0.813413	cv_agg&#39;s rmse: 0.361937
[318]	cv_agg&#39;s auc: 0.813404	cv_agg&#39;s rmse: 0.361934
[319]	cv_agg&#39;s auc: 0.813487	cv_agg&#39;s rmse: 0.361912
[320]	cv_agg&#39;s auc: 0.813569	cv_agg&#39;s rmse: 0.361865
[321]	cv_agg&#39;s auc: 0.813651	cv_agg&#39;s rmse: 0.361831
[322]	cv_agg&#39;s auc: 0.81369	cv_agg&#39;s rmse: 0.361817
[323]	cv_agg&#39;s auc: 0.813776	cv_agg&#39;s rmse: 0.361783
[324]	cv_agg&#39;s auc: 0.81379	cv_agg&#39;s rmse: 0.361774
[325]	cv_agg&#39;s auc: 0.813827	cv_agg&#39;s rmse: 0.361753
[326]	cv_agg&#39;s auc: 0.813864	cv_agg&#39;s rmse: 0.361734
[327]	cv_agg&#39;s auc: 0.813916	cv_agg&#39;s rmse: 0.361711
[328]	cv_agg&#39;s auc: 0.813933	cv_agg&#39;s rmse: 0.361705
[329]	cv_agg&#39;s auc: 0.814016	cv_agg&#39;s rmse: 0.361679
[330]	cv_agg&#39;s auc: 0.814062	cv_agg&#39;s rmse: 0.361654
[331]	cv_agg&#39;s auc: 0.81419	cv_agg&#39;s rmse: 0.361597
[332]	cv_agg&#39;s auc: 0.814254	cv_agg&#39;s rmse: 0.361562
[333]	cv_agg&#39;s auc: 0.814307	cv_agg&#39;s rmse: 0.36154
[334]	cv_agg&#39;s auc: 0.814346	cv_agg&#39;s rmse: 0.361525
[335]	cv_agg&#39;s auc: 0.814363	cv_agg&#39;s rmse: 0.361511
[336]	cv_agg&#39;s auc: 0.814458	cv_agg&#39;s rmse: 0.361476
[337]	cv_agg&#39;s auc: 0.814511	cv_agg&#39;s rmse: 0.361459
[338]	cv_agg&#39;s auc: 0.814574	cv_agg&#39;s rmse: 0.361448
[339]	cv_agg&#39;s auc: 0.814575	cv_agg&#39;s rmse: 0.361444
[340]	cv_agg&#39;s auc: 0.81462	cv_agg&#39;s rmse: 0.361418
[341]	cv_agg&#39;s auc: 0.814667	cv_agg&#39;s rmse: 0.361396
[342]	cv_agg&#39;s auc: 0.814695	cv_agg&#39;s rmse: 0.361377
[343]	cv_agg&#39;s auc: 0.814722	cv_agg&#39;s rmse: 0.361354
[344]	cv_agg&#39;s auc: 0.814753	cv_agg&#39;s rmse: 0.361334
[345]	cv_agg&#39;s auc: 0.814753	cv_agg&#39;s rmse: 0.361332
[346]	cv_agg&#39;s auc: 0.814782	cv_agg&#39;s rmse: 0.361316
[347]	cv_agg&#39;s auc: 0.814788	cv_agg&#39;s rmse: 0.361299
[348]	cv_agg&#39;s auc: 0.814864	cv_agg&#39;s rmse: 0.361262
[349]	cv_agg&#39;s auc: 0.814855	cv_agg&#39;s rmse: 0.361261
[350]	cv_agg&#39;s auc: 0.814871	cv_agg&#39;s rmse: 0.361247
[351]	cv_agg&#39;s auc: 0.814874	cv_agg&#39;s rmse: 0.361246
[352]	cv_agg&#39;s auc: 0.814864	cv_agg&#39;s rmse: 0.361241
[353]	cv_agg&#39;s auc: 0.814872	cv_agg&#39;s rmse: 0.361224
[354]	cv_agg&#39;s auc: 0.814904	cv_agg&#39;s rmse: 0.361215
[355]	cv_agg&#39;s auc: 0.814963	cv_agg&#39;s rmse: 0.361191
[356]	cv_agg&#39;s auc: 0.814972	cv_agg&#39;s rmse: 0.361181
[357]	cv_agg&#39;s auc: 0.815001	cv_agg&#39;s rmse: 0.361169
[358]	cv_agg&#39;s auc: 0.815019	cv_agg&#39;s rmse: 0.361157
[359]	cv_agg&#39;s auc: 0.815123	cv_agg&#39;s rmse: 0.361118
[360]	cv_agg&#39;s auc: 0.815128	cv_agg&#39;s rmse: 0.361115
[361]	cv_agg&#39;s auc: 0.815149	cv_agg&#39;s rmse: 0.361108
[362]	cv_agg&#39;s auc: 0.815248	cv_agg&#39;s rmse: 0.361068
[363]	cv_agg&#39;s auc: 0.815244	cv_agg&#39;s rmse: 0.361069
[364]	cv_agg&#39;s auc: 0.815267	cv_agg&#39;s rmse: 0.361052
[365]	cv_agg&#39;s auc: 0.815304	cv_agg&#39;s rmse: 0.361034
[366]	cv_agg&#39;s auc: 0.815454	cv_agg&#39;s rmse: 0.360985
[367]	cv_agg&#39;s auc: 0.815473	cv_agg&#39;s rmse: 0.360969
[368]	cv_agg&#39;s auc: 0.815546	cv_agg&#39;s rmse: 0.360942
[369]	cv_agg&#39;s auc: 0.815564	cv_agg&#39;s rmse: 0.360933
[370]	cv_agg&#39;s auc: 0.815685	cv_agg&#39;s rmse: 0.360877
[371]	cv_agg&#39;s auc: 0.815784	cv_agg&#39;s rmse: 0.360829
[372]	cv_agg&#39;s auc: 0.815792	cv_agg&#39;s rmse: 0.360815
[373]	cv_agg&#39;s auc: 0.815803	cv_agg&#39;s rmse: 0.360812
[374]	cv_agg&#39;s auc: 0.815818	cv_agg&#39;s rmse: 0.360806
[375]	cv_agg&#39;s auc: 0.81585	cv_agg&#39;s rmse: 0.360797
[376]	cv_agg&#39;s auc: 0.815836	cv_agg&#39;s rmse: 0.360788
[377]	cv_agg&#39;s auc: 0.815829	cv_agg&#39;s rmse: 0.360785
[378]	cv_agg&#39;s auc: 0.815832	cv_agg&#39;s rmse: 0.360783
[379]	cv_agg&#39;s auc: 0.815874	cv_agg&#39;s rmse: 0.360756
[380]	cv_agg&#39;s auc: 0.815883	cv_agg&#39;s rmse: 0.360751
[381]	cv_agg&#39;s auc: 0.815889	cv_agg&#39;s rmse: 0.360739
[382]	cv_agg&#39;s auc: 0.815925	cv_agg&#39;s rmse: 0.360723
[383]	cv_agg&#39;s auc: 0.815939	cv_agg&#39;s rmse: 0.360715
[384]	cv_agg&#39;s auc: 0.815928	cv_agg&#39;s rmse: 0.360717
[385]	cv_agg&#39;s auc: 0.815967	cv_agg&#39;s rmse: 0.360701
[386]	cv_agg&#39;s auc: 0.815981	cv_agg&#39;s rmse: 0.360682
[387]	cv_agg&#39;s auc: 0.816005	cv_agg&#39;s rmse: 0.360666
[388]	cv_agg&#39;s auc: 0.81608	cv_agg&#39;s rmse: 0.360641
[389]	cv_agg&#39;s auc: 0.816134	cv_agg&#39;s rmse: 0.360624
[390]	cv_agg&#39;s auc: 0.816135	cv_agg&#39;s rmse: 0.36062
[391]	cv_agg&#39;s auc: 0.816195	cv_agg&#39;s rmse: 0.360592
[392]	cv_agg&#39;s auc: 0.816223	cv_agg&#39;s rmse: 0.360577
[393]	cv_agg&#39;s auc: 0.816245	cv_agg&#39;s rmse: 0.360567
[394]	cv_agg&#39;s auc: 0.816286	cv_agg&#39;s rmse: 0.360552
[395]	cv_agg&#39;s auc: 0.816286	cv_agg&#39;s rmse: 0.360555
[396]	cv_agg&#39;s auc: 0.816274	cv_agg&#39;s rmse: 0.360554
[397]	cv_agg&#39;s auc: 0.816314	cv_agg&#39;s rmse: 0.360544
[398]	cv_agg&#39;s auc: 0.816342	cv_agg&#39;s rmse: 0.360531
[399]	cv_agg&#39;s auc: 0.816535	cv_agg&#39;s rmse: 0.360473
[400]	cv_agg&#39;s auc: 0.816628	cv_agg&#39;s rmse: 0.360438
[401]	cv_agg&#39;s auc: 0.816668	cv_agg&#39;s rmse: 0.360425
[402]	cv_agg&#39;s auc: 0.816714	cv_agg&#39;s rmse: 0.360401
[403]	cv_agg&#39;s auc: 0.816804	cv_agg&#39;s rmse: 0.360373
[404]	cv_agg&#39;s auc: 0.816805	cv_agg&#39;s rmse: 0.360369
[405]	cv_agg&#39;s auc: 0.816831	cv_agg&#39;s rmse: 0.360354
[406]	cv_agg&#39;s auc: 0.816825	cv_agg&#39;s rmse: 0.360357
[407]	cv_agg&#39;s auc: 0.816894	cv_agg&#39;s rmse: 0.360333
[408]	cv_agg&#39;s auc: 0.816934	cv_agg&#39;s rmse: 0.36032
[409]	cv_agg&#39;s auc: 0.816969	cv_agg&#39;s rmse: 0.360303
[410]	cv_agg&#39;s auc: 0.81697	cv_agg&#39;s rmse: 0.360302
[411]	cv_agg&#39;s auc: 0.816979	cv_agg&#39;s rmse: 0.360291
[412]	cv_agg&#39;s auc: 0.81702	cv_agg&#39;s rmse: 0.360273
[413]	cv_agg&#39;s auc: 0.817132	cv_agg&#39;s rmse: 0.360218
[414]	cv_agg&#39;s auc: 0.81715	cv_agg&#39;s rmse: 0.360205
[415]	cv_agg&#39;s auc: 0.817146	cv_agg&#39;s rmse: 0.360202
[416]	cv_agg&#39;s auc: 0.817198	cv_agg&#39;s rmse: 0.360183
[417]	cv_agg&#39;s auc: 0.817229	cv_agg&#39;s rmse: 0.360174
[418]	cv_agg&#39;s auc: 0.817311	cv_agg&#39;s rmse: 0.36015
[419]	cv_agg&#39;s auc: 0.817323	cv_agg&#39;s rmse: 0.360145
[420]	cv_agg&#39;s auc: 0.81734	cv_agg&#39;s rmse: 0.360138
[421]	cv_agg&#39;s auc: 0.817342	cv_agg&#39;s rmse: 0.360134
[422]	cv_agg&#39;s auc: 0.817359	cv_agg&#39;s rmse: 0.360128
[423]	cv_agg&#39;s auc: 0.817354	cv_agg&#39;s rmse: 0.360124
[424]	cv_agg&#39;s auc: 0.817426	cv_agg&#39;s rmse: 0.360101
[425]	cv_agg&#39;s auc: 0.817449	cv_agg&#39;s rmse: 0.360088
[426]	cv_agg&#39;s auc: 0.817485	cv_agg&#39;s rmse: 0.360069
[427]	cv_agg&#39;s auc: 0.817578	cv_agg&#39;s rmse: 0.360032
[428]	cv_agg&#39;s auc: 0.817641	cv_agg&#39;s rmse: 0.360011
[429]	cv_agg&#39;s auc: 0.817657	cv_agg&#39;s rmse: 0.360006
[430]	cv_agg&#39;s auc: 0.817677	cv_agg&#39;s rmse: 0.359997
[431]	cv_agg&#39;s auc: 0.817689	cv_agg&#39;s rmse: 0.35999
[432]	cv_agg&#39;s auc: 0.817699	cv_agg&#39;s rmse: 0.35998
[433]	cv_agg&#39;s auc: 0.817718	cv_agg&#39;s rmse: 0.359971
[434]	cv_agg&#39;s auc: 0.817752	cv_agg&#39;s rmse: 0.359955
[435]	cv_agg&#39;s auc: 0.817784	cv_agg&#39;s rmse: 0.359942
[436]	cv_agg&#39;s auc: 0.817821	cv_agg&#39;s rmse: 0.35993
[437]	cv_agg&#39;s auc: 0.817863	cv_agg&#39;s rmse: 0.359909
[438]	cv_agg&#39;s auc: 0.817862	cv_agg&#39;s rmse: 0.359902
[439]	cv_agg&#39;s auc: 0.817861	cv_agg&#39;s rmse: 0.359895
[440]	cv_agg&#39;s auc: 0.817953	cv_agg&#39;s rmse: 0.359866
[441]	cv_agg&#39;s auc: 0.817957	cv_agg&#39;s rmse: 0.359856
[442]	cv_agg&#39;s auc: 0.817953	cv_agg&#39;s rmse: 0.359848
[443]	cv_agg&#39;s auc: 0.817956	cv_agg&#39;s rmse: 0.359846
[444]	cv_agg&#39;s auc: 0.818023	cv_agg&#39;s rmse: 0.359821
[445]	cv_agg&#39;s auc: 0.818046	cv_agg&#39;s rmse: 0.359814
[446]	cv_agg&#39;s auc: 0.818068	cv_agg&#39;s rmse: 0.359807
[447]	cv_agg&#39;s auc: 0.818104	cv_agg&#39;s rmse: 0.359796
[448]	cv_agg&#39;s auc: 0.818136	cv_agg&#39;s rmse: 0.359774
[449]	cv_agg&#39;s auc: 0.818177	cv_agg&#39;s rmse: 0.359757
[450]	cv_agg&#39;s auc: 0.818175	cv_agg&#39;s rmse: 0.359752
[451]	cv_agg&#39;s auc: 0.818192	cv_agg&#39;s rmse: 0.359739
[452]	cv_agg&#39;s auc: 0.818225	cv_agg&#39;s rmse: 0.359727
[453]	cv_agg&#39;s auc: 0.81832	cv_agg&#39;s rmse: 0.359694
[454]	cv_agg&#39;s auc: 0.818359	cv_agg&#39;s rmse: 0.359675
[455]	cv_agg&#39;s auc: 0.818402	cv_agg&#39;s rmse: 0.359656
[456]	cv_agg&#39;s auc: 0.818405	cv_agg&#39;s rmse: 0.359651
[457]	cv_agg&#39;s auc: 0.818413	cv_agg&#39;s rmse: 0.359649
[458]	cv_agg&#39;s auc: 0.818433	cv_agg&#39;s rmse: 0.359638
[459]	cv_agg&#39;s auc: 0.818489	cv_agg&#39;s rmse: 0.359616
[460]	cv_agg&#39;s auc: 0.818488	cv_agg&#39;s rmse: 0.359617
[461]	cv_agg&#39;s auc: 0.818535	cv_agg&#39;s rmse: 0.3596
[462]	cv_agg&#39;s auc: 0.81854	cv_agg&#39;s rmse: 0.3596
[463]	cv_agg&#39;s auc: 0.818541	cv_agg&#39;s rmse: 0.359592
[464]	cv_agg&#39;s auc: 0.818589	cv_agg&#39;s rmse: 0.359575
[465]	cv_agg&#39;s auc: 0.818611	cv_agg&#39;s rmse: 0.359566
[466]	cv_agg&#39;s auc: 0.818713	cv_agg&#39;s rmse: 0.359525
[467]	cv_agg&#39;s auc: 0.818717	cv_agg&#39;s rmse: 0.35952
[468]	cv_agg&#39;s auc: 0.818704	cv_agg&#39;s rmse: 0.359523
[469]	cv_agg&#39;s auc: 0.81876	cv_agg&#39;s rmse: 0.359504
[470]	cv_agg&#39;s auc: 0.818781	cv_agg&#39;s rmse: 0.359492
[471]	cv_agg&#39;s auc: 0.818829	cv_agg&#39;s rmse: 0.359472
[472]	cv_agg&#39;s auc: 0.818816	cv_agg&#39;s rmse: 0.35948
[473]	cv_agg&#39;s auc: 0.818857	cv_agg&#39;s rmse: 0.359463
[474]	cv_agg&#39;s auc: 0.818944	cv_agg&#39;s rmse: 0.359433
[475]	cv_agg&#39;s auc: 0.818974	cv_agg&#39;s rmse: 0.35941
[476]	cv_agg&#39;s auc: 0.819065	cv_agg&#39;s rmse: 0.35937
[477]	cv_agg&#39;s auc: 0.819077	cv_agg&#39;s rmse: 0.359365
[478]	cv_agg&#39;s auc: 0.819138	cv_agg&#39;s rmse: 0.359342
[479]	cv_agg&#39;s auc: 0.819186	cv_agg&#39;s rmse: 0.359318
[480]	cv_agg&#39;s auc: 0.81919	cv_agg&#39;s rmse: 0.359317
[481]	cv_agg&#39;s auc: 0.819221	cv_agg&#39;s rmse: 0.359299
[482]	cv_agg&#39;s auc: 0.81924	cv_agg&#39;s rmse: 0.359293
[483]	cv_agg&#39;s auc: 0.819227	cv_agg&#39;s rmse: 0.359294
[484]	cv_agg&#39;s auc: 0.819291	cv_agg&#39;s rmse: 0.359267
[485]	cv_agg&#39;s auc: 0.819319	cv_agg&#39;s rmse: 0.359259
[486]	cv_agg&#39;s auc: 0.819325	cv_agg&#39;s rmse: 0.35925
[487]	cv_agg&#39;s auc: 0.819342	cv_agg&#39;s rmse: 0.359238
[488]	cv_agg&#39;s auc: 0.819329	cv_agg&#39;s rmse: 0.359239
[489]	cv_agg&#39;s auc: 0.819362	cv_agg&#39;s rmse: 0.359222
[490]	cv_agg&#39;s auc: 0.819388	cv_agg&#39;s rmse: 0.359211
[491]	cv_agg&#39;s auc: 0.819483	cv_agg&#39;s rmse: 0.359168
[492]	cv_agg&#39;s auc: 0.819522	cv_agg&#39;s rmse: 0.359149
[493]	cv_agg&#39;s auc: 0.819559	cv_agg&#39;s rmse: 0.359141
[494]	cv_agg&#39;s auc: 0.819562	cv_agg&#39;s rmse: 0.359141
[495]	cv_agg&#39;s auc: 0.819586	cv_agg&#39;s rmse: 0.35913
[496]	cv_agg&#39;s auc: 0.819598	cv_agg&#39;s rmse: 0.359126
[497]	cv_agg&#39;s auc: 0.819601	cv_agg&#39;s rmse: 0.359118
[498]	cv_agg&#39;s auc: 0.819608	cv_agg&#39;s rmse: 0.359116
[499]	cv_agg&#39;s auc: 0.819603	cv_agg&#39;s rmse: 0.359113
[500]	cv_agg&#39;s auc: 0.819622	cv_agg&#39;s rmse: 0.359098
[501]	cv_agg&#39;s auc: 0.81961	cv_agg&#39;s rmse: 0.359096
[502]	cv_agg&#39;s auc: 0.819615	cv_agg&#39;s rmse: 0.35909
[503]	cv_agg&#39;s auc: 0.819649	cv_agg&#39;s rmse: 0.35908
[504]	cv_agg&#39;s auc: 0.819657	cv_agg&#39;s rmse: 0.359077
[505]	cv_agg&#39;s auc: 0.819708	cv_agg&#39;s rmse: 0.359059
[506]	cv_agg&#39;s auc: 0.819759	cv_agg&#39;s rmse: 0.359044
[507]	cv_agg&#39;s auc: 0.819821	cv_agg&#39;s rmse: 0.359003
[508]	cv_agg&#39;s auc: 0.819825	cv_agg&#39;s rmse: 0.358996
[509]	cv_agg&#39;s auc: 0.819843	cv_agg&#39;s rmse: 0.358983
[510]	cv_agg&#39;s auc: 0.819867	cv_agg&#39;s rmse: 0.358971
[511]	cv_agg&#39;s auc: 0.819956	cv_agg&#39;s rmse: 0.358936
[512]	cv_agg&#39;s auc: 0.819951	cv_agg&#39;s rmse: 0.358934
[513]	cv_agg&#39;s auc: 0.819997	cv_agg&#39;s rmse: 0.358916
[514]	cv_agg&#39;s auc: 0.820009	cv_agg&#39;s rmse: 0.358908
[515]	cv_agg&#39;s auc: 0.820096	cv_agg&#39;s rmse: 0.358876
[516]	cv_agg&#39;s auc: 0.820106	cv_agg&#39;s rmse: 0.358872
[517]	cv_agg&#39;s auc: 0.820122	cv_agg&#39;s rmse: 0.358862
[518]	cv_agg&#39;s auc: 0.820212	cv_agg&#39;s rmse: 0.358832
[519]	cv_agg&#39;s auc: 0.82023	cv_agg&#39;s rmse: 0.358829
[520]	cv_agg&#39;s auc: 0.820233	cv_agg&#39;s rmse: 0.358825
[521]	cv_agg&#39;s auc: 0.820277	cv_agg&#39;s rmse: 0.358806
[522]	cv_agg&#39;s auc: 0.820295	cv_agg&#39;s rmse: 0.358796
[523]	cv_agg&#39;s auc: 0.820391	cv_agg&#39;s rmse: 0.358767
[524]	cv_agg&#39;s auc: 0.820469	cv_agg&#39;s rmse: 0.358744
[525]	cv_agg&#39;s auc: 0.820534	cv_agg&#39;s rmse: 0.358719
[526]	cv_agg&#39;s auc: 0.820541	cv_agg&#39;s rmse: 0.358711
[527]	cv_agg&#39;s auc: 0.820565	cv_agg&#39;s rmse: 0.3587
[528]	cv_agg&#39;s auc: 0.820595	cv_agg&#39;s rmse: 0.358693
[529]	cv_agg&#39;s auc: 0.820652	cv_agg&#39;s rmse: 0.358667
[530]	cv_agg&#39;s auc: 0.820719	cv_agg&#39;s rmse: 0.358633
[531]	cv_agg&#39;s auc: 0.820724	cv_agg&#39;s rmse: 0.358629
[532]	cv_agg&#39;s auc: 0.820756	cv_agg&#39;s rmse: 0.358614
[533]	cv_agg&#39;s auc: 0.820765	cv_agg&#39;s rmse: 0.35861
[534]	cv_agg&#39;s auc: 0.820857	cv_agg&#39;s rmse: 0.358573
[535]	cv_agg&#39;s auc: 0.82088	cv_agg&#39;s rmse: 0.358563
[536]	cv_agg&#39;s auc: 0.820901	cv_agg&#39;s rmse: 0.358559
[537]	cv_agg&#39;s auc: 0.820931	cv_agg&#39;s rmse: 0.358541
[538]	cv_agg&#39;s auc: 0.820952	cv_agg&#39;s rmse: 0.358535
[539]	cv_agg&#39;s auc: 0.820978	cv_agg&#39;s rmse: 0.358527
[540]	cv_agg&#39;s auc: 0.82101	cv_agg&#39;s rmse: 0.358511
[541]	cv_agg&#39;s auc: 0.820994	cv_agg&#39;s rmse: 0.358515
[542]	cv_agg&#39;s auc: 0.821014	cv_agg&#39;s rmse: 0.358509
[543]	cv_agg&#39;s auc: 0.821015	cv_agg&#39;s rmse: 0.358507
[544]	cv_agg&#39;s auc: 0.821028	cv_agg&#39;s rmse: 0.358502
[545]	cv_agg&#39;s auc: 0.821034	cv_agg&#39;s rmse: 0.358495
[546]	cv_agg&#39;s auc: 0.821015	cv_agg&#39;s rmse: 0.358498
[547]	cv_agg&#39;s auc: 0.821031	cv_agg&#39;s rmse: 0.35849
[548]	cv_agg&#39;s auc: 0.821099	cv_agg&#39;s rmse: 0.358457
[549]	cv_agg&#39;s auc: 0.821116	cv_agg&#39;s rmse: 0.35845
[550]	cv_agg&#39;s auc: 0.821118	cv_agg&#39;s rmse: 0.358448
[551]	cv_agg&#39;s auc: 0.821133	cv_agg&#39;s rmse: 0.358443
[552]	cv_agg&#39;s auc: 0.82115	cv_agg&#39;s rmse: 0.358437
[553]	cv_agg&#39;s auc: 0.821176	cv_agg&#39;s rmse: 0.358427
[554]	cv_agg&#39;s auc: 0.821205	cv_agg&#39;s rmse: 0.358418
[555]	cv_agg&#39;s auc: 0.821183	cv_agg&#39;s rmse: 0.35842
[556]	cv_agg&#39;s auc: 0.821203	cv_agg&#39;s rmse: 0.358409
[557]	cv_agg&#39;s auc: 0.821216	cv_agg&#39;s rmse: 0.358404
[558]	cv_agg&#39;s auc: 0.821212	cv_agg&#39;s rmse: 0.358399
[559]	cv_agg&#39;s auc: 0.821223	cv_agg&#39;s rmse: 0.358393
[560]	cv_agg&#39;s auc: 0.821324	cv_agg&#39;s rmse: 0.358354
[561]	cv_agg&#39;s auc: 0.821377	cv_agg&#39;s rmse: 0.35833
[562]	cv_agg&#39;s auc: 0.821367	cv_agg&#39;s rmse: 0.358336
[563]	cv_agg&#39;s auc: 0.821378	cv_agg&#39;s rmse: 0.358334
[564]	cv_agg&#39;s auc: 0.821369	cv_agg&#39;s rmse: 0.358339
[565]	cv_agg&#39;s auc: 0.821361	cv_agg&#39;s rmse: 0.358339
[566]	cv_agg&#39;s auc: 0.821408	cv_agg&#39;s rmse: 0.358324
[567]	cv_agg&#39;s auc: 0.821428	cv_agg&#39;s rmse: 0.358319
[568]	cv_agg&#39;s auc: 0.82146	cv_agg&#39;s rmse: 0.35831
[569]	cv_agg&#39;s auc: 0.821536	cv_agg&#39;s rmse: 0.35828
[570]	cv_agg&#39;s auc: 0.821537	cv_agg&#39;s rmse: 0.358274
[571]	cv_agg&#39;s auc: 0.821611	cv_agg&#39;s rmse: 0.35824
[572]	cv_agg&#39;s auc: 0.821643	cv_agg&#39;s rmse: 0.358223
[573]	cv_agg&#39;s auc: 0.821653	cv_agg&#39;s rmse: 0.358218
[574]	cv_agg&#39;s auc: 0.821722	cv_agg&#39;s rmse: 0.358183
[575]	cv_agg&#39;s auc: 0.821733	cv_agg&#39;s rmse: 0.358174
[576]	cv_agg&#39;s auc: 0.821769	cv_agg&#39;s rmse: 0.358159
[577]	cv_agg&#39;s auc: 0.821775	cv_agg&#39;s rmse: 0.358155
[578]	cv_agg&#39;s auc: 0.82179	cv_agg&#39;s rmse: 0.358148
[579]	cv_agg&#39;s auc: 0.821817	cv_agg&#39;s rmse: 0.358137
[580]	cv_agg&#39;s auc: 0.821852	cv_agg&#39;s rmse: 0.358118
[581]	cv_agg&#39;s auc: 0.821934	cv_agg&#39;s rmse: 0.358086
[582]	cv_agg&#39;s auc: 0.821945	cv_agg&#39;s rmse: 0.358076
[583]	cv_agg&#39;s auc: 0.821958	cv_agg&#39;s rmse: 0.358066
[584]	cv_agg&#39;s auc: 0.821952	cv_agg&#39;s rmse: 0.358068
[585]	cv_agg&#39;s auc: 0.821957	cv_agg&#39;s rmse: 0.358065
[586]	cv_agg&#39;s auc: 0.821947	cv_agg&#39;s rmse: 0.358067
[587]	cv_agg&#39;s auc: 0.821935	cv_agg&#39;s rmse: 0.35807
[588]	cv_agg&#39;s auc: 0.821987	cv_agg&#39;s rmse: 0.358053
[589]	cv_agg&#39;s auc: 0.822014	cv_agg&#39;s rmse: 0.358045
[590]	cv_agg&#39;s auc: 0.822009	cv_agg&#39;s rmse: 0.358044
[591]	cv_agg&#39;s auc: 0.821983	cv_agg&#39;s rmse: 0.358049
[592]	cv_agg&#39;s auc: 0.82203	cv_agg&#39;s rmse: 0.358026
[593]	cv_agg&#39;s auc: 0.822055	cv_agg&#39;s rmse: 0.358019
[594]	cv_agg&#39;s auc: 0.822068	cv_agg&#39;s rmse: 0.35801
[595]	cv_agg&#39;s auc: 0.822066	cv_agg&#39;s rmse: 0.358004
[596]	cv_agg&#39;s auc: 0.822111	cv_agg&#39;s rmse: 0.357984
[597]	cv_agg&#39;s auc: 0.822109	cv_agg&#39;s rmse: 0.357984
[598]	cv_agg&#39;s auc: 0.822139	cv_agg&#39;s rmse: 0.357972
[599]	cv_agg&#39;s auc: 0.822127	cv_agg&#39;s rmse: 0.357975
[600]	cv_agg&#39;s auc: 0.822155	cv_agg&#39;s rmse: 0.357963
[601]	cv_agg&#39;s auc: 0.822139	cv_agg&#39;s rmse: 0.357967
[602]	cv_agg&#39;s auc: 0.822123	cv_agg&#39;s rmse: 0.357968
[603]	cv_agg&#39;s auc: 0.822205	cv_agg&#39;s rmse: 0.357927
[604]	cv_agg&#39;s auc: 0.822215	cv_agg&#39;s rmse: 0.357922
[605]	cv_agg&#39;s auc: 0.822265	cv_agg&#39;s rmse: 0.357907
[606]	cv_agg&#39;s auc: 0.822249	cv_agg&#39;s rmse: 0.357907
[607]	cv_agg&#39;s auc: 0.822321	cv_agg&#39;s rmse: 0.357888
[608]	cv_agg&#39;s auc: 0.822296	cv_agg&#39;s rmse: 0.35789
[609]	cv_agg&#39;s auc: 0.822382	cv_agg&#39;s rmse: 0.357852
[610]	cv_agg&#39;s auc: 0.822381	cv_agg&#39;s rmse: 0.357845
[611]	cv_agg&#39;s auc: 0.82246	cv_agg&#39;s rmse: 0.357811
[612]	cv_agg&#39;s auc: 0.8225	cv_agg&#39;s rmse: 0.357791
[613]	cv_agg&#39;s auc: 0.822493	cv_agg&#39;s rmse: 0.357791
[614]	cv_agg&#39;s auc: 0.822519	cv_agg&#39;s rmse: 0.357776
[615]	cv_agg&#39;s auc: 0.822569	cv_agg&#39;s rmse: 0.357756
[616]	cv_agg&#39;s auc: 0.822634	cv_agg&#39;s rmse: 0.357728
[617]	cv_agg&#39;s auc: 0.822633	cv_agg&#39;s rmse: 0.357728
[618]	cv_agg&#39;s auc: 0.822621	cv_agg&#39;s rmse: 0.357731
[619]	cv_agg&#39;s auc: 0.822626	cv_agg&#39;s rmse: 0.357727
[620]	cv_agg&#39;s auc: 0.822691	cv_agg&#39;s rmse: 0.357705
[621]	cv_agg&#39;s auc: 0.822719	cv_agg&#39;s rmse: 0.357694
[622]	cv_agg&#39;s auc: 0.822695	cv_agg&#39;s rmse: 0.357696
[623]	cv_agg&#39;s auc: 0.822719	cv_agg&#39;s rmse: 0.357683
[624]	cv_agg&#39;s auc: 0.822753	cv_agg&#39;s rmse: 0.357673
[625]	cv_agg&#39;s auc: 0.822767	cv_agg&#39;s rmse: 0.357666
[626]	cv_agg&#39;s auc: 0.82278	cv_agg&#39;s rmse: 0.357658
[627]	cv_agg&#39;s auc: 0.82278	cv_agg&#39;s rmse: 0.357655
[628]	cv_agg&#39;s auc: 0.822848	cv_agg&#39;s rmse: 0.357628
[629]	cv_agg&#39;s auc: 0.822829	cv_agg&#39;s rmse: 0.357631
[630]	cv_agg&#39;s auc: 0.822884	cv_agg&#39;s rmse: 0.357612
[631]	cv_agg&#39;s auc: 0.8229	cv_agg&#39;s rmse: 0.357599
[632]	cv_agg&#39;s auc: 0.8229	cv_agg&#39;s rmse: 0.357599
[633]	cv_agg&#39;s auc: 0.822921	cv_agg&#39;s rmse: 0.357589
[634]	cv_agg&#39;s auc: 0.823032	cv_agg&#39;s rmse: 0.357544
[635]	cv_agg&#39;s auc: 0.823034	cv_agg&#39;s rmse: 0.357542
[636]	cv_agg&#39;s auc: 0.823007	cv_agg&#39;s rmse: 0.357546
[637]	cv_agg&#39;s auc: 0.823083	cv_agg&#39;s rmse: 0.357518
[638]	cv_agg&#39;s auc: 0.823106	cv_agg&#39;s rmse: 0.357511
[639]	cv_agg&#39;s auc: 0.823123	cv_agg&#39;s rmse: 0.357506
[640]	cv_agg&#39;s auc: 0.823137	cv_agg&#39;s rmse: 0.357497
[641]	cv_agg&#39;s auc: 0.823108	cv_agg&#39;s rmse: 0.357503
[642]	cv_agg&#39;s auc: 0.823151	cv_agg&#39;s rmse: 0.357487
[643]	cv_agg&#39;s auc: 0.823173	cv_agg&#39;s rmse: 0.357481
[644]	cv_agg&#39;s auc: 0.823206	cv_agg&#39;s rmse: 0.357465
[645]	cv_agg&#39;s auc: 0.823185	cv_agg&#39;s rmse: 0.357472
[646]	cv_agg&#39;s auc: 0.823191	cv_agg&#39;s rmse: 0.357466
[647]	cv_agg&#39;s auc: 0.823202	cv_agg&#39;s rmse: 0.357464
[648]	cv_agg&#39;s auc: 0.823201	cv_agg&#39;s rmse: 0.357464
[649]	cv_agg&#39;s auc: 0.823202	cv_agg&#39;s rmse: 0.35746
[650]	cv_agg&#39;s auc: 0.82322	cv_agg&#39;s rmse: 0.357456
[651]	cv_agg&#39;s auc: 0.823326	cv_agg&#39;s rmse: 0.357417
[652]	cv_agg&#39;s auc: 0.823304	cv_agg&#39;s rmse: 0.357422
[653]	cv_agg&#39;s auc: 0.823305	cv_agg&#39;s rmse: 0.357421
[654]	cv_agg&#39;s auc: 0.823328	cv_agg&#39;s rmse: 0.35741
[655]	cv_agg&#39;s auc: 0.823308	cv_agg&#39;s rmse: 0.357417
[656]	cv_agg&#39;s auc: 0.82334	cv_agg&#39;s rmse: 0.357404
[657]	cv_agg&#39;s auc: 0.823335	cv_agg&#39;s rmse: 0.357396
[658]	cv_agg&#39;s auc: 0.823341	cv_agg&#39;s rmse: 0.357391
[659]	cv_agg&#39;s auc: 0.823372	cv_agg&#39;s rmse: 0.357376
[660]	cv_agg&#39;s auc: 0.823355	cv_agg&#39;s rmse: 0.35738
[661]	cv_agg&#39;s auc: 0.823365	cv_agg&#39;s rmse: 0.357373
[662]	cv_agg&#39;s auc: 0.823386	cv_agg&#39;s rmse: 0.357365
[663]	cv_agg&#39;s auc: 0.823402	cv_agg&#39;s rmse: 0.35736
[664]	cv_agg&#39;s auc: 0.823414	cv_agg&#39;s rmse: 0.357355
[665]	cv_agg&#39;s auc: 0.823419	cv_agg&#39;s rmse: 0.357353
[666]	cv_agg&#39;s auc: 0.823404	cv_agg&#39;s rmse: 0.357357
[667]	cv_agg&#39;s auc: 0.823458	cv_agg&#39;s rmse: 0.35733
[668]	cv_agg&#39;s auc: 0.823462	cv_agg&#39;s rmse: 0.357329
[669]	cv_agg&#39;s auc: 0.823501	cv_agg&#39;s rmse: 0.357315
[670]	cv_agg&#39;s auc: 0.82351	cv_agg&#39;s rmse: 0.357308
[671]	cv_agg&#39;s auc: 0.823565	cv_agg&#39;s rmse: 0.35729
[672]	cv_agg&#39;s auc: 0.82357	cv_agg&#39;s rmse: 0.357289
[673]	cv_agg&#39;s auc: 0.823601	cv_agg&#39;s rmse: 0.357278
[674]	cv_agg&#39;s auc: 0.823583	cv_agg&#39;s rmse: 0.357284
[675]	cv_agg&#39;s auc: 0.823662	cv_agg&#39;s rmse: 0.357255
[676]	cv_agg&#39;s auc: 0.823649	cv_agg&#39;s rmse: 0.357257
[677]	cv_agg&#39;s auc: 0.823662	cv_agg&#39;s rmse: 0.357253
[678]	cv_agg&#39;s auc: 0.823662	cv_agg&#39;s rmse: 0.357254
[679]	cv_agg&#39;s auc: 0.823702	cv_agg&#39;s rmse: 0.357238
[680]	cv_agg&#39;s auc: 0.823729	cv_agg&#39;s rmse: 0.357228
[681]	cv_agg&#39;s auc: 0.823753	cv_agg&#39;s rmse: 0.357219
[682]	cv_agg&#39;s auc: 0.82376	cv_agg&#39;s rmse: 0.357213
[683]	cv_agg&#39;s auc: 0.823748	cv_agg&#39;s rmse: 0.357213
[684]	cv_agg&#39;s auc: 0.823763	cv_agg&#39;s rmse: 0.357205
[685]	cv_agg&#39;s auc: 0.823795	cv_agg&#39;s rmse: 0.357188
[686]	cv_agg&#39;s auc: 0.823839	cv_agg&#39;s rmse: 0.357174
[687]	cv_agg&#39;s auc: 0.823847	cv_agg&#39;s rmse: 0.357171
[688]	cv_agg&#39;s auc: 0.823951	cv_agg&#39;s rmse: 0.357129
[689]	cv_agg&#39;s auc: 0.823964	cv_agg&#39;s rmse: 0.357122
[690]	cv_agg&#39;s auc: 0.823955	cv_agg&#39;s rmse: 0.357121
[691]	cv_agg&#39;s auc: 0.824007	cv_agg&#39;s rmse: 0.357096
[692]	cv_agg&#39;s auc: 0.824005	cv_agg&#39;s rmse: 0.357098
[693]	cv_agg&#39;s auc: 0.82401	cv_agg&#39;s rmse: 0.357088
[694]	cv_agg&#39;s auc: 0.82404	cv_agg&#39;s rmse: 0.357075
[695]	cv_agg&#39;s auc: 0.824024	cv_agg&#39;s rmse: 0.357081
[696]	cv_agg&#39;s auc: 0.824027	cv_agg&#39;s rmse: 0.357081
[697]	cv_agg&#39;s auc: 0.824036	cv_agg&#39;s rmse: 0.357076
[698]	cv_agg&#39;s auc: 0.824035	cv_agg&#39;s rmse: 0.357068
[699]	cv_agg&#39;s auc: 0.824052	cv_agg&#39;s rmse: 0.357062
[700]	cv_agg&#39;s auc: 0.824046	cv_agg&#39;s rmse: 0.357061
[701]	cv_agg&#39;s auc: 0.824069	cv_agg&#39;s rmse: 0.357052
[702]	cv_agg&#39;s auc: 0.82406	cv_agg&#39;s rmse: 0.357051
[703]	cv_agg&#39;s auc: 0.824128	cv_agg&#39;s rmse: 0.357019
[704]	cv_agg&#39;s auc: 0.824146	cv_agg&#39;s rmse: 0.357011
[705]	cv_agg&#39;s auc: 0.82413	cv_agg&#39;s rmse: 0.357012
[706]	cv_agg&#39;s auc: 0.82414	cv_agg&#39;s rmse: 0.357005
[707]	cv_agg&#39;s auc: 0.824147	cv_agg&#39;s rmse: 0.357004
[708]	cv_agg&#39;s auc: 0.824158	cv_agg&#39;s rmse: 0.357001
[709]	cv_agg&#39;s auc: 0.824159	cv_agg&#39;s rmse: 0.356998
[710]	cv_agg&#39;s auc: 0.824148	cv_agg&#39;s rmse: 0.356998
[711]	cv_agg&#39;s auc: 0.82415	cv_agg&#39;s rmse: 0.356995
[712]	cv_agg&#39;s auc: 0.824181	cv_agg&#39;s rmse: 0.356985
[713]	cv_agg&#39;s auc: 0.82423	cv_agg&#39;s rmse: 0.356962
[714]	cv_agg&#39;s auc: 0.824237	cv_agg&#39;s rmse: 0.356957
[715]	cv_agg&#39;s auc: 0.824234	cv_agg&#39;s rmse: 0.356953
[716]	cv_agg&#39;s auc: 0.824223	cv_agg&#39;s rmse: 0.356955
[717]	cv_agg&#39;s auc: 0.82426	cv_agg&#39;s rmse: 0.356945
[718]	cv_agg&#39;s auc: 0.824296	cv_agg&#39;s rmse: 0.356935
[719]	cv_agg&#39;s auc: 0.82432	cv_agg&#39;s rmse: 0.356923
[720]	cv_agg&#39;s auc: 0.824334	cv_agg&#39;s rmse: 0.356913
[721]	cv_agg&#39;s auc: 0.824379	cv_agg&#39;s rmse: 0.356895
[722]	cv_agg&#39;s auc: 0.824367	cv_agg&#39;s rmse: 0.356894
[723]	cv_agg&#39;s auc: 0.824403	cv_agg&#39;s rmse: 0.356881
[724]	cv_agg&#39;s auc: 0.824412	cv_agg&#39;s rmse: 0.356873
[725]	cv_agg&#39;s auc: 0.824431	cv_agg&#39;s rmse: 0.356868
[726]	cv_agg&#39;s auc: 0.824455	cv_agg&#39;s rmse: 0.35686
[727]	cv_agg&#39;s auc: 0.824467	cv_agg&#39;s rmse: 0.35685
[728]	cv_agg&#39;s auc: 0.82447	cv_agg&#39;s rmse: 0.356844
[729]	cv_agg&#39;s auc: 0.82451	cv_agg&#39;s rmse: 0.356824
[730]	cv_agg&#39;s auc: 0.824492	cv_agg&#39;s rmse: 0.356828
[731]	cv_agg&#39;s auc: 0.82449	cv_agg&#39;s rmse: 0.356829
[732]	cv_agg&#39;s auc: 0.824506	cv_agg&#39;s rmse: 0.356821
[733]	cv_agg&#39;s auc: 0.824517	cv_agg&#39;s rmse: 0.356821
[734]	cv_agg&#39;s auc: 0.824522	cv_agg&#39;s rmse: 0.356818
[735]	cv_agg&#39;s auc: 0.824548	cv_agg&#39;s rmse: 0.356808
[736]	cv_agg&#39;s auc: 0.824549	cv_agg&#39;s rmse: 0.356807
[737]	cv_agg&#39;s auc: 0.824544	cv_agg&#39;s rmse: 0.356804
[738]	cv_agg&#39;s auc: 0.824565	cv_agg&#39;s rmse: 0.356797
[739]	cv_agg&#39;s auc: 0.824564	cv_agg&#39;s rmse: 0.356797
[740]	cv_agg&#39;s auc: 0.824583	cv_agg&#39;s rmse: 0.356787
[741]	cv_agg&#39;s auc: 0.824582	cv_agg&#39;s rmse: 0.356787
[742]	cv_agg&#39;s auc: 0.824585	cv_agg&#39;s rmse: 0.356784
[743]	cv_agg&#39;s auc: 0.824638	cv_agg&#39;s rmse: 0.356764
[744]	cv_agg&#39;s auc: 0.824652	cv_agg&#39;s rmse: 0.356752
[745]	cv_agg&#39;s auc: 0.824656	cv_agg&#39;s rmse: 0.356747
[746]	cv_agg&#39;s auc: 0.824669	cv_agg&#39;s rmse: 0.356744
[747]	cv_agg&#39;s auc: 0.824678	cv_agg&#39;s rmse: 0.356737
[748]	cv_agg&#39;s auc: 0.824706	cv_agg&#39;s rmse: 0.356723
[749]	cv_agg&#39;s auc: 0.824721	cv_agg&#39;s rmse: 0.356717
[750]	cv_agg&#39;s auc: 0.824721	cv_agg&#39;s rmse: 0.356716
[751]	cv_agg&#39;s auc: 0.824747	cv_agg&#39;s rmse: 0.356704
[752]	cv_agg&#39;s auc: 0.824766	cv_agg&#39;s rmse: 0.356695
[753]	cv_agg&#39;s auc: 0.824767	cv_agg&#39;s rmse: 0.356694
[754]	cv_agg&#39;s auc: 0.824758	cv_agg&#39;s rmse: 0.356696
[755]	cv_agg&#39;s auc: 0.824752	cv_agg&#39;s rmse: 0.356697
[756]	cv_agg&#39;s auc: 0.824769	cv_agg&#39;s rmse: 0.356692
[757]	cv_agg&#39;s auc: 0.824778	cv_agg&#39;s rmse: 0.356689
[758]	cv_agg&#39;s auc: 0.824799	cv_agg&#39;s rmse: 0.356676
[759]	cv_agg&#39;s auc: 0.824835	cv_agg&#39;s rmse: 0.356662
[760]	cv_agg&#39;s auc: 0.824835	cv_agg&#39;s rmse: 0.356666
[761]	cv_agg&#39;s auc: 0.824836	cv_agg&#39;s rmse: 0.356662
[762]	cv_agg&#39;s auc: 0.824825	cv_agg&#39;s rmse: 0.356667
[763]	cv_agg&#39;s auc: 0.824834	cv_agg&#39;s rmse: 0.356658
[764]	cv_agg&#39;s auc: 0.82483	cv_agg&#39;s rmse: 0.356658
[765]	cv_agg&#39;s auc: 0.82485	cv_agg&#39;s rmse: 0.356646
[766]	cv_agg&#39;s auc: 0.824891	cv_agg&#39;s rmse: 0.356625
[767]	cv_agg&#39;s auc: 0.824901	cv_agg&#39;s rmse: 0.356617
[768]	cv_agg&#39;s auc: 0.824893	cv_agg&#39;s rmse: 0.356618
[769]	cv_agg&#39;s auc: 0.824904	cv_agg&#39;s rmse: 0.356613
[770]	cv_agg&#39;s auc: 0.824873	cv_agg&#39;s rmse: 0.356617
[771]	cv_agg&#39;s auc: 0.824853	cv_agg&#39;s rmse: 0.356618
[772]	cv_agg&#39;s auc: 0.82486	cv_agg&#39;s rmse: 0.356609
[773]	cv_agg&#39;s auc: 0.824845	cv_agg&#39;s rmse: 0.356614
[774]	cv_agg&#39;s auc: 0.824864	cv_agg&#39;s rmse: 0.356614
[775]	cv_agg&#39;s auc: 0.824865	cv_agg&#39;s rmse: 0.356608
[776]	cv_agg&#39;s auc: 0.824872	cv_agg&#39;s rmse: 0.356603
[777]	cv_agg&#39;s auc: 0.824873	cv_agg&#39;s rmse: 0.356601
[778]	cv_agg&#39;s auc: 0.82486	cv_agg&#39;s rmse: 0.356601
[779]	cv_agg&#39;s auc: 0.824875	cv_agg&#39;s rmse: 0.356593
[780]	cv_agg&#39;s auc: 0.824885	cv_agg&#39;s rmse: 0.356587
[781]	cv_agg&#39;s auc: 0.824905	cv_agg&#39;s rmse: 0.356578
[782]	cv_agg&#39;s auc: 0.824916	cv_agg&#39;s rmse: 0.356568
[783]	cv_agg&#39;s auc: 0.824949	cv_agg&#39;s rmse: 0.356554
[784]	cv_agg&#39;s auc: 0.824959	cv_agg&#39;s rmse: 0.356548
[785]	cv_agg&#39;s auc: 0.824949	cv_agg&#39;s rmse: 0.356552
[786]	cv_agg&#39;s auc: 0.82497	cv_agg&#39;s rmse: 0.356541
[787]	cv_agg&#39;s auc: 0.824985	cv_agg&#39;s rmse: 0.356536
[788]	cv_agg&#39;s auc: 0.824991	cv_agg&#39;s rmse: 0.356532
[789]	cv_agg&#39;s auc: 0.825008	cv_agg&#39;s rmse: 0.356524
[790]	cv_agg&#39;s auc: 0.825025	cv_agg&#39;s rmse: 0.35651
[791]	cv_agg&#39;s auc: 0.825077	cv_agg&#39;s rmse: 0.356488
[792]	cv_agg&#39;s auc: 0.825096	cv_agg&#39;s rmse: 0.356477
[793]	cv_agg&#39;s auc: 0.825113	cv_agg&#39;s rmse: 0.356468
[794]	cv_agg&#39;s auc: 0.825129	cv_agg&#39;s rmse: 0.356462
[795]	cv_agg&#39;s auc: 0.825147	cv_agg&#39;s rmse: 0.356448
[796]	cv_agg&#39;s auc: 0.825174	cv_agg&#39;s rmse: 0.356442
[797]	cv_agg&#39;s auc: 0.825157	cv_agg&#39;s rmse: 0.356444
[798]	cv_agg&#39;s auc: 0.825234	cv_agg&#39;s rmse: 0.356417
[799]	cv_agg&#39;s auc: 0.825236	cv_agg&#39;s rmse: 0.356412
[800]	cv_agg&#39;s auc: 0.825232	cv_agg&#39;s rmse: 0.356414
[801]	cv_agg&#39;s auc: 0.825216	cv_agg&#39;s rmse: 0.356419
[802]	cv_agg&#39;s auc: 0.825227	cv_agg&#39;s rmse: 0.356411
[803]	cv_agg&#39;s auc: 0.82524	cv_agg&#39;s rmse: 0.35641
[804]	cv_agg&#39;s auc: 0.825276	cv_agg&#39;s rmse: 0.356393
[805]	cv_agg&#39;s auc: 0.825288	cv_agg&#39;s rmse: 0.356386
[806]	cv_agg&#39;s auc: 0.825289	cv_agg&#39;s rmse: 0.356381
[807]	cv_agg&#39;s auc: 0.82533	cv_agg&#39;s rmse: 0.35637
[808]	cv_agg&#39;s auc: 0.825333	cv_agg&#39;s rmse: 0.356369
[809]	cv_agg&#39;s auc: 0.825353	cv_agg&#39;s rmse: 0.356361
[810]	cv_agg&#39;s auc: 0.825338	cv_agg&#39;s rmse: 0.35636
[811]	cv_agg&#39;s auc: 0.825339	cv_agg&#39;s rmse: 0.356358
[812]	cv_agg&#39;s auc: 0.825333	cv_agg&#39;s rmse: 0.356356
[813]	cv_agg&#39;s auc: 0.825327	cv_agg&#39;s rmse: 0.356356
[814]	cv_agg&#39;s auc: 0.825318	cv_agg&#39;s rmse: 0.356353
[815]	cv_agg&#39;s auc: 0.825338	cv_agg&#39;s rmse: 0.35634
[816]	cv_agg&#39;s auc: 0.825415	cv_agg&#39;s rmse: 0.356306
[817]	cv_agg&#39;s auc: 0.825415	cv_agg&#39;s rmse: 0.356304
[818]	cv_agg&#39;s auc: 0.825425	cv_agg&#39;s rmse: 0.356303
[819]	cv_agg&#39;s auc: 0.825447	cv_agg&#39;s rmse: 0.3563
[820]	cv_agg&#39;s auc: 0.825464	cv_agg&#39;s rmse: 0.356296
[821]	cv_agg&#39;s auc: 0.825487	cv_agg&#39;s rmse: 0.356286
[822]	cv_agg&#39;s auc: 0.825502	cv_agg&#39;s rmse: 0.356279
[823]	cv_agg&#39;s auc: 0.825522	cv_agg&#39;s rmse: 0.356274
[824]	cv_agg&#39;s auc: 0.825636	cv_agg&#39;s rmse: 0.356222
[825]	cv_agg&#39;s auc: 0.825634	cv_agg&#39;s rmse: 0.356224
[826]	cv_agg&#39;s auc: 0.825635	cv_agg&#39;s rmse: 0.35622
[827]	cv_agg&#39;s auc: 0.825635	cv_agg&#39;s rmse: 0.356222
[828]	cv_agg&#39;s auc: 0.825627	cv_agg&#39;s rmse: 0.356224
[829]	cv_agg&#39;s auc: 0.825625	cv_agg&#39;s rmse: 0.356226
[830]	cv_agg&#39;s auc: 0.825622	cv_agg&#39;s rmse: 0.356219
[831]	cv_agg&#39;s auc: 0.825626	cv_agg&#39;s rmse: 0.356218
[832]	cv_agg&#39;s auc: 0.825624	cv_agg&#39;s rmse: 0.356219
[833]	cv_agg&#39;s auc: 0.82566	cv_agg&#39;s rmse: 0.356198
[834]	cv_agg&#39;s auc: 0.825659	cv_agg&#39;s rmse: 0.356197
[835]	cv_agg&#39;s auc: 0.825668	cv_agg&#39;s rmse: 0.356189
[836]	cv_agg&#39;s auc: 0.825724	cv_agg&#39;s rmse: 0.356161
[837]	cv_agg&#39;s auc: 0.825726	cv_agg&#39;s rmse: 0.356156
[838]	cv_agg&#39;s auc: 0.825729	cv_agg&#39;s rmse: 0.356153
[839]	cv_agg&#39;s auc: 0.825766	cv_agg&#39;s rmse: 0.356141
[840]	cv_agg&#39;s auc: 0.825798	cv_agg&#39;s rmse: 0.35613
[841]	cv_agg&#39;s auc: 0.825793	cv_agg&#39;s rmse: 0.356132
[842]	cv_agg&#39;s auc: 0.825805	cv_agg&#39;s rmse: 0.356122
[843]	cv_agg&#39;s auc: 0.825789	cv_agg&#39;s rmse: 0.356129
[844]	cv_agg&#39;s auc: 0.825794	cv_agg&#39;s rmse: 0.356126
[845]	cv_agg&#39;s auc: 0.825808	cv_agg&#39;s rmse: 0.356123
[846]	cv_agg&#39;s auc: 0.825801	cv_agg&#39;s rmse: 0.356125
[847]	cv_agg&#39;s auc: 0.825801	cv_agg&#39;s rmse: 0.356125
[848]	cv_agg&#39;s auc: 0.825836	cv_agg&#39;s rmse: 0.356109
[849]	cv_agg&#39;s auc: 0.825843	cv_agg&#39;s rmse: 0.356108
[850]	cv_agg&#39;s auc: 0.825846	cv_agg&#39;s rmse: 0.356102
[851]	cv_agg&#39;s auc: 0.825873	cv_agg&#39;s rmse: 0.356094
[852]	cv_agg&#39;s auc: 0.825888	cv_agg&#39;s rmse: 0.356088
[853]	cv_agg&#39;s auc: 0.825901	cv_agg&#39;s rmse: 0.356086
[854]	cv_agg&#39;s auc: 0.825929	cv_agg&#39;s rmse: 0.356076
[855]	cv_agg&#39;s auc: 0.825949	cv_agg&#39;s rmse: 0.356071
[856]	cv_agg&#39;s auc: 0.825964	cv_agg&#39;s rmse: 0.356067
[857]	cv_agg&#39;s auc: 0.825981	cv_agg&#39;s rmse: 0.356058
[858]	cv_agg&#39;s auc: 0.825983	cv_agg&#39;s rmse: 0.356059
[859]	cv_agg&#39;s auc: 0.826013	cv_agg&#39;s rmse: 0.356045
[860]	cv_agg&#39;s auc: 0.826038	cv_agg&#39;s rmse: 0.356039
[861]	cv_agg&#39;s auc: 0.82607	cv_agg&#39;s rmse: 0.356027
[862]	cv_agg&#39;s auc: 0.826086	cv_agg&#39;s rmse: 0.356018
[863]	cv_agg&#39;s auc: 0.82609	cv_agg&#39;s rmse: 0.356017
[864]	cv_agg&#39;s auc: 0.826083	cv_agg&#39;s rmse: 0.356018
[865]	cv_agg&#39;s auc: 0.826093	cv_agg&#39;s rmse: 0.356012
[866]	cv_agg&#39;s auc: 0.826114	cv_agg&#39;s rmse: 0.356002
[867]	cv_agg&#39;s auc: 0.826104	cv_agg&#39;s rmse: 0.356003
[868]	cv_agg&#39;s auc: 0.826128	cv_agg&#39;s rmse: 0.355993
[869]	cv_agg&#39;s auc: 0.826115	cv_agg&#39;s rmse: 0.355998
[870]	cv_agg&#39;s auc: 0.826144	cv_agg&#39;s rmse: 0.355985
[871]	cv_agg&#39;s auc: 0.826149	cv_agg&#39;s rmse: 0.355977
[872]	cv_agg&#39;s auc: 0.826176	cv_agg&#39;s rmse: 0.355969
[873]	cv_agg&#39;s auc: 0.826182	cv_agg&#39;s rmse: 0.355971
[874]	cv_agg&#39;s auc: 0.826178	cv_agg&#39;s rmse: 0.35597
[875]	cv_agg&#39;s auc: 0.826174	cv_agg&#39;s rmse: 0.355972
[876]	cv_agg&#39;s auc: 0.82616	cv_agg&#39;s rmse: 0.355977
[877]	cv_agg&#39;s auc: 0.826164	cv_agg&#39;s rmse: 0.355975
[878]	cv_agg&#39;s auc: 0.826217	cv_agg&#39;s rmse: 0.355956
[879]	cv_agg&#39;s auc: 0.826236	cv_agg&#39;s rmse: 0.355951
[880]	cv_agg&#39;s auc: 0.826232	cv_agg&#39;s rmse: 0.355951
[881]	cv_agg&#39;s auc: 0.826245	cv_agg&#39;s rmse: 0.355945
[882]	cv_agg&#39;s auc: 0.826245	cv_agg&#39;s rmse: 0.355938
[883]	cv_agg&#39;s auc: 0.826251	cv_agg&#39;s rmse: 0.355939
[884]	cv_agg&#39;s auc: 0.826251	cv_agg&#39;s rmse: 0.355939
[885]	cv_agg&#39;s auc: 0.826378	cv_agg&#39;s rmse: 0.35589
[886]	cv_agg&#39;s auc: 0.826361	cv_agg&#39;s rmse: 0.355894
[887]	cv_agg&#39;s auc: 0.826337	cv_agg&#39;s rmse: 0.3559
[888]	cv_agg&#39;s auc: 0.82634	cv_agg&#39;s rmse: 0.355896
[889]	cv_agg&#39;s auc: 0.826351	cv_agg&#39;s rmse: 0.355895
[890]	cv_agg&#39;s auc: 0.82637	cv_agg&#39;s rmse: 0.355891
[891]	cv_agg&#39;s auc: 0.82637	cv_agg&#39;s rmse: 0.355887
[892]	cv_agg&#39;s auc: 0.826368	cv_agg&#39;s rmse: 0.355889
[893]	cv_agg&#39;s auc: 0.826371	cv_agg&#39;s rmse: 0.355884
[894]	cv_agg&#39;s auc: 0.826363	cv_agg&#39;s rmse: 0.355887
[895]	cv_agg&#39;s auc: 0.826406	cv_agg&#39;s rmse: 0.355875
[896]	cv_agg&#39;s auc: 0.82641	cv_agg&#39;s rmse: 0.35587
[897]	cv_agg&#39;s auc: 0.826423	cv_agg&#39;s rmse: 0.35587
[898]	cv_agg&#39;s auc: 0.826414	cv_agg&#39;s rmse: 0.355873
[899]	cv_agg&#39;s auc: 0.826393	cv_agg&#39;s rmse: 0.355875
[900]	cv_agg&#39;s auc: 0.826383	cv_agg&#39;s rmse: 0.355875
[901]	cv_agg&#39;s auc: 0.826381	cv_agg&#39;s rmse: 0.355875
[902]	cv_agg&#39;s auc: 0.82638	cv_agg&#39;s rmse: 0.355873
[903]	cv_agg&#39;s auc: 0.826377	cv_agg&#39;s rmse: 0.355871
[904]	cv_agg&#39;s auc: 0.826375	cv_agg&#39;s rmse: 0.355873
[905]	cv_agg&#39;s auc: 0.826409	cv_agg&#39;s rmse: 0.355856
[906]	cv_agg&#39;s auc: 0.826448	cv_agg&#39;s rmse: 0.355835
[907]	cv_agg&#39;s auc: 0.826525	cv_agg&#39;s rmse: 0.355805
[908]	cv_agg&#39;s auc: 0.826528	cv_agg&#39;s rmse: 0.355804
[909]	cv_agg&#39;s auc: 0.826525	cv_agg&#39;s rmse: 0.355803
[910]	cv_agg&#39;s auc: 0.826556	cv_agg&#39;s rmse: 0.355787
[911]	cv_agg&#39;s auc: 0.826551	cv_agg&#39;s rmse: 0.355782
[912]	cv_agg&#39;s auc: 0.826546	cv_agg&#39;s rmse: 0.355786
[913]	cv_agg&#39;s auc: 0.826534	cv_agg&#39;s rmse: 0.355785
[914]	cv_agg&#39;s auc: 0.826527	cv_agg&#39;s rmse: 0.355788
[915]	cv_agg&#39;s auc: 0.826506	cv_agg&#39;s rmse: 0.355791
[916]	cv_agg&#39;s auc: 0.826586	cv_agg&#39;s rmse: 0.35575
[917]	cv_agg&#39;s auc: 0.826596	cv_agg&#39;s rmse: 0.355747
[918]	cv_agg&#39;s auc: 0.826585	cv_agg&#39;s rmse: 0.355751
[919]	cv_agg&#39;s auc: 0.826587	cv_agg&#39;s rmse: 0.355755
[920]	cv_agg&#39;s auc: 0.826594	cv_agg&#39;s rmse: 0.355751
[921]	cv_agg&#39;s auc: 0.82661	cv_agg&#39;s rmse: 0.355741
[922]	cv_agg&#39;s auc: 0.826607	cv_agg&#39;s rmse: 0.355738
[923]	cv_agg&#39;s auc: 0.82661	cv_agg&#39;s rmse: 0.355737
[924]	cv_agg&#39;s auc: 0.826604	cv_agg&#39;s rmse: 0.35574
[925]	cv_agg&#39;s auc: 0.826635	cv_agg&#39;s rmse: 0.355732
[926]	cv_agg&#39;s auc: 0.826633	cv_agg&#39;s rmse: 0.355729
[927]	cv_agg&#39;s auc: 0.826612	cv_agg&#39;s rmse: 0.355735
[928]	cv_agg&#39;s auc: 0.826636	cv_agg&#39;s rmse: 0.35572
[929]	cv_agg&#39;s auc: 0.82663	cv_agg&#39;s rmse: 0.355724
[930]	cv_agg&#39;s auc: 0.826643	cv_agg&#39;s rmse: 0.355713
[931]	cv_agg&#39;s auc: 0.826724	cv_agg&#39;s rmse: 0.355679
[932]	cv_agg&#39;s auc: 0.826732	cv_agg&#39;s rmse: 0.355674
[933]	cv_agg&#39;s auc: 0.826731	cv_agg&#39;s rmse: 0.355675
[934]	cv_agg&#39;s auc: 0.826773	cv_agg&#39;s rmse: 0.355664
[935]	cv_agg&#39;s auc: 0.826786	cv_agg&#39;s rmse: 0.355659
[936]	cv_agg&#39;s auc: 0.826787	cv_agg&#39;s rmse: 0.355656
[937]	cv_agg&#39;s auc: 0.826804	cv_agg&#39;s rmse: 0.355645
[938]	cv_agg&#39;s auc: 0.826791	cv_agg&#39;s rmse: 0.355649
[939]	cv_agg&#39;s auc: 0.826837	cv_agg&#39;s rmse: 0.355629
[940]	cv_agg&#39;s auc: 0.826861	cv_agg&#39;s rmse: 0.355621
[941]	cv_agg&#39;s auc: 0.82685	cv_agg&#39;s rmse: 0.355623
[942]	cv_agg&#39;s auc: 0.826846	cv_agg&#39;s rmse: 0.355621
[943]	cv_agg&#39;s auc: 0.826848	cv_agg&#39;s rmse: 0.355619
[944]	cv_agg&#39;s auc: 0.826837	cv_agg&#39;s rmse: 0.355624
[945]	cv_agg&#39;s auc: 0.826828	cv_agg&#39;s rmse: 0.35563
[946]	cv_agg&#39;s auc: 0.826827	cv_agg&#39;s rmse: 0.355626
[947]	cv_agg&#39;s auc: 0.826817	cv_agg&#39;s rmse: 0.355629
[948]	cv_agg&#39;s auc: 0.826822	cv_agg&#39;s rmse: 0.355624
[949]	cv_agg&#39;s auc: 0.826847	cv_agg&#39;s rmse: 0.355619
[950]	cv_agg&#39;s auc: 0.826834	cv_agg&#39;s rmse: 0.355622
[951]	cv_agg&#39;s auc: 0.826853	cv_agg&#39;s rmse: 0.355614
[952]	cv_agg&#39;s auc: 0.826854	cv_agg&#39;s rmse: 0.355609
[953]	cv_agg&#39;s auc: 0.826815	cv_agg&#39;s rmse: 0.355618
[954]	cv_agg&#39;s auc: 0.826805	cv_agg&#39;s rmse: 0.355617
[955]	cv_agg&#39;s auc: 0.826788	cv_agg&#39;s rmse: 0.355623
[956]	cv_agg&#39;s auc: 0.826811	cv_agg&#39;s rmse: 0.355611
[957]	cv_agg&#39;s auc: 0.826819	cv_agg&#39;s rmse: 0.35561
[958]	cv_agg&#39;s auc: 0.826809	cv_agg&#39;s rmse: 0.355609
[959]	cv_agg&#39;s auc: 0.826832	cv_agg&#39;s rmse: 0.355598
[960]	cv_agg&#39;s auc: 0.826809	cv_agg&#39;s rmse: 0.355606
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Set number of boost rds w/ CV result</span>
<span class="n">cv_result</span><span class="p">[[</span><span class="s1">&#39;train-rmse-mean&#39;</span><span class="p">,</span> <span class="s1">&#39;test-rmse-mean&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="k">if</span> <span class="n">cv_result</span><span class="p">:</span>
    <span class="n">rds</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv_result</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;num rds from cv = </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rds</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">rds</span> <span class="o">=</span> <span class="mi">940</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;rds manually set to </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">rds</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># See if we can get away with 63 bins (much faster than 255).</span>
<span class="n">lgb_params_2</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s1">&#39;task&#39;</span><span class="p">:</span> <span class="s1">&#39;train&#39;</span><span class="p">,</span>
    <span class="s1">&#39;boosting_type&#39;</span><span class="p">:</span> <span class="s1">&#39;gbdt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;objective&#39;</span><span class="p">:</span> <span class="s1">&#39;binary&#39;</span><span class="p">,</span>
    <span class="s1">&#39;metric&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;rmse&#39;</span><span class="p">,</span> <span class="s1">&#39;auc&#39;</span><span class="p">],</span>
    <span class="s1">&#39;num_leaves&#39;</span><span class="p">:</span> <span class="mi">31</span><span class="p">,</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;max_bin&#39;</span><span class="p">:</span><span class="mi">63</span><span class="p">,</span>
    <span class="s1">&#39;min_split_gain&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;min_child_weight&#39;</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span>
    <span class="s1">&#39;min_child_samples&#39;</span><span class="p">:</span><span class="mi">30</span><span class="p">,</span>
    <span class="s1">&#39;subsample&#39;</span><span class="p">:</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;subsample_freq&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:</span><span class="o">.</span><span class="mi">8</span><span class="p">,</span>
    <span class="s1">&#39;reg_alpha&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;reg_lambda&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;seed&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;nthread&#39;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;verbose&#39;</span><span class="p">:</span> <span class="mi">20</span>
<span class="p">}</span>

<span class="n">cv_result</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
<span class="n">lgb_params_2</span><span class="p">,</span> 
<span class="n">lgb_train</span><span class="p">,</span> 
<span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">num_boost_round</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> 
<span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="n">verbose_eval</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
<span class="n">show_stdv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="n">seed</span><span class="o">=</span><span class="mi">2001</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Num rds from 10-fold CV, lrate =.01 == </span><span class="si">%i</span><span class="s1">.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv_result</span><span class="p">[</span><span class="s1">&#39;auc-mean&#39;</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[20]	cv_agg&#39;s auc: 0.761433	cv_agg&#39;s rmse: 0.389428
[40]	cv_agg&#39;s auc: 0.775898	cv_agg&#39;s rmse: 0.379144
[60]	cv_agg&#39;s auc: 0.785548	cv_agg&#39;s rmse: 0.374442
[80]	cv_agg&#39;s auc: 0.791878	cv_agg&#39;s rmse: 0.371547
[100]	cv_agg&#39;s auc: 0.795907	cv_agg&#39;s rmse: 0.369703
[120]	cv_agg&#39;s auc: 0.798611	cv_agg&#39;s rmse: 0.368399
[140]	cv_agg&#39;s auc: 0.80139	cv_agg&#39;s rmse: 0.36722
[160]	cv_agg&#39;s auc: 0.803493	cv_agg&#39;s rmse: 0.366354
[180]	cv_agg&#39;s auc: 0.805466	cv_agg&#39;s rmse: 0.36556
[200]	cv_agg&#39;s auc: 0.80707	cv_agg&#39;s rmse: 0.364862
[220]	cv_agg&#39;s auc: 0.808439	cv_agg&#39;s rmse: 0.36427
[240]	cv_agg&#39;s auc: 0.809595	cv_agg&#39;s rmse: 0.363733
[260]	cv_agg&#39;s auc: 0.810681	cv_agg&#39;s rmse: 0.363239
[280]	cv_agg&#39;s auc: 0.81216	cv_agg&#39;s rmse: 0.362589
[300]	cv_agg&#39;s auc: 0.812937	cv_agg&#39;s rmse: 0.362215
[320]	cv_agg&#39;s auc: 0.813569	cv_agg&#39;s rmse: 0.361865
[340]	cv_agg&#39;s auc: 0.81462	cv_agg&#39;s rmse: 0.361418
[360]	cv_agg&#39;s auc: 0.815128	cv_agg&#39;s rmse: 0.361115
[380]	cv_agg&#39;s auc: 0.815883	cv_agg&#39;s rmse: 0.360751
[400]	cv_agg&#39;s auc: 0.816628	cv_agg&#39;s rmse: 0.360438
[420]	cv_agg&#39;s auc: 0.81734	cv_agg&#39;s rmse: 0.360138
[440]	cv_agg&#39;s auc: 0.817953	cv_agg&#39;s rmse: 0.359866
[460]	cv_agg&#39;s auc: 0.818488	cv_agg&#39;s rmse: 0.359617
[480]	cv_agg&#39;s auc: 0.81919	cv_agg&#39;s rmse: 0.359317
[500]	cv_agg&#39;s auc: 0.819622	cv_agg&#39;s rmse: 0.359098
[520]	cv_agg&#39;s auc: 0.820233	cv_agg&#39;s rmse: 0.358825
[540]	cv_agg&#39;s auc: 0.82101	cv_agg&#39;s rmse: 0.358511
[560]	cv_agg&#39;s auc: 0.821324	cv_agg&#39;s rmse: 0.358354
[580]	cv_agg&#39;s auc: 0.821852	cv_agg&#39;s rmse: 0.358118
[600]	cv_agg&#39;s auc: 0.822155	cv_agg&#39;s rmse: 0.357963
[620]	cv_agg&#39;s auc: 0.822691	cv_agg&#39;s rmse: 0.357705
[640]	cv_agg&#39;s auc: 0.823137	cv_agg&#39;s rmse: 0.357497
[660]	cv_agg&#39;s auc: 0.823355	cv_agg&#39;s rmse: 0.35738
[680]	cv_agg&#39;s auc: 0.823729	cv_agg&#39;s rmse: 0.357228
[700]	cv_agg&#39;s auc: 0.824046	cv_agg&#39;s rmse: 0.357061
[720]	cv_agg&#39;s auc: 0.824334	cv_agg&#39;s rmse: 0.356913
[740]	cv_agg&#39;s auc: 0.824583	cv_agg&#39;s rmse: 0.356787
[760]	cv_agg&#39;s auc: 0.824835	cv_agg&#39;s rmse: 0.356666
[780]	cv_agg&#39;s auc: 0.824885	cv_agg&#39;s rmse: 0.356587
[800]	cv_agg&#39;s auc: 0.825232	cv_agg&#39;s rmse: 0.356414
[820]	cv_agg&#39;s auc: 0.825464	cv_agg&#39;s rmse: 0.356296
[840]	cv_agg&#39;s auc: 0.825798	cv_agg&#39;s rmse: 0.35613
[860]	cv_agg&#39;s auc: 0.826038	cv_agg&#39;s rmse: 0.356039
[880]	cv_agg&#39;s auc: 0.826232	cv_agg&#39;s rmse: 0.355951
[900]	cv_agg&#39;s auc: 0.826383	cv_agg&#39;s rmse: 0.355875
[920]	cv_agg&#39;s auc: 0.826594	cv_agg&#39;s rmse: 0.355751
[940]	cv_agg&#39;s auc: 0.826861	cv_agg&#39;s rmse: 0.355621
[960]	cv_agg&#39;s auc: 0.826809	cv_agg&#39;s rmse: 0.355606
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Check AUC vs. iter</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cv_result</span><span class="p">[</span><span class="s1">&#39;auc-mean&#39;</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Get perf of starter on test set</span>

<span class="n">gbm</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LGBMClassifier</span><span class="p">(</span>
    <span class="n">objective</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span>
    <span class="n">num_leaves</span><span class="o">=</span><span class="mi">31</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">960</span><span class="p">,</span>
    <span class="n">boosting_type</span><span class="o">=</span><span class="s1">&#39;gbdt&#39;</span><span class="p">,</span>
    <span class="n">max_depth</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">max_bin</span><span class="o">=</span><span class="mi">63</span><span class="p">,</span>
    <span class="n">min_split_gain</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">min_child_samples</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">subsample</span><span class="o">=.</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">subsample_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">colsample_bytree</span><span class="o">=.</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">reg_alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">reg_lambda</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">nthread</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">silent</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gbm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[49]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">score_classif_on_test</span><span class="p">(</span><span class="n">gbm</span><span class="p">,</span> <span class="n">X_test_enc</span><span class="p">,</span> <span class="n">Y_test_enc</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>AUC Score:
0.819382956997

LogLoss:
0.41099269183

             precision    recall  f1-score   support

      False      0.827     0.961     0.889     25951
       True      0.736     0.351     0.476      8023

avg / total      0.806     0.817     0.792     33974

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tune LightGBM with random search - far more efficient than grid search.</span>

<span class="n">param_dists</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;num_leaves&#39;</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">63</span><span class="p">),</span>
    <span class="s1">&#39;min_split_gain&#39;</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="p">(</span><span class="n">scale</span><span class="o">=.</span><span class="mi">5</span><span class="p">),</span>
    <span class="s1">&#39;min_child_weight&#39;</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>  <span class="c1"># from 2 to 2+20</span>
    <span class="s1">&#39;min_child_samples&#39;</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
    <span class="s1">&#39;subsample&#39;</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="o">.</span><span class="mi">35</span><span class="p">),</span>
    <span class="c1"># subsample_freq=1,</span>
    <span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">.</span><span class="mi">6</span><span class="p">,</span> <span class="o">.</span><span class="mi">35</span><span class="p">),</span>
    <span class="c1"># reg_alpha=0,</span>
    <span class="s1">&#39;reg_lambda&#39;</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="p">(</span><span class="n">scale</span><span class="o">=.</span><span class="mi">5</span><span class="p">),</span>    
<span class="p">}</span>


<span class="k">def</span> <span class="nf">test_params_random</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_value_dict</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run a grid search for a model using given param values and print results.</span>
<span class="sd">    CV is stratified K-Fold in this instance.</span>
<span class="sd">    (SKL is smart enough to know to prefer stratified CV in classification problems.)</span>
<span class="sd">    Note that for LightGBM we&#39;d be better off using n_jobs = n_threads on processor</span>
<span class="sd">      and setting GBM model&#39;s threads to 1! This is because LightGBM parallelizes</span>
<span class="sd">      by feature, and our dataset doesn&#39;t have very many cols.</span>
<span class="sd">    However, for simplicity, I&#39;ve just left LightGBM at max threads and not parallelized</span>
<span class="sd">      the random search process.</span>
<span class="sd">    </span>
<span class="sd">    :param model: SciKit-Learn model, i.e. one that has </span>
<span class="sd">      .fit() and .predict() methods</span>
<span class="sd">    :param param_value_dict: dict, with keys of param names </span>
<span class="sd">      and values of [values] to try for given param</span>
<span class="sd">    :return: None, prints results to stdout</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># First, convert ROC_AUC from SKL metric to scorer API.</span>
    <span class="n">roc_scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">)</span>
    <span class="c1"># Init RandomizedSearchCV instance.</span>
    <span class="n">rsearch</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span>
        <span class="n">gbm</span><span class="p">,</span> 
        <span class="n">param_dists</span><span class="p">,</span> 
        <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter</span><span class="p">,</span> 
        <span class="n">scoring</span><span class="o">=</span><span class="n">roc_scorer</span><span class="p">,</span> 
        <span class="n">fit_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
        <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">pre_dispatch</span><span class="o">=</span><span class="s1">&#39;n_jobs&#39;</span><span class="p">,</span> 
        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>
    <span class="n">rsearch</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Raw grid search scores:&#39;</span><span class="p">)</span>
        <span class="n">rsearch</span><span class="o">.</span><span class="n">grid_scores_</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best params for this search:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">rsearch</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best AUC for these params:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">rsearch</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test_params_random</span><span class="p">(</span><span class="n">gbm</span><span class="p">,</span> <span class="n">param_dists</span><span class="p">,</span> <span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Best params for this search:
{&#39;colsample_bytree&#39;: 0.63379236343961032, &#39;min_child_samples&#39;: 43, &#39;min_child_weight&#39;: 4.6296559858225521, &#39;min_split_gain&#39;: 0.028483268112510837, &#39;num_leaves&#39;: 60, &#39;reg_lambda&#39;: 0.3346514335803939, &#39;subsample&#39;: 0.76048009328282773}
Best AUC for these params:
0.656695475342
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Try out params from above.</span>
<span class="c1"># Were our ranges reasonable, or will we do worse than defaults?</span>
<span class="n">lgb_params_2</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s1">&#39;task&#39;</span><span class="p">:</span> <span class="s1">&#39;train&#39;</span><span class="p">,</span>
    <span class="s1">&#39;boosting_type&#39;</span><span class="p">:</span> <span class="s1">&#39;gbdt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;objective&#39;</span><span class="p">:</span> <span class="s1">&#39;binary&#39;</span><span class="p">,</span>
    <span class="s1">&#39;metric&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;rmse&#39;</span><span class="p">,</span> <span class="s1">&#39;auc&#39;</span><span class="p">],</span>
    <span class="s1">&#39;num_leaves&#39;</span><span class="p">:</span> <span class="mi">60</span><span class="p">,</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;max_bin&#39;</span><span class="p">:</span><span class="mi">63</span><span class="p">,</span>
    <span class="s1">&#39;min_split_gain&#39;</span><span class="p">:</span><span class="mf">0.28</span><span class="p">,</span>
    <span class="s1">&#39;min_child_weight&#39;</span><span class="p">:</span><span class="mf">4.6</span><span class="p">,</span>
    <span class="s1">&#39;min_child_samples&#39;</span><span class="p">:</span><span class="mi">43</span><span class="p">,</span>
    <span class="s1">&#39;subsample&#39;</span><span class="p">:</span><span class="o">.</span><span class="mi">76</span><span class="p">,</span>
    <span class="s1">&#39;subsample_freq&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:</span><span class="o">.</span><span class="mi">63</span><span class="p">,</span>
    <span class="s1">&#39;reg_alpha&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;reg_lambda&#39;</span><span class="p">:</span><span class="mf">0.33</span><span class="p">,</span>
    <span class="s1">&#39;seed&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;nthread&#39;</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;verbose&#39;</span><span class="p">:</span> <span class="mi">20</span>
<span class="p">}</span>

<span class="n">cv_result</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
<span class="n">lgb_params_2</span><span class="p">,</span> 
<span class="n">lgb_train</span><span class="p">,</span> 
<span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">num_boost_round</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> 
<span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="n">verbose_eval</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
<span class="n">show_stdv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="n">seed</span><span class="o">=</span><span class="mi">2001</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Num rds from 10-fold CV, lrate =.01 == </span><span class="si">%i</span><span class="s1">.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv_result</span><span class="p">[</span><span class="s1">&#39;auc-mean&#39;</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[20]	cv_agg&#39;s auc: 0.770988	cv_agg&#39;s rmse: 0.387766
[40]	cv_agg&#39;s auc: 0.785937	cv_agg&#39;s rmse: 0.375396
[60]	cv_agg&#39;s auc: 0.793637	cv_agg&#39;s rmse: 0.370853
[80]	cv_agg&#39;s auc: 0.798058	cv_agg&#39;s rmse: 0.368747
[100]	cv_agg&#39;s auc: 0.801501	cv_agg&#39;s rmse: 0.367319
[120]	cv_agg&#39;s auc: 0.802906	cv_agg&#39;s rmse: 0.366443
[140]	cv_agg&#39;s auc: 0.804429	cv_agg&#39;s rmse: 0.36571
[160]	cv_agg&#39;s auc: 0.805322	cv_agg&#39;s rmse: 0.365298
[180]	cv_agg&#39;s auc: 0.806251	cv_agg&#39;s rmse: 0.364898
[200]	cv_agg&#39;s auc: 0.807183	cv_agg&#39;s rmse: 0.3645
[220]	cv_agg&#39;s auc: 0.807568	cv_agg&#39;s rmse: 0.364272
[240]	cv_agg&#39;s auc: 0.808284	cv_agg&#39;s rmse: 0.36395
[260]	cv_agg&#39;s auc: 0.809206	cv_agg&#39;s rmse: 0.363571
[280]	cv_agg&#39;s auc: 0.809977	cv_agg&#39;s rmse: 0.363245
[300]	cv_agg&#39;s auc: 0.810512	cv_agg&#39;s rmse: 0.36298
[320]	cv_agg&#39;s auc: 0.811113	cv_agg&#39;s rmse: 0.362682
[340]	cv_agg&#39;s auc: 0.811729	cv_agg&#39;s rmse: 0.3624
[360]	cv_agg&#39;s auc: 0.811852	cv_agg&#39;s rmse: 0.362323
[380]	cv_agg&#39;s auc: 0.812225	cv_agg&#39;s rmse: 0.362127
[400]	cv_agg&#39;s auc: 0.812532	cv_agg&#39;s rmse: 0.361905
[420]	cv_agg&#39;s auc: 0.812881	cv_agg&#39;s rmse: 0.36173
[440]	cv_agg&#39;s auc: 0.813456	cv_agg&#39;s rmse: 0.361463
[460]	cv_agg&#39;s auc: 0.813712	cv_agg&#39;s rmse: 0.36133
[480]	cv_agg&#39;s auc: 0.813767	cv_agg&#39;s rmse: 0.36128
[500]	cv_agg&#39;s auc: 0.814278	cv_agg&#39;s rmse: 0.361092
[520]	cv_agg&#39;s auc: 0.814397	cv_agg&#39;s rmse: 0.36101
[540]	cv_agg&#39;s auc: 0.814612	cv_agg&#39;s rmse: 0.360894
[560]	cv_agg&#39;s auc: 0.814664	cv_agg&#39;s rmse: 0.360821
[580]	cv_agg&#39;s auc: 0.815021	cv_agg&#39;s rmse: 0.360656
[600]	cv_agg&#39;s auc: 0.815085	cv_agg&#39;s rmse: 0.360606
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Check AUC vs. iter</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cv_result</span><span class="p">[</span><span class="s1">&#39;auc-mean&#39;</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Best params for this search:
{&#39;colsample_bytree&#39;: 0.72330908142018657, &#39;min_child_samples&#39;: 69, &#39;min_child_weight&#39;: 21.976940131357331, &#39;min_split_gain&#39;: 0.080935043349130792, &#39;num_leaves&#39;: 62, &#39;reg_lambda&#39;: 0.019141456103673956, &#39;subsample&#39;: 0.77791040700261171}
Best AUC for these params:
0.651684753249
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Raw grid search scores:
Best params for this search:
{&#39;colsample_bytree&#39;: 0.73011936988780124, &#39;min_child_samples&#39;: 34, &#39;min_child_weight&#39;: 6.7309770347573288, &#39;subsample&#39;: 0.75998025533188096}
Best AUC for these params:
0.651641914448
</pre>
</div>
</div>

<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Raw grid search scores:
Best params for this search:
{&#39;min_child_samples&#39;: 24, &#39;num_leaves&#39;: 113}
Best AUC for these params:
0.673458934264
</pre>
</div>
</div>

<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Raw grid search scores:
Best params for this search:
{&#39;min_child_samples&#39;: 41, &#39;min_child_weight&#39;: 6.2363042875116852, &#39;num_leaves&#39;: 123}
Best AUC for these params:
0.674730830726
</pre>
</div>
</div>

<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Raw grid search scores:
Best params for this search:
{&#39;max_depth&#39;: 11, &#39;min_data_in_bin&#39;: 3, &#39;reg_lambda&#39;: 0.1}
Best AUC for these params:
0.672165523869
</pre>
</div>
</div>

<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Tune LightGBM with random search - far more efficient than grid search.</span>

<span class="c1"># Record of random search runs, 30-100 iters each:</span>

<span class="c1"># Best params for this search:</span>
<span class="c1"># {&#39;colsample_bytree&#39;: 0.72330908142018657, &#39;min_child_samples&#39;: 69, &#39;min_child_weight&#39;: 21.976940131357331, &#39;min_split_gain&#39;: 0.080935043349130792, &#39;num_leaves&#39;: 62, &#39;reg_lambda&#39;: 0.019141456103673956, &#39;subsample&#39;: 0.77791040700261171}</span>
<span class="c1"># Best AUC for these params:</span>
<span class="c1"># 0.651684753249</span>

<span class="c1"># ---</span>

<span class="c1"># Raw grid search scores:</span>
<span class="c1"># Best params for this search:</span>
<span class="c1"># {&#39;colsample_bytree&#39;: 0.73011936988780124, &#39;min_child_samples&#39;: 34, &#39;min_child_weight&#39;: 6.7309770347573288, &#39;subsample&#39;: 0.75998025533188096}</span>
<span class="c1"># Best AUC for these params:</span>
<span class="c1"># 0.651641914448</span>

<span class="c1"># ---</span>

<span class="c1"># Raw grid search scores:</span>
<span class="c1"># Best params for this search:</span>
<span class="c1"># {&#39;min_child_samples&#39;: 24, &#39;num_leaves&#39;: 113}</span>
<span class="c1"># Best AUC for these params:</span>
<span class="c1"># 0.673458934264</span>

<span class="c1"># Raw grid search scores:</span>
<span class="c1"># Best params for this search:</span>
<span class="c1"># {&#39;min_child_samples&#39;: 41, &#39;min_child_weight&#39;: 6.2363042875116852, &#39;num_leaves&#39;: 123}</span>
<span class="c1"># Best AUC for these params:</span>
<span class="c1"># 0.674730830726</span>

<span class="c1"># ---</span>

<span class="c1"># Use entire sample now.</span>
<span class="c1"># Try finding if ANY L2 reg is desirable.</span>
<span class="c1"># Also tune min_data_in_bin (how many obs must be in each of 63 bins per feature)</span>
<span class="c1"># Also tune max_depth (though may just be -1 = unlimited, and we&#39;re already limited by num_leaves)</span>

<span class="c1"># Raw grid search scores:</span>
<span class="c1"># Best params for this search:</span>
<span class="c1"># {&#39;max_depth&#39;: 11, &#39;min_data_in_bin&#39;: 3, &#39;reg_lambda&#39;: 0.1}</span>
<span class="c1"># Best AUC for these params:</span>
<span class="c1"># 0.672165523869</span>

<span class="c1"># ---</span>

<span class="c1"># Clearly want lower number of min_data_in_bin, high max_depth, and little-to-no l2 reg.</span>
<span class="c1"># Makes sense - have lots of data -&gt; hard to overfit</span>
<span class="c1"># Fine-tune</span>

<span class="n">param_dists</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c1"># &#39;num_leaves&#39;: scipy.stats.randint(100, 127),</span>
    <span class="c1"># &#39;min_child_weight&#39;: scipy.stats.uniform(2, 40),  # from 2 to 2+10</span>
    <span class="c1"># &#39;min_child_samples&#39;: scipy.stats.randint(10, 80),</span>
    <span class="c1">#&#39;subsample&#39;: scipy.stats.uniform(.6, .25),</span>
    <span class="c1"># subsample_freq=1,</span>
    <span class="c1"># &#39;colsample_bytree&#39;: scipy.stats.uniform(.6, .25),</span>
    <span class="c1"># reg_alpha=0,</span>
    <span class="s1">&#39;reg_lambda&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">,</span> <span class="o">.</span><span class="mi">25</span><span class="p">,</span> <span class="o">.</span><span class="mi">30</span><span class="p">,</span> <span class="o">.</span><span class="mi">40</span><span class="p">],</span>  <span class="c1"># try using very small amt of L2 reg</span>
    <span class="s1">&#39;min_data_in_bin&#39;</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> 
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">test_params_random</span><span class="p">(</span><span class="n">gbm</span><span class="p">,</span> <span class="n">param_dists</span><span class="p">,</span> <span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>



<span class="c1"># first run:</span>
<span class="c1"># Raw grid search scores:</span>
<span class="c1"># Best params for this search:</span>
<span class="c1"># {&#39;max_depth&#39;: 15, &#39;min_data_in_bin&#39;: 1, &#39;reg_lambda&#39;: 0.2}</span>
<span class="c1"># Best AUC for these params:</span>
<span class="c1"># 0.674529497263</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Raw grid search scores:
Best params for this search:
{&#39;max_depth&#39;: 17, &#39;min_data_in_bin&#39;: 1, &#39;reg_lambda&#39;: 0.1}
Best AUC for these params:
0.674993629946
</pre>
</div>
</div>

<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/jan/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20
  DeprecationWarning)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># TODO: CLEAN UP above random search :)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[46]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Now that params are tuned, let&#39;s see how well we can do on CV </span>
<span class="c1"># using a lower lrate of .01 (from .1).</span>
<span class="c1"># New seed so we aren&#39;t overfitting on the particular slices from param optimization.</span>

<span class="n">params3</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;objective&#39;</span><span class="p">:</span> <span class="s1">&#39;binary&#39;</span><span class="p">,</span>
    <span class="s1">&#39;num_leaves&#39;</span><span class="p">:</span> <span class="mi">127</span><span class="p">,</span>
    <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span>
    <span class="s1">&#39;boosting_type&#39;</span><span class="p">:</span> <span class="s1">&#39;gbdt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="mi">17</span><span class="p">,</span>
    <span class="s1">&#39;max_bin&#39;</span><span class="p">:</span> <span class="mi">63</span><span class="p">,</span>
    <span class="s1">&#39;min_split_gain&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;min_child_weight&#39;</span><span class="p">:</span> <span class="mf">6.5</span><span class="p">,</span>
    <span class="s1">&#39;min_child_samples&#39;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s1">&#39;subsample&#39;</span><span class="p">:</span> <span class="o">.</span><span class="mi">76</span><span class="p">,</span>
    <span class="s1">&#39;subsample_freq&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:</span> <span class="o">.</span><span class="mi">73</span><span class="p">,</span>
    <span class="s1">&#39;bin_construct_sample_cnt&#39;</span><span class="p">:</span> <span class="mi">999999</span><span class="p">,</span> <span class="c1"># use entire dataset</span>
    <span class="s1">&#39;min_data_in_bin&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;reg_alpha&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;reg_lambda&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s1">&#39;seed&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;nthread&#39;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">cv_result</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
<span class="n">params3</span><span class="p">,</span> 
<span class="n">lgb_train</span><span class="p">,</span> 
<span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="c1"># stratified=True,</span>
<span class="n">num_boost_round</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> 
<span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;auc&#39;</span><span class="p">,</span><span class="s1">&#39;rmse&#39;</span><span class="p">],</span>
<span class="n">verbose_eval</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
<span class="n">show_stdv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="n">seed</span><span class="o">=</span><span class="mi">2002</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Num rds from 10-fold CV, lrate =.01 == </span><span class="si">%i</span><span class="s1">.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv_result</span><span class="p">[</span><span class="s1">&#39;auc-mean&#39;</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[100]	cv_agg&#39;s rmse: 0.402503	cv_agg&#39;s auc: 0.78138
[200]	cv_agg&#39;s rmse: 0.380142	cv_agg&#39;s auc: 0.789801
[300]	cv_agg&#39;s rmse: 0.372706	cv_agg&#39;s auc: 0.797537
[400]	cv_agg&#39;s rmse: 0.368524	cv_agg&#39;s auc: 0.80358
[500]	cv_agg&#39;s rmse: 0.365974	cv_agg&#39;s auc: 0.80826
[600]	cv_agg&#39;s rmse: 0.364149	cv_agg&#39;s auc: 0.811478
[700]	cv_agg&#39;s rmse: 0.362806	cv_agg&#39;s auc: 0.814102
[800]	cv_agg&#39;s rmse: 0.361837	cv_agg&#39;s auc: 0.815976
[900]	cv_agg&#39;s rmse: 0.360906	cv_agg&#39;s auc: 0.817905
[1000]	cv_agg&#39;s rmse: 0.36018	cv_agg&#39;s auc: 0.819351
[1100]	cv_agg&#39;s rmse: 0.359573	cv_agg&#39;s auc: 0.820524
[1200]	cv_agg&#39;s rmse: 0.359019	cv_agg&#39;s auc: 0.821602
[1300]	cv_agg&#39;s rmse: 0.358566	cv_agg&#39;s auc: 0.822431
[1400]	cv_agg&#39;s rmse: 0.357991	cv_agg&#39;s auc: 0.823639
[1500]	cv_agg&#39;s rmse: 0.357601	cv_agg&#39;s auc: 0.824364
[1600]	cv_agg&#39;s rmse: 0.357254	cv_agg&#39;s auc: 0.824962
[1700]	cv_agg&#39;s rmse: 0.356817	cv_agg&#39;s auc: 0.825834
[1800]	cv_agg&#39;s rmse: 0.356524	cv_agg&#39;s auc: 0.82634
[1900]	cv_agg&#39;s rmse: 0.356223	cv_agg&#39;s auc: 0.826907
[2000]	cv_agg&#39;s rmse: 0.355986	cv_agg&#39;s auc: 0.827308
[2100]	cv_agg&#39;s rmse: 0.355693	cv_agg&#39;s auc: 0.827866
[2200]	cv_agg&#39;s rmse: 0.355461	cv_agg&#39;s auc: 0.828257
[2300]	cv_agg&#39;s rmse: 0.355234	cv_agg&#39;s auc: 0.828636
[2400]	cv_agg&#39;s rmse: 0.355097	cv_agg&#39;s auc: 0.828812
[2500]	cv_agg&#39;s rmse: 0.354891	cv_agg&#39;s auc: 0.829164
[2600]	cv_agg&#39;s rmse: 0.354721	cv_agg&#39;s auc: 0.829447
[2700]	cv_agg&#39;s rmse: 0.354519	cv_agg&#39;s auc: 0.829806
Num rds from 10-fold CV, lrate =.01 == 2778.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Now w/ stratified cv</span>

<span class="n">cv_result</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
<span class="n">params3</span><span class="p">,</span> 
<span class="n">lgb_train</span><span class="p">,</span> 
<span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">stratified</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="n">num_boost_round</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> 
<span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;auc&#39;</span><span class="p">,</span><span class="s1">&#39;rmse&#39;</span><span class="p">],</span>
<span class="n">verbose_eval</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
<span class="n">show_stdv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="n">seed</span><span class="o">=</span><span class="mi">2002</span>
<span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>[100]	cv_agg&#39;s rmse: 0.402457	cv_agg&#39;s auc: 0.781738
[200]	cv_agg&#39;s rmse: 0.380053	cv_agg&#39;s auc: 0.79018
[300]	cv_agg&#39;s rmse: 0.372578	cv_agg&#39;s auc: 0.798026
[400]	cv_agg&#39;s rmse: 0.368402	cv_agg&#39;s auc: 0.803908
[500]	cv_agg&#39;s rmse: 0.365867	cv_agg&#39;s auc: 0.808466
[600]	cv_agg&#39;s rmse: 0.36403	cv_agg&#39;s auc: 0.811831
[700]	cv_agg&#39;s rmse: 0.362653	cv_agg&#39;s auc: 0.814554
[800]	cv_agg&#39;s rmse: 0.361719	cv_agg&#39;s auc: 0.816314
[900]	cv_agg&#39;s rmse: 0.360773	cv_agg&#39;s auc: 0.818253
[1000]	cv_agg&#39;s rmse: 0.360013	cv_agg&#39;s auc: 0.819767
[1100]	cv_agg&#39;s rmse: 0.359428	cv_agg&#39;s auc: 0.820871
[1200]	cv_agg&#39;s rmse: 0.358867	cv_agg&#39;s auc: 0.821946
[1300]	cv_agg&#39;s rmse: 0.358408	cv_agg&#39;s auc: 0.822776
[1400]	cv_agg&#39;s rmse: 0.357832	cv_agg&#39;s auc: 0.823953
[1500]	cv_agg&#39;s rmse: 0.357441	cv_agg&#39;s auc: 0.824663
[1600]	cv_agg&#39;s rmse: 0.357081	cv_agg&#39;s auc: 0.825315
[1700]	cv_agg&#39;s rmse: 0.356663	cv_agg&#39;s auc: 0.826133
[1800]	cv_agg&#39;s rmse: 0.356355	cv_agg&#39;s auc: 0.826681
[1900]	cv_agg&#39;s rmse: 0.356038	cv_agg&#39;s auc: 0.827234
[2000]	cv_agg&#39;s rmse: 0.355757	cv_agg&#39;s auc: 0.827749
[2100]	cv_agg&#39;s rmse: 0.355473	cv_agg&#39;s auc: 0.828281
{&#39;rmse-mean&#39;: [0.49802825097685866, 0.49624278603207123, 0.4944831774101292, 0.49269142440304819, 0.49084047741255388, 0.48910070129757333, 0.487292125662867, 0.4856059376603265, 0.48386268787083131, 0.48228347115526587, 0.48098543776433916, 0.47947297017422885, 0.47787978828883021, 0.47654240184902913, 0.47507615051318741, 0.47346810304873743, 0.47192445071896583, 0.4705270913831372, 0.46907405802935315, 0.46762891685928487, 0.46618674620692274, 0.46474600000545169, 0.46329156924217668, 0.46189875953256132, 0.46057433044776452, 0.45933188464072705, 0.45808430789703103, 0.45679591863686292, 0.45543188784565947, 0.45415816838220352, 0.45302597001101946, 0.45189384027419388, 0.45078999668670683, 0.44956798481837251, 0.4483835752112405, 0.44721702837647248, 0.44607354710389613, 0.44495400289574966, 0.44385942732638711, 0.44274888184337646, 0.44170564300379678, 0.44067540201493288, 0.43963677495104109, 0.43860877224072647, 0.43760635357534267, 0.43660040762370605, 0.43563782455752093, 0.43465797840617071, 0.43372492171492161, 0.43277865537278604, 0.43193888134642167, 0.43104095952423266, 0.43014361550875496, 0.42935762179358627, 0.42845623739119337, 0.42762252204886142, 0.42687389732566705, 0.42607363679650845, 0.42547915972844824, 0.42486775633053658, 0.42407163906829892, 0.42338891173186138, 0.42261933085254738, 0.42186456972977587, 0.42120394772257103, 0.42065359771061778, 0.41995440021992725, 0.41932885607308651, 0.41870699285875579, 0.41809535588726082, 0.41742487961927904, 0.4167433249334036, 0.41608398635551563, 0.41544612982668705, 0.41482241578338153, 0.41427075776055161, 0.41360936892949862, 0.41308654874561557, 0.41249955768315594, 0.411910699119655, 0.41134632278341332, 0.41083831132161996, 0.41035460087618525, 0.40980942667748871, 0.40932433682698949, 0.40886964845868884, 0.40830538896081991, 0.40785408206581258, 0.40734476033348088, 0.40691382346479699, 0.406406869720911, 0.40591472908445869, 0.40549262157016619, 0.40500577630187673, 0.4045184197694125, 0.4041914822939357, 0.40374038983530047, 0.40328168344438992, 0.40283228162921969, 0.40245734269140787, 0.40202190289868217, 0.40162322866129541, 0.40122259005716776, 0.40083330978768306, 0.40041215718271123, 0.39999554011651289, 0.39961345989645453, 0.39920345569310356, 0.39881826118537145, 0.39849135130336083, 0.39823958527330644, 0.39784786786698512, 0.39749831732504831, 0.39713615254700718, 0.39678872009820271, 0.3963934245036318, 0.39605705953872661, 0.39571889118196585, 0.39539727473944908, 0.39506409796872438, 0.3947363616463534, 0.39442710466404785, 0.3941139509535988, 0.39383462157141447, 0.39353509430395156, 0.39315681115045914, 0.39289588994742408, 0.39260851623442461, 0.3923257991191223, 0.39205334521313023, 0.39175771539672805, 0.3915072849977923, 0.39116689920297387, 0.39090083060145675, 0.39064986870874802, 0.3904025344018871, 0.39014963801465141, 0.38993029109156913, 0.38971381726594573, 0.38946885761655853, 0.38923234559524955, 0.38900282526331231, 0.38886228251911842, 0.38866334589671159, 0.38852466159949134, 0.38829519107523713, 0.38806593252594074, 0.38782611145587048, 0.38763671900788571, 0.38741349537772346, 0.38720903204831458, 0.38700493014371518, 0.386833462406036, 0.38664305342512456, 0.38652752817677227, 0.38633270320871987, 0.38613819790981585, 0.38595414359753433, 0.3857908334594663, 0.38562733639843316, 0.38543764398970021, 0.38533662900440185, 0.38516992285105944, 0.38498360884948962, 0.38480541559229225, 0.3846475640533174, 0.38449783324692499, 0.38433711137262278, 0.38418164389957082, 0.38409008595983291, 0.38395346967425975, 0.3837864103270433, 0.38364291932027933, 0.38347784479619162, 0.38334077564503455, 0.38320627604909291, 0.38307360314011241, 0.38290003116711874, 0.38273896032649585, 0.38266505620283492, 0.38248838898787724, 0.38232867122946385, 0.38220099338293306, 0.38205774569691608, 0.3819056961755134, 0.38176437490058124, 0.38164429684870849, 0.38149986540299774, 0.38137191290146788, 0.38124232888862924, 0.38111938261190853, 0.38097100247506599, 0.38083040927596373, 0.38071375714524386, 0.38058582824985471, 0.38046312372724145, 0.38033391382579512, 0.38020348370068513, 0.38010691022632892, 0.38005271903458704, 0.38000305479071578, 0.37989737671721985, 0.37978227408143417, 0.37967873995672907, 0.37957171276581742, 0.37952354461047777, 0.37941301114471704, 0.37929648772627184, 0.37918252245555689, 0.37908092633657064, 0.37897218004831684, 0.37886853851417202, 0.37878146627782155, 0.37866779183187155, 0.37856107602267153, 0.37851710648870501, 0.37840773676187489, 0.37830953240608584, 0.37821379943567968, 0.37817420417538161, 0.37807272667163533, 0.37797874908557227, 0.37788497074148764, 0.37779368380378553, 0.37769822782913054, 0.37766042391638838, 0.37757227061736043, 0.37747754273277712, 0.37738503024244474, 0.37726504364560226, 0.3771756606743914, 0.37707291522482628, 0.37697688109823713, 0.3768901784419374, 0.37680445944369745, 0.37673396483813015, 0.37665451089793434, 0.37657387005201626, 0.37649721682364529, 0.37646563245718984, 0.37634762827496365, 0.37631823759246846, 0.37624007587436004, 0.37616627070135628, 0.37607954819151235, 0.37600327443823317, 0.37592796414841301, 0.37585452546459897, 0.37582537896393337, 0.37568689186056059, 0.37565887493264516, 0.37558769338994236, 0.37550146157178366, 0.37543731792020746, 0.37536785395843297, 0.37529758374276878, 0.37516668431348288, 0.37503735373327485, 0.37496789741768249, 0.37490691545230809, 0.37483493973025772, 0.37476815407136854, 0.37470452253606046, 0.37463790306168099, 0.37455432134103239, 0.3744825636436615, 0.37445884061230073, 0.37439381266158428, 0.37433398770725806, 0.37426317765989708, 0.37420025356680969, 0.37414005306948955, 0.37408094871566266, 0.3740283785063947, 0.37397516229105465, 0.37395227502844064, 0.37386533300363067, 0.37380264827739823, 0.37372060518927269, 0.37366120803552999, 0.3736414033634089, 0.3735560459820928, 0.37350511355455235, 0.3734418984995499, 0.37333627401586411, 0.37327204884458642, 0.37320113208081762, 0.37314207557436563, 0.37308400445292245, 0.37306350959603585, 0.373006320482062, 0.37298785700908488, 0.372921481553248, 0.37286950668910135, 0.37280699069551709, 0.37278796867607594, 0.37273122180268786, 0.37268077725255389, 0.3726331081981204, 0.37257800937151997, 0.37252332938149169, 0.37247492307160501, 0.3724225674428322, 0.37237336794865417, 0.37232756571791431, 0.37225351640247417, 0.3721987330615073, 0.37214669268924205, 0.37209921171614802, 0.37205395020048587, 0.37200634625144269, 0.37194080255048562, 0.37190106200289935, 0.37186068593724408, 0.37181254790350088, 0.37176162599643348, 0.37171958972212793, 0.37167019578103888, 0.37162174815253335, 0.37154349508180634, 0.37149571515857688, 0.37146162647116265, 0.37144559324843568, 0.37137029707005925, 0.37132389041479985, 0.37130774960840085, 0.37126998120075749, 0.37120877438589372, 0.37116291713946858, 0.37112632523896755, 0.3710740056359676, 0.37103249100535696, 0.37097645999593326, 0.37091204223213159, 0.37089804401482473, 0.37085296786538535, 0.37081392487724596, 0.37077477901205275, 0.37073122368032518, 0.37069470067708399, 0.37065595400105705, 0.37061746290493219, 0.37058121952896272, 0.37054418972494918, 0.37047096435544324, 0.37042670707925957, 0.37041280472719318, 0.37037157379362207, 0.37036083895863264, 0.37028581619439865, 0.37025111561762014, 0.37021903727703787, 0.37015209897207618, 0.37008800248381679, 0.3700456868413628, 0.37000733237962774, 0.3699767323078183, 0.36993595268683455, 0.36992327485413024, 0.3699095107408073, 0.36989717793730331, 0.36986127980414168, 0.36982609751300993, 0.36979210092457138, 0.36972215538300329, 0.36967749415279644, 0.36964367780889035, 0.3695960117129482, 0.369564517587749, 0.36952711004944427, 0.3694960391811728, 0.36946357583645972, 0.36942337588925578, 0.36936859169250491, 0.36931061649306868, 0.36927986296430643, 0.36922758272680245, 0.36916806049102668, 0.36914068241420417, 0.36910691977685051, 0.36906434094405371, 0.36902947164175115, 0.36899820711642878, 0.36893593554597298, 0.36890689662069837, 0.36884639291744847, 0.36881233477449893, 0.36878549641493868, 0.36875851413626465, 0.36870103023349909, 0.36866020396756005, 0.36862962060492316, 0.36859711383596677, 0.36856882565123311, 0.36853210499446093, 0.36850030926140853, 0.36847677929552647, 0.36844877874546189, 0.36844008565377862, 0.36840171349655088, 0.36836579694823313, 0.3683411226229234, 0.36830420619675336, 0.36825159391837753, 0.36822565299927446, 0.36817086682236305, 0.36814351376716059, 0.36811751635095552, 0.36808209991293711, 0.36805540439582696, 0.36802898619720742, 0.36799308797030017, 0.36794733641568672, 0.36792629384978393, 0.36789658506191153, 0.36786862139868115, 0.36784503123562751, 0.36782371287927995, 0.36777957986237453, 0.36775168600018249, 0.36770859582275561, 0.36768408827213916, 0.36765769925659741, 0.36763462493746879, 0.36761274135186489, 0.36759216899513891, 0.36756927451784904, 0.36754306574159196, 0.36753348607742381, 0.36750528709722735, 0.36748223850821937, 0.36745564624349047, 0.36742887587832218, 0.3673970653626869, 0.36736743788324111, 0.36734008360922876, 0.36731564615392659, 0.36728169642769865, 0.36725672364380163, 0.36722712220368853, 0.36720380836037014, 0.36716297685415772, 0.36714168966042171, 0.36710711573307281, 0.36708761413871072, 0.36706211039430714, 0.36703854683577608, 0.36701718247779674, 0.36699429226981028, 0.36695633632696323, 0.36694637174344458, 0.36692426708645931, 0.36689867706768364, 0.36687150339923491, 0.36684608127222684, 0.3668206211611153, 0.36678682258108236, 0.36676522510477777, 0.36674532427841122, 0.36671803394319291, 0.36669467296484148, 0.36665885019126254, 0.36664005641933672, 0.36661159950839212, 0.36659039686950634, 0.36658341271875283, 0.36657470976493067, 0.3665495930690551, 0.36652222985105543, 0.36648702557693225, 0.3664552032931484, 0.36643633464790909, 0.36641301635219659, 0.3663865001940918, 0.36635208608386616, 0.36632986350901026, 0.36630795333251492, 0.36628225732928199, 0.36627447078263736, 0.36626861940046057, 0.36624713019309907, 0.36622801849651315, 0.36621384183803429, 0.36619163839076158, 0.36618386522659874, 0.36616015444655053, 0.36613833508352445, 0.36612090816711013, 0.36609704743885763, 0.36607422037285314, 0.36604707014832871, 0.36602953935862004, 0.36600606097825167, 0.3659887197031283, 0.36596551894856477, 0.36594609134942535, 0.3659373728900509, 0.36591614763370511, 0.36588599992014509, 0.36586731934462591, 0.36584435155309242, 0.36581221935529595, 0.36578813302015639, 0.36576934714135328, 0.36575235806199591, 0.36573203644245633, 0.36569941633490655, 0.36569119421524943, 0.36566401981741026, 0.36564466249716315, 0.36561548587870951, 0.36559381965571575, 0.3655732304719298, 0.36555179019630168, 0.36553206366800001, 0.3655166131212973, 0.36549565628236269, 0.36547782295779824, 0.36545051656005306, 0.36542143991709108, 0.36539897051754427, 0.36538305946068755, 0.36536425239714504, 0.36534521422733451, 0.3653388210231075, 0.36531226044692727, 0.36529397922299595, 0.36527820758093149, 0.36525557891974858, 0.36522718270966376, 0.36520222153500664, 0.36517767545789803, 0.3651564785124341, 0.36512579002061157, 0.36510995343629971, 0.36509023605981517, 0.36507279331322862, 0.36504319466056467, 0.36502729940969642, 0.36500940681848582, 0.36499195950564284, 0.36497571894563885, 0.36495520566439665, 0.36494144369013998, 0.36493028056058496, 0.36490471756960913, 0.36489149994323916, 0.3648847885887902, 0.36486733191069065, 0.36486094884533282, 0.36485524065246039, 0.36484127812742767, 0.36482681020304786, 0.36481103286751276, 0.36479335024210152, 0.36478101400445562, 0.36476749445682288, 0.36475193520091431, 0.36473388481281377, 0.36469391841230514, 0.3646802591110388, 0.36466811242397612, 0.36465337110262841, 0.36463009325759438, 0.36461492699423148, 0.36460085791478447, 0.36457965312165397, 0.36456251403433571, 0.36454306914118123, 0.36452851832892175, 0.36450070592141964, 0.36446323341478415, 0.36444448459960188, 0.3644296169979645, 0.3644134037556529, 0.36439646334045661, 0.36437837216404445, 0.36435554756009442, 0.36433943144284592, 0.36432357314244734, 0.36430455276152618, 0.36428450195177087, 0.36426792304520028, 0.3642561167618526, 0.36424363706894375, 0.36422777792015681, 0.3642214291397321, 0.36420117148866893, 0.36418214453852221, 0.36416641440247965, 0.36415305094663064, 0.36414095242938616, 0.36412598848301159, 0.36411430257350169, 0.36409475086590037, 0.36407370085350221, 0.36406691580913508, 0.3640515840298173, 0.36404152226560182, 0.36402972493056374, 0.36402237637035578, 0.36400962540916409, 0.36399481198346806, 0.36397848290444101, 0.36397167245918105, 0.36395731828678202, 0.36394794770152894, 0.36393546238621477, 0.36391444196644573, 0.36389687716086144, 0.36386855847153549, 0.36384805268553705, 0.36383432814687994, 0.36381829903413399, 0.36380699631259211, 0.36378601583449083, 0.36377453464562393, 0.36376064360368082, 0.36373719613897526, 0.36371283268883092, 0.36369578252414797, 0.36367875162814084, 0.36366062872506977, 0.36363704368412508, 0.36362696050566046, 0.36360623264140884, 0.36359339823178322, 0.3635675077346664, 0.36355694190456073, 0.36354461422514595, 0.36353406549808354, 0.36352454062293216, 0.36351288566908579, 0.36349162506491062, 0.36348074673260966, 0.36346923970290651, 0.36343692985431408, 0.36342502587948045, 0.36340544215327131, 0.36339413603899035, 0.36338121134165002, 0.36336480778342273, 0.36335328108174086, 0.36333496943942822, 0.36332609653216802, 0.36331646253404432, 0.36330392186249916, 0.36328754053441686, 0.36327344801530875, 0.36326252634489953, 0.36325553099984809, 0.36324681001681358, 0.36323586506164368, 0.36321366934564331, 0.3632023908434211, 0.36319213368523867, 0.36317916319039156, 0.36316400350648709, 0.36314728120379114, 0.36313907650374844, 0.36312703231369253, 0.36311402376294488, 0.36310558093543482, 0.36309548787929946, 0.36307588322602247, 0.36306488637529399, 0.36304707711828338, 0.36303394563363822, 0.36302277166380137, 0.36300933346678443, 0.36299260979372833, 0.36298259769377339, 0.36297311846688829, 0.36296063032862602, 0.36294653910799096, 0.36293524089509605, 0.36291959319627592, 0.36290879663616915, 0.36288775291286812, 0.36287726293536726, 0.36286576902824408, 0.36285649170495393, 0.36284697034356128, 0.36283972319948654, 0.36281137138065095, 0.36279970075919354, 0.3627876239683091, 0.36277414073060521, 0.36276107013703185, 0.36275187026782835, 0.36274796009807536, 0.36274030933889712, 0.36273018368536153, 0.36272078779328737, 0.3627143315139304, 0.36270500780079101, 0.36269311581021496, 0.36268435352113504, 0.36266330314548423, 0.36265286908065597, 0.36264242301418614, 0.36263103467117203, 0.3626269696738243, 0.3626164610739494, 0.36260936710361402, 0.36260385930306022, 0.36259473279879084, 0.36257982395662325, 0.36254725235580326, 0.36253801850495226, 0.36253337362838028, 0.36252515781381467, 0.36252166401627173, 0.36251450316463851, 0.36251152731866615, 0.36250137579423408, 0.36249230434905944, 0.36248602106515815, 0.36247709669001277, 0.36247039406553122, 0.36245585848187223, 0.36244604478429471, 0.36242843985695533, 0.36242369245730421, 0.36241713717415275, 0.36240647993297453, 0.36239619520020011, 0.36238599369618035, 0.36237795933757844, 0.36236614597094463, 0.36235519747546474, 0.3623385223268486, 0.36233197006791018, 0.36232015410137319, 0.36231183248365817, 0.36230341170673075, 0.36229606699701888, 0.36228500222878479, 0.36227858242660671, 0.36226636171607285, 0.36225011018959707, 0.36223973135364906, 0.3622258740663345, 0.36221695004700438, 0.36221101173997527, 0.36220214488754221, 0.36219678848104037, 0.36218834484161372, 0.36218052155472497, 0.36215953630759151, 0.36212931870152121, 0.36211789863326882, 0.36210953716468797, 0.36210654012765586, 0.36208968241189482, 0.36208139256024741, 0.36207431441524335, 0.36206373910265038, 0.36206047960420401, 0.36205113459843674, 0.36204294698858519, 0.36203568753850168, 0.3620281460855922, 0.36201895170175269, 0.36201294329193623, 0.36200379764960733, 0.36199119875346458, 0.3619844308909882, 0.36197640643149193, 0.36196814606861583, 0.36196375723935093, 0.36195268577923351, 0.36194517270919441, 0.36193973000898555, 0.36193424602907198, 0.36192492897449302, 0.36191768388214052, 0.36191211845327331, 0.36189999086949504, 0.3618932943234841, 0.36188599384100545, 0.36187778515381563, 0.361868309658464, 0.36185992482962676, 0.36185359142358231, 0.36184671878449454, 0.36184004369105244, 0.36183405295914378, 0.36182437700766867, 0.36180603314891319, 0.3618026712986836, 0.36179421417100838, 0.36178416702092736, 0.36177426244257133, 0.36176580078245607, 0.36176148883858095, 0.36175140919246451, 0.36173472222351677, 0.36172701884065434, 0.36171893211033929, 0.36170926673689852, 0.36169091066117731, 0.36168382472973937, 0.36167131983293377, 0.36165964948011942, 0.36164689252932913, 0.36163596625190059, 0.36162910955219946, 0.36160379751416505, 0.36159530781608329, 0.36158823377693583, 0.36158330223957458, 0.36157447758825279, 0.36157050633603677, 0.36156033044838176, 0.3615451010860512, 0.36153979830996963, 0.36153223246347593, 0.36152537544000718, 0.36150858613716308, 0.36149814569542188, 0.36148996595585575, 0.36147551072791922, 0.36144556637212005, 0.3614291494695881, 0.36141965969718287, 0.36141356729982543, 0.36140988822783848, 0.36140342204180509, 0.36139537787995768, 0.36138889793545687, 0.36138169429946249, 0.36137762900708192, 0.36136730310653614, 0.3613582399125943, 0.36134716030875264, 0.3613391872987809, 0.36133296471142334, 0.36131666928769535, 0.36131169641041944, 0.36130306258507089, 0.3612984514181391, 0.36129518025947005, 0.36129320049508845, 0.3612883823525721, 0.36128462447371856, 0.36128023663158082, 0.36127401810594828, 0.36125486373442284, 0.36124808146012638, 0.36123288418645771, 0.36122598829869312, 0.36121724101793939, 0.36120064733021501, 0.36119505907726246, 0.36118645080293144, 0.36117908789675485, 0.36117248731356516, 0.36115207555918827, 0.36114490262379401, 0.36113294223578429, 0.36112198920123983, 0.36111762550805121, 0.36111105825293499, 0.36110612286675031, 0.36108549403588303, 0.36107806166149975, 0.36107021648048859, 0.36106075546233896, 0.36105395431780207, 0.36104650063164351, 0.36103229226122951, 0.3610238351153458, 0.3610165055642377, 0.3610079030454339, 0.36100095195587462, 0.36099519121416651, 0.36098319408533852, 0.36097620464367725, 0.36096748954515612, 0.36095958110592952, 0.36095561845318402, 0.36094694109077896, 0.36093821440937623, 0.36091638853027186, 0.3608965222601267, 0.36089208006056317, 0.3608845616190749, 0.36087852025517286, 0.36085093342984847, 0.36084436593553942, 0.36083912622472331, 0.36083578042171582, 0.36083023092366256, 0.36080379501003196, 0.36079790171345627, 0.36078515858075699, 0.36077966980190862, 0.36077462696004403, 0.36077299276769492, 0.3607667810025168, 0.36075891960690504, 0.36075052257300927, 0.36074420770096155, 0.36073282399656492, 0.36072181282137405, 0.36069648758373646, 0.36067616828627475, 0.36067080242291266, 0.36066509755290066, 0.36064772058526073, 0.36064118605276774, 0.36063518646567028, 0.36063038460265989, 0.36062369139255146, 0.36060377434024204, 0.36059683908232992, 0.36059269764748231, 0.3605883014429398, 0.36057676123377552, 0.36056950797290332, 0.36056424259502368, 0.36056212273148525, 0.36055971991874891, 0.36055059966326064, 0.36054580686245452, 0.36053881792174192, 0.36053333177426949, 0.36052879480561389, 0.36051822151384327, 0.36049693588924664, 0.36048956143143301, 0.3604846396456764, 0.36047939017161379, 0.36047385634652784, 0.36046347030055098, 0.36046210335774165, 0.36045437850923934, 0.36043151528540751, 0.36042924551368893, 0.36042460576416546, 0.36041975977860857, 0.36041419979173428, 0.3604074261233276, 0.36040695149830554, 0.36039380590726772, 0.36038786589716371, 0.3603754529835248, 0.36037000475725339, 0.3603601125282484, 0.36034291776037508, 0.36033764747953173, 0.36033141054088536, 0.3603253279530535, 0.36031989829644118, 0.36031420472881986, 0.36030545010560705, 0.36029929126686955, 0.3602934299513364, 0.36028578690008806, 0.36027836761549975, 0.36027329010347675, 0.36026919053303452, 0.36026474812321596, 0.36025794706681102, 0.36024523973744671, 0.3602413303529447, 0.36023205613777221, 0.36021999532157167, 0.36021377765718271, 0.3602090126448142, 0.36019232771864107, 0.36017550398288456, 0.36017356608250173, 0.36016608867300992, 0.36016433778702017, 0.36016121754269748, 0.3601557008287572, 0.36015291104329211, 0.36014807286325218, 0.36014268303752794, 0.36013548007372137, 0.36012885197983158, 0.36012494222450953, 0.36012141133717057, 0.36011610451050313, 0.36011231176419317, 0.36010766774153763, 0.36010148921817126, 0.36009600601754072, 0.36008950199211842, 0.3600856085894234, 0.36007585105095774, 0.3600652362628779, 0.36005328532211844, 0.36004278564791142, 0.3600358544981721, 0.36002490779534829, 0.36002013529777427, 0.36001341055835911, 0.36000936761560376, 0.36000664592256654, 0.35999999755837697, 0.35999107302482275, 0.35998599152708421, 0.35998395611607659, 0.35997831611748304, 0.35996575778812112, 0.35995849637148825, 0.3599526118821551, 0.35993764801054162, 0.35993543689191043, 0.35992973374988957, 0.35992024216460372, 0.35991516550980951, 0.35990332831301242, 0.35989796086796, 0.35989413563019124, 0.35988960852505636, 0.35988594336149909, 0.35988412631368655, 0.35988176162914842, 0.35987096563677634, 0.35986161555669388, 0.35985715046669109, 0.35985090931243791, 0.35984835333036297, 0.35984375759008302, 0.35983825957533466, 0.35983516310342967, 0.35983072691860502, 0.35982791896741551, 0.35982101317148019, 0.3598148943445893, 0.35980813947450108, 0.35980050227468452, 0.35979458879833881, 0.35979107662206333, 0.35978743330614282, 0.35978433329711557, 0.35978180668525261, 0.35977770733527376, 0.35977090399796741, 0.35976654872190356, 0.35975894532571667, 0.3597537023024785, 0.35974691667836683, 0.35974346041904348, 0.35973810546744095, 0.35973583695575462, 0.35973150212447197, 0.35971976608646306, 0.35971489064828199, 0.35970902507734087, 0.35970331852983956, 0.35970159347735342, 0.35969444052687705, 0.35969018622290572, 0.35968183851123903, 0.35967834485348932, 0.35967432373386615, 0.35966685420250988, 0.35966425794842466, 0.35965962857150496, 0.35965163767923264, 0.35964707832283815, 0.35964424337851503, 0.35963909774642333, 0.35963514720582934, 0.35963231533536788, 0.35962819092189979, 0.35961791357594353, 0.35961397804851108, 0.35958636237348462, 0.35958220333534774, 0.3595776517270966, 0.35957038322467783, 0.35955954467224166, 0.35954271900201185, 0.35953806442558911, 0.35953483639598527, 0.35952622300029324, 0.35951681747056757, 0.35951354260174073, 0.35950922922472589, 0.35950440596282168, 0.35949857228564064, 0.35947875179551736, 0.35947480228389905, 0.35947323351951355, 0.35946831146314751, 0.35945792361991863, 0.3594551371550514, 0.35945094707758235, 0.3594479919765079, 0.35943948991497765, 0.35943564303167508, 0.35943092308342323, 0.35942860769901858, 0.35942764702303898, 0.35942267229541752, 0.35941804377603226, 0.35941609701128802, 0.35941237636551016, 0.35940838628263999, 0.35940150104830526, 0.35939659757320536, 0.35938592466165192, 0.35937633900363253, 0.3593631310681516, 0.3593594258384134, 0.3593554214249447, 0.35935129019911261, 0.35934145537477447, 0.35933652246644054, 0.35932709416261216, 0.35932189437440953, 0.35931874978201706, 0.35931228914735158, 0.35930695873793794, 0.35930180253400762, 0.35928729505606272, 0.3592805556406401, 0.35925888891865754, 0.35924394584016101, 0.35924053869370576, 0.35923853398615024, 0.3592267919984688, 0.35922336901893176, 0.35921842745302457, 0.35921071659900777, 0.35920769990418061, 0.35919602056283234, 0.3591928623223416, 0.3591879691980237, 0.359186234780491, 0.3591845856968775, 0.35917885237973329, 0.35915987281206035, 0.35915440592117726, 0.3591526427851669, 0.35914896114825201, 0.35914726770059263, 0.35914042374103355, 0.35913423848675563, 0.3591314307830894, 0.35912003665251019, 0.35911549440146839, 0.35911195756491854, 0.35910410610115873, 0.35909987667124971, 0.35909256854495097, 0.35908244412554524, 0.35907741230031454, 0.35907587639047556, 0.35907355874469982, 0.35906520965849664, 0.3590632307310761, 0.35905806296707621, 0.35905130534964569, 0.35904733173623166, 0.35904546556071903, 0.35904530820343938, 0.35904295671050562, 0.35903698687510194, 0.35903098574652692, 0.3590279432976422, 0.35902252852982908, 0.35901468620284438, 0.35901087568081452, 0.35900880671025942, 0.35900316251540554, 0.35899861187441612, 0.35898996323858257, 0.35898609891293332, 0.3589813740535, 0.35897947364920002, 0.35897695642268362, 0.35897176731500852, 0.35896645049835779, 0.35896266763742268, 0.35895379456469784, 0.35894825201535385, 0.35894182980483347, 0.35893683079974481, 0.35893236857145516, 0.35892859524125076, 0.3589238634853556, 0.35891965323116259, 0.35891508509149866, 0.35891226399772846, 0.35890491081166948, 0.3588998948164851, 0.35889692821884162, 0.35888918100910072, 0.35888779709616275, 0.35888619299195251, 0.3588834269462714, 0.35887309312352694, 0.35886669758167139, 0.35886367988915591, 0.35886161276197587, 0.35885647686203959, 0.35885441158238168, 0.35885152111079033, 0.35884724359421205, 0.35884281213603908, 0.35884196935234813, 0.3588355222353754, 0.35883322312857596, 0.35882986655392302, 0.35882479675937679, 0.35882301050651177, 0.35881295309071831, 0.35881212417590308, 0.35880439445115908, 0.35880181931114441, 0.35879779343740992, 0.35879529844849922, 0.35879076817174888, 0.35878825443070428, 0.35878573598189345, 0.35878220535520811, 0.35877706351686556, 0.35877281718782872, 0.35877007694048935, 0.35876830062793685, 0.35876333073788891, 0.35875789816808368, 0.35875253962416004, 0.35874871839734473, 0.3587473459832814, 0.35873159479998468, 0.35872961026924932, 0.35872628448726174, 0.35872346456042642, 0.35872105896571804, 0.35871698629785942, 0.35871328215039677, 0.3587112905542757, 0.35870239879114951, 0.35869919056998373, 0.35869570061192563, 0.35869225865020038, 0.35868842046979749, 0.35868820619190156, 0.35868393603990062, 0.35868112918147893, 0.35867944544594338, 0.35867049566618919, 0.35866534004283135, 0.35866262757139966, 0.35863706635761661, 0.35863294189707184, 0.3586297857834192, 0.35861091137333967, 0.3586063795843753, 0.3586039353702476, 0.35860188144693206, 0.35859792218627728, 0.3585876541354609, 0.3585834166630974, 0.35857731315972413, 0.35857394919965113, 0.35857298100232232, 0.35856814767203604, 0.35856518885636174, 0.35856266815541232, 0.35855836465616092, 0.35853223654437205, 0.35852883249501188, 0.35852403430465835, 0.35852034875961902, 0.35851874862346517, 0.35851432339072076, 0.35851335325031841, 0.35851147254594018, 0.35850664278322514, 0.35850611991165882, 0.35850197600691941, 0.35849808413477235, 0.35849660125654065, 0.35849351372659533, 0.35847382559135665, 0.35847214461834803, 0.35847146311738604, 0.35847049806502834, 0.35846612064620714, 0.35846315926298006, 0.35844975226754944, 0.35844566368259062, 0.3584371641823359, 0.35843366908500329, 0.35842421901815458, 0.35842100033566038, 0.35841759716275473, 0.35841509978929503, 0.35841301403856918, 0.35840900053196262, 0.35840752432095735, 0.35838887344021125, 0.35838684597493692, 0.35838357539861965, 0.358376003953643, 0.35837594127572686, 0.3583612857926981, 0.35835883966638327, 0.35835647098198675, 0.35835292326468648, 0.35834863153708607, 0.35833401473481269, 0.35833005787943667, 0.35832925792050063, 0.35831464720791328, 0.35831095745754438, 0.35829542567411332, 0.35828857826958072, 0.35828749494812795, 0.358283196676696, 0.35827675617057186, 0.35827340809543051, 0.35826803154161457, 0.35826014900612951, 0.35825672902094402, 0.35825526228526694, 0.35824976565649874, 0.35824272538554131, 0.35823380333402877, 0.35821707123037405, 0.35821140163485743, 0.35821039865272403, 0.35820709763706765, 0.35819798462276231, 0.35819706605095003, 0.35819688958913842, 0.35819394355490564, 0.35818934051664214, 0.35818367057204209, 0.35818168843739356, 0.35817432554457651, 0.3581734367479763, 0.35815732795738198, 0.35815473724838753, 0.35815138329811791, 0.3581462340325906, 0.35814434392683009, 0.35813714762928422, 0.35813536479683256, 0.35813108698218932, 0.35812409504117954, 0.35810744695486513, 0.358102299027517, 0.35809906069262765, 0.35809732552414547, 0.35809499788524091, 0.35809028275942106, 0.3580883990045694, 0.35808698093395797, 0.35808614661013394, 0.3580784249270576, 0.35807224627324008, 0.3580711284773358, 0.35806019699622044, 0.35805068523216477, 0.35804846321544709, 0.35802324492912685, 0.35801797100657817, 0.35801580065837857, 0.35800833184713243, 0.35800394316763029, 0.35799992155184907, 0.3579830983199182, 0.35798189061218227, 0.35797905490921961, 0.35797370858317829, 0.35797045948336492, 0.35796673951691627, 0.35796570924277704, 0.35796268533104841, 0.35795337051254317, 0.35795057613490755, 0.35794029614939404, 0.35791763152417327, 0.35791390126927147, 0.3579129135786932, 0.35791062972659332, 0.3579042286991313, 0.35790206989454221, 0.35790151197233111, 0.35788671208814032, 0.35788471482488199, 0.35788282068657262, 0.35786873049937545, 0.35786690317826958, 0.35785262711061838, 0.35784996337207853, 0.35784160393147491, 0.35783766764797575, 0.35783480592006323, 0.35783225164598831, 0.3578243247888514, 0.35781916426806637, 0.35781889975235259, 0.35781682516430308, 0.35781397435916573, 0.35780649602101711, 0.35780227431361472, 0.35779249231560306, 0.3577892729106863, 0.35778128415327071, 0.35777881128571809, 0.35776968806016785, 0.35775974005294642, 0.35775795526182336, 0.35775655488989544, 0.35775466067978201, 0.35775264272026258, 0.35775132373117075, 0.35774905024885267, 0.35774671358346671, 0.35774581891022206, 0.35773535682545893, 0.35773185702585708, 0.35772708820334803, 0.35772657997782464, 0.35771957762352102, 0.35771865079538068, 0.35771573537821777, 0.35771214506812576, 0.35770558150323473, 0.35770445770717918, 0.35770014602937056, 0.35769987500807304, 0.35768503015585862, 0.35766745161337371, 0.35766600624694844, 0.35765925228930545, 0.35765234939179374, 0.35765148346760645, 0.3576478395468955, 0.35764362224837098, 0.35764204406160915, 0.35764225335427774, 0.35763989670827639, 0.3576356127936039, 0.35763290494507077, 0.35762538338398275, 0.35762232191019244, 0.35761225867069812, 0.35761016361337983, 0.35760790460659947, 0.3576057010216836, 0.3575888436351421, 0.35758071090609234, 0.35757548260339811, 0.35757403709488356, 0.35757233850810261, 0.35756575256023515, 0.3575637917192338, 0.3575618258329985, 0.35756137117036563, 0.35756081127885114, 0.3575441764054097, 0.35754109469738038, 0.357539350884576, 0.35753759033346788, 0.35753569530461693, 0.35753298900209007, 0.35753054485676339, 0.35752909240419728, 0.35752634715422005, 0.35752323519941964, 0.35752185328477798, 0.35752098256848697, 0.35752001021893448, 0.35751830300498477, 0.35751731308032142, 0.35751384398389113, 0.35751214728252223, 0.35751237340364606, 0.35751002951858085, 0.35750851721838006, 0.35750457158800092, 0.3575020375969149, 0.35749897574027678, 0.35749259381564569, 0.35749062083088928, 0.35748859007049438, 0.35748798767439005, 0.35747679237932528, 0.35746825972879548, 0.35746781847153947, 0.35745917298252172, 0.35745859992069173, 0.35745758043699077, 0.35745084641804903, 0.35744827946985636, 0.35744217380414034, 0.35744121851680427, 0.35744062386482722, 0.35743466042893512, 0.35742878774991016, 0.35742371587942934, 0.35742355601378445, 0.35741658827173572, 0.35740339850849168, 0.35740152272390124, 0.357400059795345, 0.35739764978996658, 0.35739649562699405, 0.35739057479679187, 0.3573898027193364, 0.35738592919351592, 0.35738520805850316, 0.35738062148648014, 0.35737599051642688, 0.35736807595666614, 0.3573680000381888, 0.35736031472816754, 0.35735797107122613, 0.35735757239688942, 0.35735650736310948, 0.3573533283203556, 0.35734766595509343, 0.3573384415203219, 0.35733575612395474, 0.3573344075783953, 0.35733197137757061, 0.35732875998378921, 0.35732672894029704, 0.3573180840477504, 0.35731633934392293, 0.35731396282877298, 0.35730771289300289, 0.35729704557442393, 0.35729669720834567, 0.35729101812068609, 0.35728663204476069, 0.35728425108358086, 0.3572803515607777, 0.3572730181341367, 0.35727323113390841, 0.35726810616732496, 0.3572655265489606, 0.3572633442271072, 0.35725647547592987, 0.35725326650428574, 0.35725418632246292, 0.35724656213079509, 0.35724002741668814, 0.35723757417127938, 0.35723625365775535, 0.35723518137433924, 0.3572338053255309, 0.35723238038776495, 0.35720956871860665, 0.35720580955244469, 0.35720221844968558, 0.35719677600792005, 0.35719589701722787, 0.35719355987683604, 0.35719213291500629, 0.35719272036852517, 0.35718293357147074, 0.35718122826414261, 0.35717577371214931, 0.35717656889583804, 0.35717612469905341, 0.35717481403869333, 0.35717417472563584, 0.35717293717150433, 0.35717249597987172, 0.35717217622920783, 0.35716950068456643, 0.35716363095160086, 0.35716353685239616, 0.3571614456747636, 0.35714666252361715, 0.35714335647367618, 0.35714122781118307, 0.35713669961244465, 0.35713672460049339, 0.35713542320204061, 0.35713400922097388, 0.35712831545884854, 0.35712763054739166, 0.35712754705711014, 0.35712572432136469, 0.35711654206370286, 0.35710875134873521, 0.35710689708039617, 0.35710533318131821, 0.35710453603090159, 0.35710133311141173, 0.35710028147749129, 0.3570905312061417, 0.35708813420587621, 0.35708445231489561, 0.35708288632532326, 0.35708097932879895, 0.35707779868408102, 0.35707148136474504, 0.35707011437267366, 0.35706505798450106, 0.35706347314995007, 0.35705866590282737, 0.3570566197505689, 0.35704972517781614, 0.35703387354895555, 0.35703288448163323, 0.35703074282881697, 0.35703007312047086, 0.35703046820758294, 0.35702540487290579, 0.35702461049338108, 0.35700972902359995, 0.3570082077972499, 0.35700701411931635, 0.35700599610591188, 0.35700258752177838, 0.35700171075057041, 0.3569939649351806, 0.35699173707188664, 0.35698741265736561, 0.35698629846341812, 0.356980343727202, 0.35697804895910767, 0.35697592858722971, 0.35697633275502905, 0.35697188364305449, 0.35696534697622945, 0.35696246580369412, 0.35696225432912754, 0.35695878770438305, 0.35695881096920934, 0.35695713913719196, 0.35694547952146105, 0.3569398588284306, 0.35693607045544956, 0.35692768506303785, 0.35692708045491006, 0.35692676871352552, 0.3569248196342461, 0.35690329213073457, 0.35690037096839367, 0.35690023941171184, 0.3568875352213105, 0.35688424611440339, 0.35687957079354321, 0.35687599346091126, 0.35685756185769851, 0.35685763124184466, 0.35685040028510218, 0.35684493325411493, 0.35684069138164948, 0.35683872002607858, 0.35682985688416596, 0.35681602145030616, 0.35681443876212959, 0.356801561492885, 0.35680038220096122, 0.35679946415891339, 0.35679722029735039, 0.35678564887758391, 0.35678466967685157, 0.35678481231370618, 0.35677455334812669, 0.35677363577496962, 0.35677092367522578, 0.35677038369623493, 0.35676597309969216, 0.35675800205670177, 0.35674715537511137, 0.35674038855073531, 0.35673415446272216, 0.35673387561076003, 0.35672986836563392, 0.356728996989636, 0.35672732486753767, 0.35672595098839865, 0.35671607427564511, 0.35671454706347794, 0.3567135251991706, 0.3567066364098026, 0.35670461853169688, 0.35670026544197897, 0.35670102686112781, 0.35669862200863406, 0.35669643625555364, 0.3566940401006965, 0.35669089251583197, 0.35668959038059728, 0.35668241864621864, 0.35668173071359127, 0.35667668154013188, 0.35666934473331546, 0.35666921640829019, 0.35666613894294591, 0.35666343678673573, 0.35666343478076579, 0.35666162788687289, 0.3566579308025174, 0.3566563311415003, 0.35665386030976293, 0.35665020838190192, 0.35664967098453787, 0.35664907575771326, 0.35664425700516023, 0.35663047095661804, 0.35662957665319783, 0.35661556755972168, 0.35661310027921522, 0.35661181933131364, 0.35660760279393844, 0.35660560044715278, 0.35659169208669422, 0.35658618837540401, 0.35658537505848353, 0.35658449554156257, 0.35658400011516278, 0.35656432193323961, 0.35656277601119168, 0.35656156207989448, 0.35655972815962589, 0.35655353430604653, 0.35655093080471356, 0.35654930297951887, 0.35654728064987046, 0.35654622283959531, 0.35654089540107181, 0.35653938687781489, 0.35653633668480633, 0.35653378436435051, 0.35652859738481457, 0.35652759129741629, 0.35652736150966058, 0.35652468948049804, 0.35652243702430514, 0.3565221143457446, 0.35652018284446069, 0.35652035725642495, 0.35651606538234976, 0.3565144423192434, 0.35651257556852589, 0.35651234721562386, 0.35651237803231656, 0.35650983250948742, 0.35650725456818905, 0.35650564513142763, 0.35650461202349365, 0.35649735900415269, 0.35649723747516221, 0.35649551347842506, 0.35649388408945493, 0.35649113359254087, 0.35649104970223405, 0.35648673556270033, 0.35648594852446497, 0.3564848941710218, 0.35648239113025404, 0.35648158115490236, 0.35648188448799345, 0.35647922535378174, 0.35647770775983639, 0.35647694285495835, 0.35647533313800228, 0.35647513837864581, 0.35647025916473946, 0.35646704717584138, 0.35644496544700977, 0.35644464532849934, 0.3564439500564377, 0.35643827840218756, 0.35643768226944283, 0.35642869536765215, 0.35642225941884448, 0.35642178681689063, 0.35641991265038225, 0.35642012546103657, 0.35641661834338795, 0.35641565910717682, 0.35641306535890283, 0.35641190956542579, 0.35640295512913667, 0.35640070669786456, 0.35639824958889615, 0.35638986222350916, 0.35638136634473394, 0.35637652286291177, 0.35637455440346438, 0.35637314438459333, 0.35637317655362388, 0.35637251058646147, 0.35637153016241307, 0.35637200395394508, 0.35637117125515527, 0.35636876952734131, 0.35636633347181268, 0.35635640689411946, 0.35635512953378939, 0.35634758064308619, 0.35633922839258225, 0.35633372933486362, 0.35633152556534475, 0.35632609277334232, 0.35632477757703124, 0.35631359528816786, 0.35631238058843101, 0.35631038254991565, 0.35630028555153531, 0.35629944653061379, 0.35628839406982915, 0.35628713315925042, 0.35628690965079468, 0.3562847883740447, 0.35628242579545699, 0.35628050268954725, 0.3562775151414091, 0.35627586223843449, 0.35627446674377528, 0.35627319411139807, 0.35627236177116378, 0.35627206175400378, 0.35627218745225187, 0.35626383513068027, 0.35626193175099952, 0.35626147677849385, 0.35626062683214837, 0.35625870751277994, 0.35625417548686505, 0.35624848473185289, 0.35624675042837139, 0.35624360182400883, 0.35624196175069694, 0.3562348882433003, 0.35622250494902891, 0.35622095118420077, 0.35622043887634391, 0.35620682006281762, 0.35620485703710736, 0.35620396737452226, 0.35620272184947505, 0.35619760904618009, 0.35619497819675344, 0.35619203877216093, 0.35619226287058692, 0.35619175747419385, 0.35618919916176728, 0.3561898454354937, 0.35618894708219734, 0.35618338814412442, 0.35618359683942885, 0.35617842742746353, 0.35617175295494369, 0.35617138596794717, 0.35616841122095011, 0.35616706064050507, 0.35615389006722842, 0.35615182800020179, 0.35615102264336318, 0.35614927159954901, 0.35614692358089606, 0.35614197731671726, 0.35614161775968894, 0.35613990310455634, 0.35613944589248508, 0.35613673254877787, 0.35613521773103007, 0.35612665956740486, 0.35611731192888885, 0.35611619698383523, 0.35610474672842918, 0.35609833085783171, 0.35609684912480294, 0.35609279941828909, 0.35609220484970833, 0.35609071751103344, 0.35608929209139195, 0.35608678210352823, 0.35608618880515136, 0.35608423237832704, 0.35608397392955482, 0.35608182620313017, 0.35608167102513444, 0.35608001468121586, 0.35607788194669288, 0.35607434737537286, 0.35607121669215952, 0.35606970878530941, 0.35606938419956957, 0.356054067099914, 0.35605260636862701, 0.35605195548116475, 0.35605086161374788, 0.35605221462441428, 0.35604408887943439, 0.35604245878156815, 0.35604091669245069, 0.3560387447975164, 0.35603806964877066, 0.35603629181090352, 0.35603503757148758, 0.35603182150124252, 0.35603045074076062, 0.35602931605025367, 0.35602798779768352, 0.35602727631486142, 0.3560197092914611, 0.35601830559721748, 0.3560168652367911, 0.35601268109923156, 0.35600818611134388, 0.35600468408230906, 0.35600247428408849, 0.35600218455444066, 0.35599878289394804, 0.35599294279054627, 0.35598521872053301, 0.35598375020514805, 0.35598217016333322, 0.3559809928245915, 0.355978243915679, 0.3559783764716567, 0.35597695382856853, 0.35597705990357686, 0.35597703471112052, 0.35597568160465248, 0.3559731938258261, 0.35597192995560739, 0.35597206887162891, 0.35596777185291145, 0.35596655368179958, 0.35596596624788751, 0.35595084583378755, 0.35595152257787804, 0.35595013661743952, 0.35595092459057776, 0.35595025401323888, 0.35594656527464907, 0.35594445377695583, 0.35594314175848746, 0.35594311375611265, 0.35594233106264528, 0.35594192351088549, 0.35593807487612306, 0.35593859999476907, 0.35593340724469635, 0.35593320948385254, 0.35593291801156973, 0.35593187388367392, 0.35592906005160163, 0.35592747363354083, 0.35592758247587586, 0.35592753436489694, 0.35592674003006541, 0.35592486741027907, 0.35592377830943689, 0.355923641266288, 0.35590824997154136, 0.35589384295542364, 0.35588549551206122, 0.35588186387890991, 0.35588249676557665, 0.35587340999694594, 0.35586725135946085, 0.35584709776209267, 0.35584626142560755, 0.35584492597240258, 0.35584216751240078, 0.35584103326403116, 0.3558410560859081, 0.35583997369575676, 0.35583881900503578, 0.35583718435579265, 0.35583568864252352, 0.35583476782318024, 0.35583409514533082, 0.35583195895956676, 0.35582846005898866, 0.35582448889511709, 0.35582352001787365, 0.35581630496535527, 0.3558151365128196, 0.35581385868422777, 0.35581299705151914, 0.35580661419198212, 0.35580507768964764, 0.35579903553650433, 0.35579601586352416, 0.3557912404454075, 0.35578535509604964, 0.35578525206399314, 0.35577996279845037, 0.35576839349963918, 0.35576620486454147, 0.35576662485802785, 0.35576500011746803, 0.3557641444366148, 0.35576228903708262, 0.35575700267356086, 0.3557535581176558, 0.35575241726995627, 0.35575228697764405, 0.35575110051052522, 0.35574689241387164, 0.3557473008228017, 0.35574109156842809, 0.35573872815025198, 0.35573373065796626, 0.3557340361605818, 0.35573241655389881, 0.35571029659448417, 0.35570766020786637, 0.35570832094907895, 0.35568860098210331, 0.35568915601995471, 0.35568655169407337, 0.35568610467184342, 0.35568623297329738, 0.35568259406630254, 0.35567524659248084, 0.35566972914118206, 0.35566452871218229, 0.35565311228000718, 0.35564131167333135, 0.35564165723800445, 0.35564243252293026, 0.35564233600906614, 0.35563072517847588, 0.35562710856101065, 0.35562586427703408, 0.35562666965486073, 0.35562607968815441, 0.35562575237536753, 0.35562598765167391, 0.35562574921580248, 0.35561885347348438, 0.35561741621825022, 0.35561695802182713, 0.35561077510341599, 0.35560806580886928, 0.35560310742765028, 0.35560055298515603, 0.35559909690463043, 0.35559775385595049, 0.35559568922770757, 0.35559629043944752, 0.35557752296461059, 0.35557604576803792, 0.35557562803691861, 0.35557412151200285, 0.3555711645395665, 0.35556964982324907, 0.35556877423238509, 0.35556863101826741, 0.35556962729603869, 0.35556792328123732, 0.3555657498717214, 0.35556525489855062, 0.3555628903560345, 0.35556102943315249, 0.35556039107202697, 0.35555884254967662, 0.35555384875894569, 0.35555267025184473, 0.35555236789637151, 0.35554218230703705, 0.35554081848316393, 0.35553847506618741, 0.35553233479073132, 0.35553163311412861, 0.35553212295573194, 0.35552849346925863, 0.35552878277206357, 0.35552880955625865, 0.3555273030916567, 0.35552649758067473, 0.35552412109524145, 0.3555250949033737, 0.35552383003711613, 0.35552468791357555, 0.35552029831337967, 0.35551862894345265, 0.35550564609259949, 0.35550046464763618, 0.35550086089989941, 0.35550151197999935, 0.35550159741303528, 0.35549509184352351, 0.35548735433368611, 0.35548198575623113, 0.35548138040662441, 0.35547994628964547, 0.35547903928918956, 0.35547679199247828, 0.35547620299242599, 0.35547518174545972, 0.3554741167694741, 0.3554728802083269, 0.35547310304317487, 0.35547198641974426, 0.35547200590208261, 0.35546053355079255, 0.35545939645037727, 0.35546067706995216, 0.35545931008776854, 0.35545772390319541, 0.35545055262237446, 0.35544852094894946, 0.35544767473364136, 0.35544857631044041, 0.35544679967284415, 0.35543425992564198, 0.35542816354516127, 0.35542116282808084, 0.35541924774322975, 0.35541984574270857, 0.35542215251110842, 0.35541561614771389, 0.35540792370221069, 0.35540645799954962, 0.35540655392735482, 0.35540475471868743, 0.35540543206592956, 0.35540457899741063, 0.35540379289320045, 0.35540277392929337, 0.35540039741310397, 0.3553960804759646, 0.35539474066466437, 0.3553934947310543, 0.35539342344083191, 0.35539245132411895, 0.3553903900892369, 0.35538097560080839, 0.35537902281685302, 0.35537716875946651, 0.35537340785012927, 0.35537205867660338, 0.35537001212780028, 0.35535267595738029, 0.35535171672493793, 0.35534958972158959, 0.35534633138622285, 0.35534132824609987, 0.35533283810899446, 0.35533232556312566, 0.35533248950714136, 0.35532759582984558, 0.355326973234803, 0.355328067915679, 0.35532813517771883, 0.35532833543468451, 0.35532589695057876, 0.35532411109631379, 0.35532284204030617, 0.35532051944283627, 0.35532008839294421, 0.35531962014443896, 0.35531875349533359, 0.35531987035191998, 0.35531423926443451], &#39;rmse-stdv&#39;: [1.4382519803177563e-05, 1.5944986449815816e-05, 1.6934447449321987e-05, 3.3538966126365944e-05, 3.8972172493251649e-05, 5.01651437528025e-05, 5.6210595286631314e-05, 6.1726144990266373e-05, 6.1269996828324311e-05, 6.2656953454165763e-05, 6.5865250591247426e-05, 6.8936563154016806e-05, 8.5286202722617416e-05, 8.9342780355853449e-05, 9.3628141469681027e-05, 0.00010057867053767439, 0.00010932207161183562, 0.00011179516171767385, 0.00012665394878902224, 0.00013872026960907665, 0.00014618562030854347, 0.00014924620359730935, 0.000158121509343585, 0.00016451839711794226, 0.00017316231482113212, 0.00017165548790998444, 0.00017974874248233516, 0.00019607388997681249, 0.00021091563650869956, 0.00020725458019886354, 0.00021126228494290086, 0.00021142749203924264, 0.00021374712620604079, 0.00022303061094113035, 0.00023217735318956066, 0.00024796223989271773, 0.00025525131029146484, 0.00026108499433428513, 0.00027402572070290394, 0.00029786957142530771, 0.00030580665086696573, 0.00032221166784273411, 0.0003345466731069549, 0.00034215843124468066, 0.0003541424596567835, 0.00035234508414737033, 0.00036353206300826258, 0.00038006620876377507, 0.00037819293304766969, 0.00038089599162332828, 0.00038155237231340434, 0.00038711084837382315, 0.00040487291992320079, 0.00041114793921654075, 0.00042958324274140395, 0.0004378005829397122, 0.00044238612082123753, 0.00044990547927726648, 0.00045566420599125942, 0.00046028941029886797, 0.00046453657490578534, 0.00046726036688724958, 0.00047281014062069737, 0.00048923387343997654, 0.00049178434549657589, 0.00049565548758466482, 0.00050079886272515303, 0.00050382771915406484, 0.00050687096224721754, 0.00051005324151640779, 0.0005235872771502814, 0.00053669291610493862, 0.00054535223166490539, 0.00055557241981191543, 0.00057547250089190266, 0.00057873202799122369, 0.00059184606839652828, 0.00059572441073448804, 0.00060169145020501174, 0.00060729344558654852, 0.00061176003992822678, 0.00061877833308765918, 0.00062516245709528216, 0.00062827527792530481, 0.00063389539454556385, 0.00063588899244939191, 0.00063851631956857277, 0.00064189100626675731, 0.00065380669888809436, 0.00065472675530461378, 0.00066616879845721145, 0.00067748382856080091, 0.00067801655238130133, 0.00069205800899344548, 0.00069929029567452293, 0.00070229227243057807, 0.00071524654539287767, 0.00072051219493982099, 0.00073954829249616902, 0.00074230969046508194, 0.00074659009971850797, 0.00075574645565411314, 0.00076618714255410707, 0.0007743033479467098, 0.00078474262294533632, 0.00080333500690721825, 0.00082091451818584271, 0.00083454254530692458, 0.00084712646115103567, 0.00084639969344252341, 0.00084949523403084312, 0.00086125795040339393, 0.00086290465600218687, 0.0008714885624879242, 0.00087024008163503021, 0.00088534672767286247, 0.00089114413398001379, 0.00089800196916379191, 0.00090154807733682266, 0.00090614271438823973, 0.00090652812442196847, 0.00090619035917519596, 0.00091135958509157614, 0.00091405414558748334, 0.00092252634393468163, 0.00093173971474713323, 0.00093519536417338729, 0.0009524495455092702, 0.00096116238211017184, 0.00096618816771888033, 0.00097535468875082981, 0.00098643646321633664, 0.0009949769205989036, 0.00099646092059181763, 0.0010013031797864917, 0.0010115462712425259, 0.0010271925243888951, 0.0010262278209581131, 0.0010272828028351571, 0.0010303605647838832, 0.0010356616390515649, 0.0010472847126207858, 0.0010496271203346435, 0.0010537345022682593, 0.0010580394945775718, 0.0010665056674559158, 0.0010706695705904852, 0.0010773430363293327, 0.0010767203156239814, 0.0010877743428065899, 0.001095553890632034, 0.0011041063244301064, 0.0011087999808180031, 0.0011159840186845053, 0.0011171008943128273, 0.0011219650430991863, 0.0011233690378211103, 0.0011225177319434112, 0.0011215465431931157, 0.0011272548687560311, 0.0011341664168669928, 0.0011350323753033292, 0.0011399304911094499, 0.0011422702301988801, 0.0011443807247353756, 0.0011482142110213294, 0.0011467273221240186, 0.0011500216018755357, 0.0011518393099840675, 0.0011537062343145007, 0.0011535585624731584, 0.0011618814223211269, 0.001163634977371053, 0.0011773284115347575, 0.0011822455410506773, 0.0011836748559577383, 0.0011881558387630744, 0.001193430560554705, 0.0012002551137660538, 0.0012037053316527864, 0.001212763259359822, 0.0012192548898505848, 0.0012202917326672046, 0.001227269822476779, 0.0012386573267173023, 0.0012393539774995202, 0.0012409989961228701, 0.001247231869761506, 0.001247865587851569, 0.0012508592450242435, 0.0012588241298787382, 0.0012612283264725342, 0.0012643359192906591, 0.001270722537920719, 0.0012799778362506239, 0.0012790331304100651, 0.0012835688915995886, 0.0012929293426150656, 0.0012986580592867102, 0.0013010686312033516, 0.0013031385894598385, 0.0013078091153248105, 0.001312232023491588, 0.0013156804045498839, 0.0013181702331749579, 0.0013211917832798546, 0.0013193349204801354, 0.0013266432840206446, 0.0013352104409673066, 0.0013343746264383898, 0.0013313738154753384, 0.0013324390213174267, 0.0013375842659458131, 0.001341236237748344, 0.0013421797920778315, 0.00134464042946409, 0.0013519024208688505, 0.0013511084021268212, 0.0013448595132670845, 0.001348832244996364, 0.0013490696031676656, 0.0013558893503615977, 0.0013717903933912051, 0.0013764806363610906, 0.0013813463257941175, 0.0013816621039283263, 0.0013827991591108175, 0.0013883479602739596, 0.0013899397379788685, 0.0014102403653618748, 0.0014106146765146877, 0.0014155294480768937, 0.0014229888451236148, 0.0014236995145918575, 0.0014250471469643236, 0.0014286856479923627, 0.0014330909055608177, 0.0014418767037442926, 0.0014416858238841389, 0.0014439146896674507, 0.0014534776584119941, 0.0014564089374125471, 0.0014662601836183391, 0.0014674273281674009, 0.0014702694294135904, 0.0014689583143981163, 0.0014677639252456435, 0.0014711977879126618, 0.0014729147276747138, 0.0014810469849520301, 0.0014818268591930246, 0.0014831689431349466, 0.0014802700061584306, 0.0014822800684091654, 0.0014847957237869691, 0.001486366135431204, 0.0014932621509517728, 0.0014981058485970926, 0.0014991129021208421, 0.0015025598536854015, 0.0015026941172814521, 0.0015039376775637789, 0.0015077328950191581, 0.0015106710744874441, 0.0015119267786011512, 0.0015106033779999119, 0.0015141317485489181, 0.0015202420385255697, 0.0015203444679783671, 0.0015217955827537813, 0.0015228255149868229, 0.0015243359801746801, 0.0015256317794730032, 0.0015269884314422829, 0.0015291713069726469, 0.0015299593888200736, 0.0015401524663743284, 0.0015477287153929796, 0.001559025955647806, 0.0015650581129679762, 0.0015686785049974466, 0.0015827378926515292, 0.0015828977663844058, 0.0015840941051227402, 0.001596339037504869, 0.0015918723364589606, 0.0015968371621951213, 0.0016019568597224949, 0.0016030726003869534, 0.0016043490180700215, 0.0016026651796479979, 0.0016055645926894479, 0.0016075800670855508, 0.0016125957276999798, 0.0016113819044142298, 0.0016091129758303781, 0.0016099300631543169, 0.0016150078479033405, 0.0016145037743307516, 0.0016120559454543078, 0.0016191129915636342, 0.0016164847260013522, 0.0016161777473023952, 0.0016170401688991064, 0.0016169732942317684, 0.0016244775213383109, 0.0016262303877511847, 0.0016242726069202313, 0.0016278700989715773, 0.0016336735118153934, 0.0016327487378428307, 0.0016292195501004219, 0.0016300924561521771, 0.0016348304627445387, 0.0016343358170956722, 0.0016364815780912244, 0.0016388881309659707, 0.0016377474599620723, 0.001639727738747877, 0.0016421803133508045, 0.0016423548031162657, 0.0016423625347903568, 0.0016423400647266463, 0.0016487797375431888, 0.0016512588170295996, 0.0016516693016931253, 0.001656489251999827, 0.0016575081058986345, 0.0016558121463241769, 0.0016554752114063158, 0.0016557285262064354, 0.0016614394972801656, 0.0016641031916625447, 0.001666466559477141, 0.0016681824614241929, 0.0016710105672639601, 0.0016732497219881328, 0.0016738854853850099, 0.001678092612077995, 0.0016793837813132771, 0.0016835082709315643, 0.0016843274133171589, 0.0016851485507250294, 0.001686129801097017, 0.001689661154609992, 0.0016904952376786263, 0.001691878776780563, 0.0016931325283398562, 0.0016940136389404995, 0.0016991721820091448, 0.001701234584961904, 0.001704829850252347, 0.0017067129197180669, 0.0017082415860073372, 0.0017091624605735981, 0.0017117979950207617, 0.0017104210133537673, 0.001718270485882731, 0.0017188166670389759, 0.0017212294644209111, 0.0017211835138971167, 0.0017204062472982763, 0.0017215205009507282, 0.0017257537818046998, 0.0017298161508547091, 0.0017307055329942567, 0.0017354138144413229, 0.0017399382840040745, 0.0017409084225088651, 0.0017398606054447934, 0.0017401437654719902, 0.0017408003496102093, 0.0017391050213595046, 0.0017334080837996938, 0.0017408095503423579, 0.0017427053664852952, 0.0017502142520367673, 0.0017582119894264841, 0.0017587601680015225, 0.0017587013105465029, 0.0017602810487898544, 0.0017581701621051935, 0.0017602232622550412, 0.0017645151908937595, 0.0017660158161658366, 0.0017741961893913678, 0.001776601636251162, 0.0017798589752884557, 0.0017808771188585774, 0.0017917325410415326, 0.0017923434656302662, 0.0017911107040769901, 0.0017892415873444446, 0.0017874219609213791, 0.0017883258659915516, 0.0017918178700786205, 0.0017940997879437215, 0.0017955898645828068, 0.0017966160890766946, 0.0018004069847358571, 0.0018003794592422917, 0.0018045395233659955, 0.00180572213912722, 0.0018089558563217276, 0.001810364605792227, 0.0018156724790018789, 0.0018163225246649757, 0.0018132938753614249, 0.0018119174930863707, 0.0018163841717464611, 0.0018177341396786667, 0.0018161423643915664, 0.0018147166332455615, 0.0018179240336973768, 0.0018191743754978344, 0.0018212614715293484, 0.0018217476692545739, 0.0018223785909701844, 0.0018268685749749252, 0.0018309407699539744, 0.0018331616020092593, 0.0018340519634277637, 0.0018342792655745065, 0.0018345501002406345, 0.0018331806739961738, 0.0018331861078434786, 0.0018358444777883181, 0.0018385837225785796, 0.0018384471912201484, 0.0018366278249646595, 0.0018329102446318791, 0.0018339616511134802, 0.0018399558469088088, 0.0018451734305413889, 0.0018427538123771119, 0.0018444058074236938, 0.0018440406852115352, 0.0018493077535105429, 0.0018524277238399312, 0.001852702658056244, 0.0018525251215907512, 0.0018558551893615529, 0.0018524509578909689, 0.001853122383049336, 0.0018521479772463352, 0.0018517905830135347, 0.0018505573341243214, 0.0018482901122879921, 0.0018506059940848509, 0.0018503255153869038, 0.0018498896124245514, 0.0018539244161924825, 0.0018548977584476594, 0.0018566411164389499, 0.0018547908796835283, 0.0018557849726374533, 0.0018594889233533502, 0.0018619760750172025, 0.0018622027479861336, 0.0018637597730606279, 0.0018648995542459149, 0.0018651129710094995, 0.0018683319144918464, 0.001867506124954469, 0.0018692136930494158, 0.0018693122185246731, 0.0018694272284849437, 0.0018723399226927271, 0.0018722649828730314, 0.0018759971061338828, 0.0018803341515068622, 0.0018825196590307326, 0.0018789773452843673, 0.0018844042529606727, 0.0018848861193277611, 0.0018875603387152056, 0.0018879064758644606, 0.0018883808152350186, 0.0018889570479925745, 0.0018891285209930038, 0.0018902480438265294, 0.0018853183727177904, 0.0018865015284598042, 0.0018894278294895681, 0.0018892091231205218, 0.0018905379415869437, 0.0018922508119069098, 0.0018914336448772738, 0.0018943895164825494, 0.0018936672442141291, 0.0018951820785892352, 0.0018975045788043226, 0.0018958276446834597, 0.0018956099935390871, 0.0018958356866249965, 0.001897101205604103, 0.0018955096169814001, 0.0018954776342389964, 0.0018977273885719297, 0.0018986861815924289, 0.0019014708385301658, 0.001905739062798755, 0.0019071288825738284, 0.0019103337464802376, 0.0019137334759379316, 0.0019160683013651634, 0.0019170358159184919, 0.0019180212614735845, 0.0019179937938552782, 0.0019202415974713559, 0.0019246247454512808, 0.0019263168808493327, 0.0019264381684480152, 0.0019228069930352786, 0.001924522530660214, 0.0019229824312794021, 0.0019251258771225501, 0.001925422289916386, 0.0019270287673825725, 0.0019311271206829148, 0.001935440419505697, 0.0019365919536598591, 0.0019407077405433615, 0.0019380643391615784, 0.0019386392152033699, 0.0019422730743455701, 0.0019454713835283092, 0.001945819106382895, 0.0019469391715259592, 0.0019451272346215904, 0.0019524463245709038, 0.0019537679126228717, 0.0019542642962337253, 0.0019570284431980208, 0.0019589552758025753, 0.0019584456535014518, 0.0019571444781766641, 0.0019559574402865372, 0.0019548782945227566, 0.001954860061899882, 0.001956937537183027, 0.0019550646399235281, 0.0019570985441250462, 0.0019551284426320321, 0.001957198561156533, 0.0019614673060868808, 0.0019636900152399774, 0.0019642720494373509, 0.0019636711204057385, 0.0019644065022901202, 0.0019649114656903311, 0.0019646736094322452, 0.0019630356803682788, 0.0019647512452672976, 0.0019681030978896514, 0.0019675964082998872, 0.0019694597568982674, 0.0019679922688945421, 0.0019682588227871601, 0.0019703792977597647, 0.001971781663468483, 0.001971101778305995, 0.0019730514580309794, 0.0019759625628710201, 0.0019781601819893382, 0.0019777259561888612, 0.0019813040197278128, 0.0019832621932814984, 0.0019802128580781574, 0.0019807457740586569, 0.0019823189448673804, 0.0019786841857128662, 0.0019823981316655164, 0.0019818821554810338, 0.0019794709178042472, 0.0019813028119871085, 0.0019822944325773345, 0.001983968760802739, 0.0019858957281105364, 0.0019863863093518229, 0.0019854226799971087, 0.0019890972058181269, 0.0019869537190070727, 0.0019884069320781354, 0.0019892113169616326, 0.0019902010940189454, 0.0019906097729265495, 0.0019877434768103244, 0.0019904155397219496, 0.001991256528774161, 0.0019919621384163577, 0.0019908043417666333, 0.0019883592068811939, 0.0019886697452683563, 0.0019939122675506748, 0.0019943927960987856, 0.001993856151246632, 0.0019966253870810969, 0.0019972177357635247, 0.0019989060617244211, 0.0019992624586540734, 0.0019984372121504934, 0.0020015990567685787, 0.0020063725141040646, 0.0020048126821718797, 0.0020066160176995707, 0.0020075150070289747, 0.0020094662116511759, 0.0020128069599021584, 0.0020136153000363073, 0.0020157178876893745, 0.0020152546885584691, 0.0020148559574249773, 0.0020157379608565177, 0.0020160180816526088, 0.0020188101662038478, 0.0020221288886002361, 0.0020219907186856943, 0.00202247084062871, 0.0020234542737932689, 0.0020263994778925414, 0.0020281468210482912, 0.002025222157889211, 0.0020272545971645184, 0.002030134420409876, 0.00203382047867919, 0.0020353933951149906, 0.0020383656836387206, 0.002039370041539795, 0.0020406416822023855, 0.0020427255090514941, 0.002044396876187043, 0.0020444112737745841, 0.0020434744116513089, 0.0020419201328000297, 0.0020441807308861967, 0.0020461636566933743, 0.0020465926273435398, 0.0020467251161198912, 0.0020468085091599167, 0.0020475273370197606, 0.0020457155860071513, 0.002042664952306488, 0.002044870931292935, 0.0020438729025165966, 0.0020429115711153743, 0.0020456769634378496, 0.0020456815600752173, 0.0020449146615408669, 0.0020450346624464439, 0.002045422677260285, 0.0020480710609685412, 0.0020498347441744642, 0.0020535476699950531, 0.0020550604444256959, 0.0020547326150429689, 0.0020579940742644566, 0.002059869822531704, 0.002059375496680017, 0.0020588418429847654, 0.0020582966021161827, 0.0020597197805265698, 0.0020612673001036029, 0.0020621496425284576, 0.0020611570510095786, 0.0020596177839616566, 0.0020615303933334997, 0.0020596806766644371, 0.002061629844123343, 0.0020652955570928167, 0.0020660937698568039, 0.0020657112968952033, 0.0020664435307583087, 0.0020665349368111691, 0.0020652746077493597, 0.0020644642171077929, 0.0020652697854928442, 0.0020658225784066803, 0.0020664435835499873, 0.0020676079436027072, 0.0020698715028046898, 0.0020705403146640722, 0.0020716105617931703, 0.0020706607119304879, 0.0020697380521260619, 0.0020708732993232198, 0.002071400399560054, 0.0020717711290776865, 0.0020716978754081765, 0.0020719634262761737, 0.0020720323369313345, 0.0020729701702549897, 0.0020767320041854978, 0.0020768040468639071, 0.0020752407445889824, 0.0020746246715888572, 0.0020769183571982841, 0.0020777277354706359, 0.002078184276267759, 0.0020780555417644142, 0.0020786473230948758, 0.0020801100626492049, 0.0020796889536866463, 0.0020810110658588286, 0.00208186719347353, 0.0020832687564610879, 0.0020846934803192283, 0.0020824292640075704, 0.0020816809493149169, 0.0020821877827302466, 0.0020827833272978294, 0.0020822172713612342, 0.0020818354929990892, 0.0020830196773409999, 0.0020836120085830682, 0.00208604078883459, 0.0020868230352958047, 0.002085190732465601, 0.0020842041212087914, 0.0020833583583484265, 0.0020850249032833452, 0.002083446314216056, 0.002085432613728253, 0.0020853347251983453, 0.0020857785483365432, 0.0020862399170316014, 0.0020893665286684363, 0.0020884195284927758, 0.0020900891050245375, 0.002088563426957353, 0.0020904334046236914, 0.0020840137073942933, 0.0020844081826061254, 0.00208527826224258, 0.0020858899928488227, 0.0020851985242318987, 0.0020847596089382564, 0.0020851017587565231, 0.0020854668480875947, 0.0020867253042481603, 0.0020825323365672044, 0.0020804247473177311, 0.00207902537336566, 0.0020805561859363899, 0.0020825629880843609, 0.0020828942191100145, 0.0020833562134288338, 0.0020859507903447298, 0.0020879516307194079, 0.0020873410887028753, 0.0020888577862997939, 0.0020880791080431838, 0.0020863004172947406, 0.0020869914985004912, 0.0020854088963577046, 0.0020848270437385814, 0.0020863465716696727, 0.0020863971448090461, 0.002085895879603129, 0.0020866866421823836, 0.0020858423249477138, 0.0020865806714052268, 0.0020880386735370395, 0.0020904556941458712, 0.0020898099744168092, 0.0020907304411547571, 0.0020880574196647747, 0.0020867566467447449, 0.0020872682195724, 0.002088235585310831, 0.0020889404511582371, 0.0020879300088917419, 0.0020894395912821242, 0.0020910048946384337, 0.0020901548165893824, 0.0020921304956472743, 0.002091948799135383, 0.0020926383781831202, 0.0020961400687121009, 0.0020974765017899985, 0.0020975621979197018, 0.0020983384738233655, 0.002099739431049821, 0.0020964495770013172, 0.0020963434034851568, 0.0020970954235598705, 0.0020997006128724333, 0.0021009215289007196, 0.0021009379862387888, 0.0021036295887891425, 0.0021035435681250915, 0.0021046777748139974, 0.0021044641157498536, 0.0021070376657663409, 0.0021066013477942184, 0.0021079987516344168, 0.0021077305730214956, 0.0021044324803571285, 0.0021052972364069122, 0.0021071098479494054, 0.0021054055166423715, 0.0021028942382239203, 0.0021031612569584607, 0.0021008394922413455, 0.0021015293739456921, 0.0021039839243192451, 0.0021027571705111501, 0.0021010550271472689, 0.0021049697066673543, 0.0021047736726514266, 0.0021059305567477761, 0.0021065820790100082, 0.0021091999578527216, 0.0021099542907573568, 0.0021123705302541151, 0.0021155015466911118, 0.002115749867847197, 0.0021157710948408806, 0.0021143297880627993, 0.0021143588450721665, 0.0021138115868719015, 0.0021141136555188349, 0.0021156540999081766, 0.0021172655230630166, 0.0021168244586819863, 0.0021172144197584434, 0.0021171840510909898, 0.0021166182754543943, 0.0021170827857876569, 0.0021176747078206831, 0.002117622547101696, 0.0021173880180779129, 0.0021181678495442616, 0.0021223533975266496, 0.0021234771118893783, 0.0021220039432825193, 0.0021223928790342415, 0.0021217343564600378, 0.0021227935405524756, 0.0021236189625604123, 0.0021248416681319618, 0.0021260802992115677, 0.0021261064075476402, 0.002125869478011278, 0.0021256699009921868, 0.0021259521065456711, 0.0021264296350032201, 0.0021266350727047346, 0.0021292785194534417, 0.0021291982218528444, 0.0021318898975011117, 0.0021331143098849864, 0.0021339109468975757, 0.0021356809726092701, 0.0021350315036925424, 0.0021362687375841185, 0.0021376934611062963, 0.0021392721202939382, 0.0021370672466808479, 0.0021365442870958352, 0.0021390128804074887, 0.0021417867428066492, 0.0021418621570988981, 0.0021429715492913733, 0.0021438929983544474, 0.0021456959258635512, 0.0021457190693855424, 0.0021458686211856468, 0.0021440749951369994, 0.0021439688821871194, 0.0021430817376396115, 0.0021404330251260811, 0.0021397080321371425, 0.0021384650630162034, 0.0021408825350996414, 0.0021410126714839959, 0.0021404414631396415, 0.0021431990831187677, 0.0021422696767266023, 0.0021440687715057606, 0.0021466801208255733, 0.0021484608963251747, 0.002150440041171865, 0.0021505663051952554, 0.0021518205451712389, 0.0021547150906102751, 0.0021552940344298035, 0.002157605696944254, 0.0021568239086184201, 0.0021572978012171398, 0.0021576380888900901, 0.0021548950333832319, 0.0021558450309646362, 0.0021556649434484607, 0.0021571877190858287, 0.002157545077383605, 0.0021555293929827373, 0.0021554922451599051, 0.0021540060227442704, 0.0021528324028905373, 0.0021554169741892872, 0.0021582917734275504, 0.0021592710796247633, 0.002160079039894985, 0.002160884608677761, 0.0021615185400337103, 0.0021607891358186512, 0.0021594814328177356, 0.0021576127058843303, 0.0021561856645266348, 0.0021545117136383737, 0.0021537293982355371, 0.0021543266964634991, 0.0021560082911526108, 0.0021550770147451461, 0.0021555010895529395, 0.0021551615861900289, 0.0021550791366685067, 0.0021558411528400045, 0.0021558214100165941, 0.0021530586084398799, 0.002155024966768978, 0.0021544822926429625, 0.0021535115679054344, 0.0021567790272885422, 0.0021565512010512967, 0.0021558135733006152, 0.0021552770691596911, 0.0021560402581909284, 0.0021537201616366737, 0.0021513310596860845, 0.002150239827466446, 0.0021525405903037751, 0.0021532126070241135, 0.0021526262706476011, 0.0021516226971751169, 0.0021514814910754392, 0.0021547751054331972, 0.0021570109341648479, 0.0021583542937903986, 0.0021590229649626805, 0.0021601998613421359, 0.0021597617866658913, 0.0021599130524191821, 0.002160771891300118, 0.0021641549526554378, 0.002164904220261353, 0.0021646588468988686, 0.0021653097897806899, 0.0021649981084435228, 0.0021659326090153709, 0.0021651174511481408, 0.0021674191672696108, 0.002167269552345751, 0.0021681141839825758, 0.002166720319649729, 0.0021708263521565607, 0.0021714255899860863, 0.0021686238171342671, 0.002167996166738038, 0.0021671800453450002, 0.0021669553263580529, 0.0021681028655722349, 0.0021704614015263203, 0.0021712423892635896, 0.002167342157892823, 0.0021667095346324483, 0.0021648828697001911, 0.002166339998763984, 0.0021693499282880602, 0.0021733308304616042, 0.0021719452478613905, 0.0021702527163647233, 0.0021706833391196916, 0.0021678205044125599, 0.002169338418576991, 0.0021696167331171829, 0.002168969438174931, 0.0021682110518367105, 0.0021690835475418335, 0.002169270160164413, 0.0021701070078666545, 0.0021709454941310526, 0.0021710323568224729, 0.0021723956367953189, 0.0021717729274676151, 0.0021731651918309547, 0.0021742230382545387, 0.0021731979298161108, 0.0021744963442974084, 0.0021739445876146411, 0.0021750883707282844, 0.002174347058876929, 0.0021743562534684902, 0.0021733083218818281, 0.0021732103818179471, 0.0021729681502087035, 0.0021733375318094368, 0.0021723542957300637, 0.0021733168210762528, 0.0021740239366431673, 0.0021742380178618741, 0.0021743323924452154, 0.0021748271304012916, 0.0021746009314011954, 0.0021739815988643294, 0.0021732623155723239, 0.0021756367517893607, 0.0021751708362020617, 0.0021783305014053673, 0.0021774694734135241, 0.0021780195013192901, 0.00217651274361366, 0.0021759376171936614, 0.0021747011123196954, 0.0021750022723181829, 0.0021768733491804612, 0.0021788805247238271, 0.0021793102358656767, 0.0021794384037017848, 0.002180169303068713, 0.0021819728143685018, 0.0021831311732725665, 0.0021836575803443033, 0.0021853331465671545, 0.0021855348229699706, 0.0021869151800041871, 0.0021859953082145177, 0.0021857954330044014, 0.0021867364225966516, 0.0021852407605410583, 0.0021857434515186653, 0.0021869492783504127, 0.0021863787789884251, 0.0021880774651408929, 0.0021858484830896037, 0.0021890621570274599, 0.0021889135313408626, 0.0021879387127164631, 0.0021878246239968912, 0.0021877978606892265, 0.0021874490180988685, 0.0021847987764308067, 0.0021858977923783158, 0.0021874515988333852, 0.0021905900069824598, 0.0021916683625523405, 0.0021908147709886936, 0.0021905461339918734, 0.002191377072050357, 0.002191879302123982, 0.0021951024170686413, 0.0021932928458407242, 0.0021943832738838104, 0.0021951487258518639, 0.0021948942930480929, 0.002194627459413808, 0.0021974904717921187, 0.0021965202855469242, 0.002196392876525107, 0.002197344112193808, 0.0021963419183058806, 0.0021964312053925467, 0.0021985033787153825, 0.0021992566593208368, 0.0022003883195105887, 0.0022015436596196322, 0.0021996665269839212, 0.002199329361010709, 0.0021970251686287768, 0.0021977795464616259, 0.0021970984884486485, 0.0021974369747100385, 0.0021978223059609648, 0.0021961884502137421, 0.0021962830547758399, 0.0021985475584254794, 0.0022003019516228066, 0.0022002197857461376, 0.0021995789376112679, 0.0021997361535332233, 0.0022018680777205761, 0.0022035404130999621, 0.0022041391712887769, 0.0022023693734882009, 0.0022012266555062488, 0.002202786618432785, 0.00220072832143742, 0.002202278022252549, 0.0022015322419885802, 0.0022016297754043376, 0.0022041096142025899, 0.0022032572914081694, 0.002202988840069438, 0.0022024710783106107, 0.0022042093666645797, 0.0022034535009945325, 0.0022045241425228414, 0.0022051606123730462, 0.0022049477406234624, 0.0022043158888167083, 0.0022045947054482071, 0.0022039353196394297, 0.0022025908075217746, 0.0022017786101414367, 0.002204638062642933, 0.0022036642238102023, 0.0022071645696192625, 0.0022103919457125529, 0.0022101163129491226, 0.0022100399932171249, 0.002209383011844025, 0.002208435105441533, 0.0022116350671084082, 0.0022115373649970412, 0.0022108747260865592, 0.0022099701600957864, 0.0022111723734263167, 0.0022106259578902623, 0.002212577661274532, 0.002211562807052089, 0.0022116742118768033, 0.0022112243017574024, 0.0022128468525146896, 0.0022108357692359375, 0.0022123846858733125, 0.0022108221003873615, 0.0022091317816370372, 0.0022066740476005126, 0.00220456929794133, 0.0022038780908827768, 0.0022042598045494887, 0.0022080793834603563, 0.002207525764094265, 0.0022074493783652135, 0.0022077623121052399, 0.0022069421229182306, 0.0022079431606418878, 0.0022108111452046231, 0.0022103913974462503, 0.0022103075788035003, 0.002211613598784468, 0.0022114610042817924, 0.0022077877403537251, 0.002204021156341926, 0.0022042494616022904, 0.0022027006076430701, 0.0022023309859555565, 0.0022045313209037186, 0.0022049100680353977, 0.0022039246350936239, 0.0022023247008491806, 0.0021988674017883714, 0.0021975533570436244, 0.0021976299815444434, 0.0021970931548740821, 0.0021957972613530097, 0.0021955179834161846, 0.0021936020230939046, 0.0021950633600336051, 0.0021924805429887218, 0.0021920143098632925, 0.0021921561751403048, 0.0021927285872778944, 0.0021919576904230473, 0.0021922957038912065, 0.0021902697455599791, 0.0021914250314282306, 0.0021937622544660829, 0.0021916222242424678, 0.0021919842722374163, 0.0021920937270453163, 0.0021910560666852982, 0.0021909873309643919, 0.002189775413113534, 0.00218761419735124, 0.0021890685321467648, 0.0021904117623724123, 0.0021895154649256752, 0.0021872434687267377, 0.0021875287202725202, 0.0021900146997638841, 0.0021896491114674435, 0.0021864500897283809, 0.0021827270696547145, 0.0021828221273642616, 0.0021845403301077185, 0.0021826815631031029, 0.0021821111542527495, 0.0021835558932681116, 0.0021868597397821576, 0.0021863109046261868, 0.0021855176528150361, 0.0021876276669384642, 0.0021905399381263314, 0.0021915982687378431, 0.0021923645292128867, 0.0021926188541779674, 0.0021904766226812891, 0.0021885611158953466, 0.002188457326743509, 0.0021890451944558235, 0.0021881547172538322, 0.0021870386940173844, 0.0021871000283342087, 0.0021875921371392673, 0.002187495202118795, 0.0021882651705085955, 0.0021895495389429695, 0.0021907578058029146, 0.0021908237888145039, 0.0021906373843926237, 0.0021916571687770688, 0.0021934745731309381, 0.0021954073237948016, 0.0021949097830577333, 0.0021942355197610308, 0.0021945009080490932, 0.0021952780242181047, 0.0021953305010923843, 0.0021956179002391276, 0.0021959010247948373, 0.0021963790999096583, 0.0021982277183592897, 0.0021995995358719588, 0.0021981721009326008, 0.0021987655008152583, 0.0022004083365982354, 0.0022006139445742683, 0.0022030651165121655, 0.0022024677315342859, 0.0022022286773674114, 0.0022046719710587981, 0.0022048239763996784, 0.0022044597498434796, 0.0022069376580748853, 0.002207244767755735, 0.002207460350732794, 0.0022076205403614465, 0.0022062991934375985, 0.0022080762267679362, 0.0022077223216944919, 0.0022069704771019668, 0.0022067320372155357, 0.0022060595601253105, 0.0022080584981259265, 0.0022055692874359549, 0.0022061231679220291, 0.0022071524638931951, 0.0022060396716239885, 0.002205761263985152, 0.0022053932127775941, 0.0022053773943256287, 0.0022056582879632657, 0.0022074554227522183, 0.0022078392320610354, 0.0022072903694397845, 0.002207207825484273, 0.002207884472792335, 0.0022047082376857505, 0.0022015772367067156, 0.0022014728888829316, 0.0022005193567482678, 0.0021976645718852191, 0.0021962349113531028, 0.0021976884863632441, 0.0021981703941264716, 0.0021966406287320783, 0.0021977362836386556, 0.0021951810127897493, 0.0021968207107544015, 0.0021989897537708822, 0.0021984797650251562, 0.0021987173893344108, 0.0021961711952548767, 0.0021972303464074081, 0.0021978618246752631, 0.002198015102036789, 0.0021982758905231331, 0.0021984528809666651, 0.0021977374803512249, 0.0021973816202045448, 0.0021959308930202498, 0.0021987216014679768, 0.0021990524897817949, 0.0021981238899835762, 0.0021971764206670011, 0.0021972608000022311, 0.0021974416521492383, 0.0021992458755880919, 0.0021994324194604753, 0.0022011156314577967, 0.0022012198735127768, 0.00220016283355093, 0.0021995918421554026, 0.0021970932906927856, 0.0021963620679893592, 0.002195887151836788, 0.0021954512960826495, 0.0021938506496176208, 0.0021913620218475065, 0.0021902437128850506, 0.0021895719644350982, 0.0021896343410441697, 0.0021883357188665531, 0.002188651944113352, 0.0021884057124098035, 0.0021880178272113316, 0.0021882131400709818, 0.0021880183916754073, 0.0021894804658821971, 0.0021919085059521688, 0.0021921085241755861, 0.0021922653704185046, 0.0021916622482573396, 0.0021864018012218594, 0.0021866650407457453, 0.0021879330202213866, 0.0021876569752990013, 0.0021881594963478936, 0.0021874737131969088, 0.0021885509941114772, 0.0021894483184205782, 0.0021916168214574259, 0.0021918249036149895, 0.0021940894684986576, 0.0021973258612839283, 0.0021951140516573379, 0.0021951226910241644, 0.0021952121669686925, 0.0021956285047265546, 0.0021975200825872778, 0.0021969721392320063, 0.0021982010753532856, 0.0021993005978402469, 0.0021977864994441316, 0.0022005873778704198, 0.0021982449918016176, 0.0021987374372082363, 0.0021978155147726664, 0.0021986313821409993, 0.0021987200484330331, 0.0022000360771734263, 0.0022016411046623828, 0.0022006779097092995, 0.0022002263289297207, 0.0022009858252571369, 0.002199689620295011, 0.0022016827016635922, 0.0022021483932731923, 0.0021986948594533225, 0.0022012461508427472, 0.0022012065992803257, 0.002203424606667679, 0.0022022902320941629, 0.0022024980272885401, 0.0022037188880256213, 0.0022035691304210878, 0.0022034566808812137, 0.002204902901368707, 0.0022076282237803312, 0.0022090917706365826, 0.0022074430536818792, 0.0022059515228023932, 0.0022050686446923209, 0.0022065682895164875, 0.002205038136315075, 0.0022067167045543303, 0.0022058950270326732, 0.0022051167863501552, 0.0022059797941956717, 0.0022070211541454893, 0.0022086258553203784, 0.0022091001447713593, 0.0022081921938047421, 0.0022070771497644283, 0.0022078075761211417, 0.0022064692501110078, 0.002205760442902768, 0.0022050773377411954, 0.0022063585294423713, 0.0022066727506916267, 0.0022083088604961617, 0.0022095794392101515, 0.0022109191137365315, 0.0022100674428906534, 0.002209486292032391, 0.0022093978228140813, 0.0022098924634619131, 0.0022087122908602068, 0.0022093451019874369, 0.0022092340242703601, 0.0022087122858964418, 0.0022095446897988551, 0.0022089547008954757, 0.0022072202538960285, 0.0022056410239505268, 0.0022056486508002528, 0.0022057236039882272, 0.0022058012307098487, 0.0022081036935504327, 0.0022089972419017615, 0.0022096495735202948, 0.0022101810729305504, 0.0022101875003042557, 0.0022098950779507872, 0.0022093038849177078, 0.0022113438828906019, 0.0022109506990889366, 0.0022080640632472343, 0.0022070870057366121, 0.0022096374423804213, 0.0022087004595084379, 0.0022089550157981192, 0.0022080878287371142, 0.0022088546015166218, 0.0022092313844147117, 0.0022100309708212324, 0.0022114715500922044, 0.0022102898550226298, 0.002210735926496316, 0.0022148501915492504, 0.0022165514581280344, 0.002218146698311033, 0.0022185362115221224, 0.0022170813419013268, 0.0022179599743069513, 0.002218848827355532, 0.0022204930540158756, 0.0022215338486774915, 0.0022225686174808666, 0.0022234205814343892, 0.0022232196457531144, 0.0022270627402445928, 0.002226199300856289, 0.0022272044551000002, 0.0022240773970043249, 0.0022235966437825962, 0.0022231206666960984, 0.0022232788696283558, 0.0022237433514788935, 0.0022236809853337022, 0.0022241707868935136, 0.0022244224138592444, 0.0022235312753208785, 0.0022230390411517044, 0.0022230949892618184, 0.0022251119126186596, 0.002226973628730236, 0.0022289863606177697, 0.0022306962636626918, 0.002231014889742409, 0.0022327724345924593, 0.0022312223875832187, 0.0022343064720609929, 0.00223530932810575, 0.0022352464465186937, 0.0022337480426352875, 0.0022336022859301375, 0.0022339857159743882, 0.0022340727705513733, 0.0022358702286782747, 0.0022356984504430274, 0.0022363035244282584, 0.0022350847579151287, 0.0022350587975956205, 0.0022367041817411529, 0.0022360603990187974, 0.0022374052630063338, 0.0022367970093013184, 0.0022366802815697016, 0.0022365868457913793, 0.0022376655961327116, 0.002237989185941704, 0.0022396896159934346, 0.0022397383840221498, 0.002240452958934522, 0.0022403130601119808, 0.0022417248844034013, 0.0022403223971650017, 0.0022407781343092136, 0.0022399708261209698, 0.0022402377497540297, 0.0022411891448092533, 0.0022412038288503289, 0.0022411381319984891, 0.002240523362036337, 0.0022405449396025951, 0.0022416883178857459, 0.0022444793646268627, 0.0022392830888503402, 0.0022402679740183224, 0.0022418054634775549, 0.0022425164078997187, 0.0022419913543228007, 0.002242427595478953, 0.0022421873650512558, 0.0022435489681841394, 0.0022433374648728375, 0.0022425459451030673, 0.0022425872056051573, 0.0022438314388968997, 0.0022452793974038739, 0.0022448214404196375, 0.0022438941475021214, 0.002244312750814098, 0.0022445592819121886, 0.0022449151951415215, 0.0022473052515436498, 0.0022473107628525721, 0.0022466741164506188, 0.002246341373065903, 0.0022471395567400065, 0.0022477343087937939, 0.0022489135868355306, 0.0022482699571048461, 0.0022481941196655655, 0.0022483688913966424, 0.0022504583092166652, 0.0022514341557553445, 0.0022513426060357866, 0.0022518853808370333, 0.0022534143576817809, 0.0022550771293323997, 0.002253318000086115, 0.0022508131609587207, 0.002251188976588144, 0.0022524640793171332, 0.00225326758015544, 0.002253290312729674, 0.002253055800585929, 0.0022536400412811858, 0.0022524669786244196, 0.0022541272554337005, 0.0022564875646355069, 0.0022570403926767614, 0.0022578370121812412, 0.0022585555350355655, 0.0022575067677136729, 0.0022602716925806303, 0.002258639962670307, 0.0022591388745070483, 0.0022604620877066041, 0.0022610104623800153, 0.0022610587508901888, 0.0022607595194113709, 0.0022598878511811233, 0.0022607569461637221, 0.0022610637413998892, 0.0022602380930944938, 0.0022614892107159982, 0.0022613752174474194, 0.0022608689893368755, 0.0022617405039102286, 0.0022616000620759762, 0.0022626203284985395, 0.0022617969821812282, 0.0022622490207248238, 0.0022605832561174155, 0.0022608571615414436, 0.0022613029098163553, 0.0022613589304615756, 0.0022619015235306379, 0.0022600419909288567, 0.0022607802935965399, 0.0022608956065366696, 0.0022610748336765194, 0.0022615154284449583, 0.0022603527610967761, 0.0022591376396063705, 0.0022591075147190844, 0.0022574245420304523, 0.0022562693497214835, 0.0022576915227338267, 0.0022588261707909649, 0.0022594950094027219, 0.0022597555729951084, 0.0022605502562003535, 0.0022615272491273587, 0.0022612274783379302, 0.0022593787287259475, 0.0022597902369586118, 0.0022591679016266395, 0.0022579757705834961, 0.0022590060628512034, 0.0022593740365017406, 0.002260310727451518, 0.0022610391244140262, 0.0022595530698606256, 0.002258289311743916, 0.002258743930714761, 0.0022587811786948411, 0.0022599140680738686, 0.0022606370150030871, 0.0022600484944859042, 0.0022577721137904661, 0.0022584972053949358, 0.0022585042120354358, 0.0022581620586794885, 0.0022583698913399611, 0.0022574233023212285, 0.0022570965497122884, 0.0022571105243103535, 0.0022574186219356932, 0.0022598373713052572, 0.002264928176966757, 0.0022661517656377412, 0.0022657722781834885, 0.002267129102180744, 0.0022672383894483858, 0.0022678123511787685, 0.0022692130400439291, 0.0022692589845590616, 0.0022705170694477333, 0.0022698458867064226, 0.0022702768692725939, 0.0022716434279901635, 0.0022714182580037645, 0.002271101821287948, 0.0022711575755040726, 0.0022719669867671682, 0.002274781682803045, 0.0022740573145905618, 0.0022721257662656727, 0.0022738483956925961, 0.0022729749810726581, 0.0022743061312230279, 0.0022753257999983891, 0.0022757606476313456, 0.0022772201019096994, 0.0022730662830235714, 0.0022749878969765217, 0.0022757850425283279, 0.0022743967449457281, 0.0022735207143485384, 0.0022729652905849073, 0.0022725517406794364, 0.0022727131108530264, 0.002273754496134742, 0.0022748636539920465, 0.0022752472556700587, 0.002276238127987427, 0.0022773172763410443, 0.0022778238150861752, 0.002278600981407274, 0.0022787878905147991, 0.0022781421234953775, 0.0022782931218011233, 0.0022776086919728232, 0.0022780542968095043, 0.0022785646427251503, 0.0022772561397302405, 0.0022747469538452977, 0.0022741206094321039, 0.0022758746071732438, 0.0022747548508726531, 0.0022772893779271042, 0.0022773444272708916, 0.0022776585763261748, 0.0022771670190715118, 0.0022768442372240252, 0.0022778931156695876, 0.0022792238676517976, 0.002278743314203724, 0.0022789725295490047, 0.0022788640447673979, 0.0022792653193962485, 0.0022796352739926353, 0.0022788165996493752, 0.0022791548086940339, 0.0022805898807001018, 0.0022827486684774143, 0.0022830691464608538, 0.0022824842115973911, 0.002284942318998672, 0.0022862196166297416, 0.002283554658464105, 0.0022837313970319826, 0.002283959564455961, 0.0022843213864160816, 0.0022825828727502939, 0.0022823182960688307, 0.0022816336693619365, 0.0022809564468555558, 0.0022822902196971426, 0.0022810767606736876, 0.0022818807027403564, 0.0022786207982718764, 0.0022781646184575756, 0.0022795817490334769, 0.0022793158523481722, 0.0022794937816138765, 0.0022809916402030056, 0.0022822580490868341, 0.0022823415667232862, 0.0022828004284830353, 0.0022826777471884178, 0.002282696006949705, 0.0022841836741324954, 0.0022855926046158196, 0.0022861120432791969, 0.0022876435241064555, 0.0022887742894844212, 0.0022883643841064387, 0.0022879441703232192, 0.002288741365589791, 0.0022885140865160614, 0.0022903643380357826, 0.0022877182856527187, 0.0022882572794873379, 0.0022893880480889884, 0.0022895988780402173, 0.0022912707587998396, 0.0022913982566694657, 0.0022930357364690786, 0.0022916862843867262, 0.0022921664134506834, 0.002291474162360088, 0.0022904710924191514, 0.0022912615225462148, 0.0022921079921688595, 0.0022912784906913282, 0.0022908962968919734, 0.0022925287603239204, 0.0022920220109474512, 0.0022915905681056442, 0.0022909623470010157, 0.0022914353727334648, 0.0022925734789255628, 0.0022951065559525404, 0.0022962597127311381, 0.0022970711390536396, 0.0022986043636797474, 0.0022983194265869702, 0.0022986115548451654, 0.0022976281546231068, 0.0022953818391023479, 0.0022972486413329711, 0.0022976529231551277, 0.0022974955263315514, 0.0022971996689325632, 0.0022982037065234426, 0.0022978811981364173, 0.0022992733167581398, 0.0022998661348686326, 0.0023000413643564852, 0.0023002888287919792, 0.0022992735076007667, 0.0022988066510583769, 0.0022988790719674517, 0.0022966204291352617, 0.002295711824807008, 0.0022952930066603883, 0.0022964710776814711, 0.0022975157659950976, 0.002298518625338235, 0.0022995900246002856, 0.0022997251075754679, 0.0023023723953334587, 0.0023017519288849519, 0.0023013993841673433, 0.0022999548397572771, 0.0023023840478925395, 0.0023020782783065448, 0.0023017910964237367, 0.0023030249464108931, 0.0023030831794953171, 0.0023019746860803848, 0.0023031683634356626, 0.00230273645795637, 0.002303253899551016, 0.002302984198277951, 0.002303175715157672, 0.0023022464558866318, 0.0023036807614964525, 0.0023045225411426934, 0.0023063667358093876, 0.0023063582201780816, 0.0023063212226980493, 0.0023061107268720535, 0.0023066437164428835, 0.0023060458155362584, 0.0023053764889739417, 0.0023058971791356866, 0.0023062700613284536, 0.0023064517125988156, 0.0023080185191630322, 0.0023100391395550796, 0.0023094967324022273, 0.0023118883758980226, 0.0023123810920042316, 0.0023126486275757752, 0.0023127996474186486, 0.0023134259446937009, 0.0023139293002762242, 0.0023147380663894913, 0.0023143389568029381, 0.0023132079323032817, 0.0023149739450884759, 0.0023159155499319346, 0.0023172554592222547, 0.0023186905601485131, 0.0023194945190711841, 0.0023197029993463733, 0.002318594498915783, 0.0023179249914763922, 0.0023165571005883501, 0.0023160093582178746, 0.0023156477639057031, 0.0023132486234567944, 0.0023124565951325991, 0.0023140474992084993, 0.0023144825514435454, 0.0023145631647202534, 0.0023132559038016231, 0.0023136017351670208, 0.0023127384701715345, 0.002312222346480969, 0.0023127712732631035, 0.0023133811067081182, 0.0023149029554990533, 0.0023159525346702829, 0.002316186731586586, 0.0023156983902908467, 0.0023156086468376119, 0.0023150585886352586, 0.0023154149458547418, 0.0023159517950422127, 0.0023169453564380522, 0.0023178784289241273, 0.0023192294883275758, 0.0023194867497530114, 0.0023204291852305992, 0.0023189925647153619, 0.0023188360394860389, 0.0023212597035285219, 0.0023197263367245005, 0.0023202118780687349, 0.0023226549390376072, 0.0023243111892366844, 0.0023256498298310615, 0.0023268986451584622, 0.0023270032326053371, 0.0023267618752892591, 0.0023276049148384099, 0.0023272314828310012, 0.002327623524131977, 0.0023297116878424769, 0.0023296712167276907, 0.0023293950687944911, 0.0023306876969288949, 0.0023309725183619165, 0.0023305359144567875, 0.0023284348354713773, 0.0023288114391507597, 0.0023289165667305192, 0.002327301522099133, 0.0023245859033761539, 0.0023256366690810643, 0.0023266473789664175, 0.0023255406364874835, 0.0023248801056075788, 0.0023252933702484603, 0.0023279027419403362, 0.0023292397809922781, 0.0023306610081019531, 0.002331719536868628, 0.0023315650826567684, 0.002332979148608116, 0.0023302020543960137, 0.002331241642904497, 0.0023343061123562159, 0.002333414964152469, 0.0023332991615282773, 0.0023344139064951312, 0.0023355537407191789, 0.0023374164657377077, 0.0023377681398223326, 0.0023361916420851017, 0.0023376993019305708, 0.002338221993964283, 0.0023386443399849195, 0.0023403814491303673, 0.0023402029303871055, 0.0023393651479561091, 0.002340837822488884, 0.0023413414087369759, 0.002341409066645535, 0.0023400277259203761, 0.0023405045920188998, 0.0023403031310548278, 0.0023408550479112116, 0.002339816318917907, 0.0023397799890765175, 0.002339999029349297, 0.0023411595010877986, 0.0023408840944018311, 0.0023419310363873053, 0.0023427083983148743, 0.0023434448019905431, 0.002343344621164437, 0.0023440037286783949, 0.0023432729673835413, 0.0023444705385115019, 0.0023448876377991875, 0.0023458783682573361, 0.0023442465437201244, 0.0023446661041412434, 0.0023446550845081363, 0.0023459504269411728, 0.0023436761571510008, 0.0023443328980007998, 0.0023455032306313323, 0.002345708340395861, 0.0023462563669162575, 0.0023476421922817982, 0.0023465534268602784, 0.0023469358091741505, 0.0023469758376882741, 0.0023477255520415424, 0.0023483491079908819, 0.0023491960337021656, 0.002349309151073899, 0.0023478855357185781, 0.0023479079246907339, 0.0023482129798465534, 0.0023480456452692304, 0.0023479067625023502, 0.0023467460871703792, 0.0023473924652290413, 0.0023477997467624247, 0.0023476288790767307, 0.0023474588622029017, 0.0023470247541066327, 0.0023468219830924666, 0.0023470478057809104, 0.002346413491411918, 0.002346774680495105, 0.0023474334728041087, 0.0023472358368911779, 0.0023477235671347028, 0.0023486339416737504, 0.002348122657542711, 0.0023484942342711611, 0.0023485399192276152, 0.0023472945463853872, 0.0023473843665377733, 0.0023479859364269472, 0.0023469517498890722, 0.0023479157472998018, 0.0023490767946679893, 0.002350485762486366, 0.002349637914006374, 0.0023502210331182204, 0.0023498104852021631, 0.0023490057148708482, 0.0023481527518880016, 0.0023486155102059631, 0.0023505312972790051, 0.002350683425300466, 0.0023497664069314478, 0.0023478574685478395, 0.0023479441054944754, 0.0023490059768357828, 0.0023477916822808604, 0.0023496450465761167, 0.0023498882484964578, 0.0023499272765241776, 0.0023503802726612898, 0.0023505032314124186, 0.0023505913888941751, 0.0023495937400354657, 0.0023501506465168627, 0.0023502962746679719, 0.0023495390584853475, 0.0023492045800513183, 0.0023485631607313684, 0.0023494312846921565, 0.0023500541500086347, 0.0023502312893622962, 0.0023503258186959876, 0.0023517825630334156, 0.0023516129210518527, 0.0023527494460222078, 0.0023518867719315374, 0.0023502877845959147, 0.0023506167744259811, 0.0023492709028843611, 0.0023474329246867117, 0.0023499145020395534, 0.0023471720546553131, 0.0023473458649179825, 0.0023488569203597904, 0.0023460580112955789, 0.0023472555555636571, 0.0023464705848607144, 0.0023480891227496492, 0.0023483512627143401, 0.0023477897099741726, 0.002348444044395658, 0.0023505865401821121, 0.0023510607541099848, 0.0023506452563187182, 0.0023502083732540786, 0.0023503760613581642, 0.0023518235054785217, 0.0023538215417250848, 0.0023535213712380172, 0.0023490393813094899, 0.0023481983347945884, 0.0023494334908496983, 0.0023489552245441205, 0.0023480272049646294, 0.0023485205540856105, 0.0023510538934346623, 0.002352630224212178, 0.0023516520373261809, 0.0023517614046959468, 0.0023517581558681832, 0.0023518850799116573, 0.0023508880143479328, 0.0023507171262573119, 0.0023499572414962949, 0.0023492885044764122, 0.002353050072110801, 0.0023534061181302705, 0.0023539137315736146, 0.0023554440203610635, 0.0023529480993228417, 0.0023521754297274916, 0.0023527065035956101, 0.0023529269492615842, 0.0023520623893746812, 0.0023518755306545349, 0.0023509489144776445, 0.0023496963379635249, 0.0023503211814714289, 0.0023512183969167763, 0.0023514904221732832, 0.0023541087227020884, 0.0023542921272981551, 0.002354154014709179, 0.0023543375591320473, 0.0023557648978413466, 0.002354930043780382, 0.0023561449379099785, 0.002357232756501734, 0.0023601701750094969, 0.0023612293018173704, 0.002361079435930911, 0.0023606277161185127, 0.0023611548928889436, 0.0023617096739818387, 0.0023603600965427947, 0.0023607977189465138, 0.0023603639303066797, 0.0023607932638400673, 0.0023606040809452012, 0.0023606358652474512, 0.0023610349308427763, 0.0023613197549806819, 0.0023617981641045624, 0.0023615562372814539, 0.0023615677461911117, 0.0023603748917820351, 0.0023609426914543942, 0.0023607700082323487, 0.0023617619674370376, 0.0023615331498155539, 0.0023625886141231535, 0.0023610417509333679, 0.0023602515419077503, 0.0023607022136051181, 0.0023598502259202041, 0.0023600455861043502, 0.0023613952839564951, 0.0023609850556555079, 0.0023605146870199173, 0.0023613225501720719, 0.0023621346885565916, 0.002360965906667456, 0.0023603660154705437, 0.0023602607021021493, 0.0023597389502351546, 0.0023590427370044883, 0.0023596940563842457, 0.0023599522812068931, 0.002359532587155105, 0.0023605044551435442, 0.0023615387156576597, 0.0023599428301212155, 0.0023590381357939207, 0.002359147465744978, 0.002358913486659587, 0.002357966650051294, 0.0023575097351368041, 0.0023587073685194301, 0.0023582290101768263, 0.0023577046534555258, 0.0023583191292728669, 0.0023589187478554718, 0.0023594725435871254, 0.0023609666186065279, 0.0023604387789300397, 0.0023597470057416261, 0.0023599141676722748, 0.0023609809374975544, 0.0023591840145173871, 0.0023592863365776711, 0.0023575599410427701, 0.0023559813213362112, 0.0023562536792901853, 0.0023549274240577934, 0.002356352424254702, 0.0023562579795450662, 0.0023562051055897666, 0.0023564196590136898, 0.0023580040535555666, 0.0023556250585887897, 0.0023567360353233761, 0.0023559346820730678, 0.0023554420495083923, 0.0023578802606583353, 0.0023555774491153523, 0.0023553689539853339, 0.0023558088775720697, 0.0023556953790215673, 0.0023563143316635672, 0.0023569280295521761, 0.0023562549568254634, 0.0023563266984954244, 0.0023558736056680457, 0.0023564146399467842, 0.0023575383114007456, 0.0023557138508640304, 0.0023563639976478851, 0.0023577840969645943, 0.0023583446325777815, 0.0023598431052059693, 0.0023598412727448255, 0.0023589247703485368, 0.0023606850051311767, 0.0023606939652523204, 0.0023613760568725879, 0.0023606635703487047, 0.0023605391115112794, 0.0023599211747745963, 0.0023613788917355655, 0.0023623168287012016, 0.0023617582094477454, 0.002360294139096318, 0.0023613965609846715, 0.002361800233235446, 0.0023629807231598443, 0.0023615157747746214, 0.0023609922505517149, 0.0023599169426231129, 0.00235972924173694, 0.0023608243077620546, 0.0023623393774785846, 0.0023612091833828219, 0.0023625940370252218], &#39;auc-mean&#39;: [0.73251291157025022, 0.74757272001190789, 0.74884287344857781, 0.76326493797028028, 0.76300973229704072, 0.76679565493906698, 0.76702335926946463, 0.76749367737349838, 0.76717966353985123, 0.76886720476783976, 0.7684174466727709, 0.76923418805644272, 0.77138764121538084, 0.77030134557736329, 0.77012226623636559, 0.77051748875426074, 0.77001746314881458, 0.7705283572402587, 0.77108534229875181, 0.77137797288411958, 0.77201496580346307, 0.77191569749144584, 0.7720983592322721, 0.77187942496050466, 0.77244850696632283, 0.77412147653036567, 0.77455307015493058, 0.77513296822709532, 0.77615247168875601, 0.77621901978894114, 0.77667265437334587, 0.77702039952102286, 0.7769705226787329, 0.77682631360670928, 0.77646847565425103, 0.77622825795547301, 0.77609545380884593, 0.77579194987896349, 0.77629617422259711, 0.77642120856903807, 0.77687106395179995, 0.77694396407632715, 0.77681988116127143, 0.77679738923275699, 0.7767034559206728, 0.77668748612177152, 0.77664526838190295, 0.77658291914464095, 0.77649482703004513, 0.776658744569929, 0.77736589648579968, 0.77719401280822553, 0.77716619912465112, 0.77719546041060872, 0.77808368697512753, 0.77789131518030008, 0.7779135763154239, 0.77805885252544604, 0.77804427177483715, 0.77799085987396777, 0.77789327327860824, 0.77806069092831676, 0.77800301524672266, 0.77835422810125765, 0.77840813627640737, 0.77839312367673963, 0.77862923686176777, 0.7786520373740401, 0.77887940862245886, 0.77913310277859593, 0.77940349721210533, 0.77933553657349308, 0.77964301404818226, 0.77957190972929202, 0.77948831101075655, 0.77953844175749887, 0.77973202335532177, 0.77980608209519187, 0.77976654539288848, 0.77973212371626055, 0.77995033302822558, 0.78016830060782405, 0.78026852490907872, 0.78036457911100499, 0.78044715184314206, 0.7805108536171772, 0.7806179809620224, 0.78086741076742416, 0.7808666060312992, 0.78095152045916449, 0.7809933422220986, 0.78099976552790307, 0.78106761675633707, 0.78110041156717502, 0.78135963196450331, 0.7813213665800397, 0.78144958548224031, 0.7814767133830911, 0.78168982863454195, 0.78173777200237349, 0.78171776335501864, 0.78179624071443343, 0.781859860514583, 0.78202245942974424, 0.78217710623081205, 0.78221698452980348, 0.78235169963779061, 0.78245722057943856, 0.78251589429807444, 0.78261334686040951, 0.78257674706525016, 0.7827938905291838, 0.78286854493007441, 0.78303304541238994, 0.78303590433939796, 0.78333342087588054, 0.78337923643529039, 0.78348902703988177, 0.78351189212424477, 0.78360898288298908, 0.78364690120764591, 0.78371934218224582, 0.78376301944664617, 0.78393581990849481, 0.78397826567313489, 0.7843885291615218, 0.78459444551914648, 0.7846674781882923, 0.78468522157295639, 0.78470061191316975, 0.78486432986629406, 0.78488332176819653, 0.78521663865039293, 0.78526312019908118, 0.78534412700620837, 0.78536611498805442, 0.78541619759534231, 0.78560942782889487, 0.78577009311125701, 0.78586872193741386, 0.78591872457034273, 0.78595929560243438, 0.78594529636844335, 0.78597328673612743, 0.78594857029516008, 0.78599266044890137, 0.78610237813975281, 0.78624165597703455, 0.78639939518436486, 0.78652814664631565, 0.78657500619253917, 0.78668651106180831, 0.78679294166146518, 0.78686810929189732, 0.78686325909416432, 0.78689099179484312, 0.78693088899861474, 0.78697276102695235, 0.78707472377255416, 0.78719188901127457, 0.78723300972256327, 0.78721008810237225, 0.78737958432669441, 0.78744507812174935, 0.78747653280305419, 0.78750845142616044, 0.78763324443608496, 0.78785590328847088, 0.78789115581545688, 0.78788621376272816, 0.78797163158434591, 0.78800997655112492, 0.78816235196895934, 0.78822624975971678, 0.78835723748941244, 0.78848180575865523, 0.78859244594713851, 0.78863442748788981, 0.78871174571119163, 0.78869930208231542, 0.78884140021264337, 0.7888978648495113, 0.78903811689349868, 0.78906659038534233, 0.78912494672567701, 0.78914960621632213, 0.78926720197937972, 0.78935125122586414, 0.78937660862240722, 0.78951631881653017, 0.78954775439923375, 0.78965786483965827, 0.78973452444451531, 0.78974357887055213, 0.78982598629302125, 0.78993103810874532, 0.79002230186002398, 0.79010670255593818, 0.79018972644311825, 0.79018019311775123, 0.79018161862713698, 0.79031527320824979, 0.79039530605961572, 0.79040956492052894, 0.79044137347235721, 0.79043210862991287, 0.79053732257046072, 0.79063174452178664, 0.79068571456852776, 0.79073157507101466, 0.79082183118224791, 0.79100276225969657, 0.79110297992821565, 0.79118284137905504, 0.79125325886824016, 0.79126745817360455, 0.79137182738497658, 0.79142208760281696, 0.79151218493234299, 0.79150010686830718, 0.79158127969683179, 0.79167724372528903, 0.7917349623053056, 0.79189344340976608, 0.79194369626231764, 0.79193914834227497, 0.79199305499887163, 0.79209235439335568, 0.7922065130243009, 0.79234954553217785, 0.7925195546960847, 0.7926047079175027, 0.79267364466834078, 0.79274803164632868, 0.79290267422236405, 0.79301051719687199, 0.79305010244930418, 0.79306786712545208, 0.79310080136355388, 0.79310348219573834, 0.79324329370900926, 0.79323329389767849, 0.79339658966368609, 0.79345564229072407, 0.7935237764741172, 0.79358646019330303, 0.79371489823382135, 0.79379362339047566, 0.79380281373224915, 0.79400252874245747, 0.79402726993858286, 0.79410828024790181, 0.79417900092806692, 0.79429305435344566, 0.79436506812135055, 0.79449883380148589, 0.79468473942806439, 0.79485334676506525, 0.79492140153570967, 0.79501036160576355, 0.79509317659614431, 0.79515889924245697, 0.79522923080797714, 0.79533192319640433, 0.79553175393916553, 0.79558335316872708, 0.79558684850673544, 0.79562913152552073, 0.79574019971331755, 0.79590786406681202, 0.79604359965905713, 0.79610885242905138, 0.79618305573762105, 0.79627264000660791, 0.79634359582078706, 0.79635621556293634, 0.79645682149015307, 0.79650558876577748, 0.79658748847746375, 0.79663548030996201, 0.79663772717118198, 0.79672914987408772, 0.79678781287309031, 0.79686039589848379, 0.79701091136566293, 0.79716750354444998, 0.79722219026016494, 0.79728549557532558, 0.79735470316244406, 0.79736600545629899, 0.79746308941885846, 0.79746319949893707, 0.79752118698049279, 0.79754266925374773, 0.79769412521714389, 0.79772000022488287, 0.79780975798763698, 0.79787057350551605, 0.79791395350151606, 0.79802596500341072, 0.79806232013202683, 0.79810312521989824, 0.79814279295714252, 0.79820200713084088, 0.79824918118662447, 0.79832793922348222, 0.79838633073923515, 0.79850516540223426, 0.79854512886821694, 0.79860258064813872, 0.79865074110858925, 0.79872286495971967, 0.798760932815614, 0.79881883706031953, 0.79889900429237337, 0.79896770126135586, 0.79900583147808857, 0.79911256069506487, 0.79919805851388726, 0.79928111770694177, 0.79931708962251713, 0.79937365871893151, 0.79938527280706784, 0.79947089799447379, 0.79952586506132828, 0.79954869022026975, 0.79958375282899175, 0.79964832718144707, 0.79975916439624206, 0.79983819247382726, 0.79997335269676673, 0.80002210732463053, 0.80016743112533428, 0.80023108928660458, 0.80024014978987545, 0.80029020771773707, 0.80034138378529929, 0.80040419425675025, 0.80046767706804101, 0.80054442389078351, 0.8005885516500838, 0.80062183668966536, 0.80066915882202283, 0.80070672044090219, 0.80079187682525566, 0.8008739912534496, 0.80088426067081797, 0.80098320921779553, 0.80100498659531694, 0.80109692813317301, 0.801142779525569, 0.80118181320347903, 0.80125255946496377, 0.80132537500125967, 0.80142756526909409, 0.80148143500867963, 0.80154450483859863, 0.80162068707993672, 0.80163203107717818, 0.80164446527956323, 0.80165163387057192, 0.80172694472338013, 0.80177478272451741, 0.80180829609456583, 0.80189753753124915, 0.80201147035960152, 0.80203821149037746, 0.80208319374378245, 0.80212386217462162, 0.80218532365107165, 0.8022164329131003, 0.80226572053385126, 0.80236565158647244, 0.8024227003056893, 0.80248494078540789, 0.80253581939642038, 0.80258798317592839, 0.80265270222544216, 0.80269118312266419, 0.80275961049121458, 0.80280183201320199, 0.80290532414897464, 0.8029612338277754, 0.80303238193546833, 0.80306830900295267, 0.80313613394462302, 0.80320153793123694, 0.80324743808745414, 0.80329394124440034, 0.80335679958619133, 0.80345456142057325, 0.80352289903366858, 0.80359461828771406, 0.80363602495735864, 0.80366442698423646, 0.80372336288849355, 0.80376474371507955, 0.80380378868967384, 0.80381570584417061, 0.80390774122522957, 0.80401509751192246, 0.80405254766426792, 0.80414891683554168, 0.80420899891134656, 0.80423985348046845, 0.8043007714960787, 0.80436044707150711, 0.80439497487522582, 0.80446922729421944, 0.80452286901330639, 0.80456178559262082, 0.80466236520846446, 0.80477948351250483, 0.80480193117313736, 0.80484536417759733, 0.80488038003561557, 0.80492423906423072, 0.80494441488484492, 0.80499208161318625, 0.80505316435228824, 0.80516570844487201, 0.80522106831542151, 0.80527834218140393, 0.80532833653667146, 0.80536533627615048, 0.80539107871029114, 0.80541932176695374, 0.80547004026825153, 0.80547827580799836, 0.80552128031904502, 0.80557623990987148, 0.80564182507136994, 0.80568373083937284, 0.80571367045148445, 0.80578893359687709, 0.80583244084083105, 0.80586920559413855, 0.80590454645977183, 0.80596097279853662, 0.80603198997378001, 0.80606473664447775, 0.80610713084942631, 0.80615141159208503, 0.80623805938826654, 0.80627417589545092, 0.80631633301456984, 0.8063731740186052, 0.80639710838706447, 0.80643951123690094, 0.8064828258328518, 0.80649673054210724, 0.80652974068412875, 0.8065795828028719, 0.80663007932588626, 0.80669633640239424, 0.80675610018040589, 0.80678725068545631, 0.80682754161556791, 0.80685231000540336, 0.8068775937341286, 0.80692429447976222, 0.80696031827311043, 0.80698997694719421, 0.80706151959987016, 0.80709761448555972, 0.8070991867608418, 0.80711069962753224, 0.80713144517950242, 0.8071904299461975, 0.80722602742846605, 0.80732247745995789, 0.80734948571260501, 0.80740180531634975, 0.80747841975329693, 0.80751023728499993, 0.80755024752044446, 0.80758704032016182, 0.80765805599203078, 0.80767046874812876, 0.80767891275172199, 0.80770547003309845, 0.8077498227682367, 0.8077807838254929, 0.80782495439591229, 0.80783227824336856, 0.80789166431197845, 0.80793533106954774, 0.8079587446374793, 0.80802328037102222, 0.80808369926514434, 0.80815255766460736, 0.80818035035326974, 0.80824411372796057, 0.80827132129369894, 0.80832777677810108, 0.80835088692126755, 0.80835903092624639, 0.80840331242721475, 0.8084379907525161, 0.80846643996330148, 0.80850974489743521, 0.8085471804526575, 0.80859660529965383, 0.8086252309324431, 0.80865984599984519, 0.80867455593309034, 0.80871210869802912, 0.80872426702830835, 0.80879500429263051, 0.80883040954960728, 0.80891605303860215, 0.80896676611324436, 0.80899534417458896, 0.80904215235571431, 0.80908139726899386, 0.80910561280314641, 0.80912053181754862, 0.8091711098541392, 0.80919840927808906, 0.80923329945415112, 0.80927105075897787, 0.80928647558785782, 0.80931755648444048, 0.80935123408873311, 0.80935670910817981, 0.80938691468801172, 0.80943418825483204, 0.80946708763936981, 0.80952163191045123, 0.80959701501491366, 0.80962614871235261, 0.80968997330522985, 0.80974175468927112, 0.80982516939809523, 0.80985341530385813, 0.80987790735095211, 0.80991087254294969, 0.80998751235928634, 0.81001409316578454, 0.81005165212166941, 0.81008624314697164, 0.81010719661285191, 0.81012961769039915, 0.81015598461714222, 0.81017172041804186, 0.81020471098842983, 0.81022080564490984, 0.81023100400336445, 0.81026330265485991, 0.81027643616279121, 0.81028361414049443, 0.81030855051040585, 0.8103350745530008, 0.81036498193704865, 0.81038393920269702, 0.81040831460882357, 0.81043099158937326, 0.81044526267466177, 0.81048914472487366, 0.81059690204089452, 0.81062794859709475, 0.81064528391015889, 0.81066913797264528, 0.81069577231627166, 0.81072667606888316, 0.81075909991872286, 0.81078133186777568, 0.81082318345878002, 0.81086415909738374, 0.81088850334681228, 0.81095624840728231, 0.81106603221164197, 0.81108343282335782, 0.81111397420992792, 0.81114550622631754, 0.81118023157185204, 0.81120122149731499, 0.81122576287877224, 0.81126570593447522, 0.81130209128990471, 0.81134993744117201, 0.8113745655973279, 0.81141399822415072, 0.81143033253501573, 0.81145228097845057, 0.81146777404892556, 0.81147569008582643, 0.81152847839743725, 0.81157241249539014, 0.81160443362871493, 0.8116134902210721, 0.8116393502900332, 0.81167802881843121, 0.81168345037557954, 0.81170686955800819, 0.81176578202350247, 0.81177425271608339, 0.81180586999663296, 0.81180860103891594, 0.81183125077025386, 0.81184587590057156, 0.81187268489027176, 0.81191429952775296, 0.81194851765080889, 0.81196157315426354, 0.81198468787835354, 0.8119930724658706, 0.81200762438485774, 0.81205955236393623, 0.81208105806359154, 0.81214849008790702, 0.81220354367437864, 0.81223528484515595, 0.81226747751242312, 0.81228399670767037, 0.81233965388283202, 0.81235044243105814, 0.81236529865495011, 0.81242155591363741, 0.81248517090248418, 0.81252764786058551, 0.81255267127414788, 0.81257206580123886, 0.81263193568418202, 0.81264405467672629, 0.8126978258538633, 0.81271570542904126, 0.81278236160198958, 0.81279808078473281, 0.81281479173054672, 0.81283229051485306, 0.81285073294760879, 0.81287403580853046, 0.81293616890204667, 0.81295857584010345, 0.81297941264315843, 0.81306286724731591, 0.81307076850197291, 0.8131156807439327, 0.81313605953379153, 0.81315277960678833, 0.813193262864641, 0.81321885404389926, 0.81326950391940611, 0.81328842680929814, 0.81330278797852951, 0.81331822361008188, 0.81333998788210082, 0.81335681546765604, 0.81337265959559846, 0.81338623019195766, 0.813394386357424, 0.81340610269828029, 0.81346797631346135, 0.81347749514921064, 0.81349630168935627, 0.8135178303497993, 0.81354902827175835, 0.81359193644127537, 0.81360413332890302, 0.81362690226994872, 0.81365590576865632, 0.81367033235062025, 0.81368938402344071, 0.81373799180073103, 0.81376241807554206, 0.81381124725061105, 0.81383197495732085, 0.81386026299332248, 0.81388540509946627, 0.8139327844629165, 0.81395115698038867, 0.8139623025274938, 0.81397766391343163, 0.81401590984708838, 0.81402416466826943, 0.81405930527204406, 0.81407320906816039, 0.81413569868637281, 0.81415739272127519, 0.81417311288928251, 0.81418839735403326, 0.81420635732016033, 0.81421667882406989, 0.81428463211274527, 0.81431076979124239, 0.81432883823990621, 0.81435808548581912, 0.81438192862403014, 0.81439959742994128, 0.8144033793270582, 0.81441168015331689, 0.81442076162522592, 0.81444150287408645, 0.81444430618711683, 0.814450072136783, 0.81446955966440648, 0.81448468126265094, 0.81453415671931084, 0.81455395224621152, 0.81458572222414782, 0.8146071130719752, 0.81461451589698419, 0.814635213974096, 0.81464456463243629, 0.81465293399184002, 0.81466721812468967, 0.81470666439222994, 0.81478843515032229, 0.81480234807964291, 0.81480863478607457, 0.81482560435201656, 0.81482955808541591, 0.8148371794026108, 0.81483456477154803, 0.81485084658129026, 0.81486179744587184, 0.814873885620095, 0.81488888973573026, 0.81489949465176781, 0.81493828414410507, 0.81496153528502879, 0.81500727843785847, 0.81500427989363367, 0.81501526677302694, 0.81504362792805285, 0.81507101274958349, 0.81508047190789445, 0.81509005194594875, 0.81511989978496202, 0.81513145147681221, 0.81517170569519748, 0.81517945429584115, 0.81520679899440318, 0.81522366079152453, 0.81523483451328482, 0.81524012693448977, 0.81526410838246688, 0.81527093783512061, 0.81530022364542298, 0.81533861048510659, 0.81535793999683936, 0.81538632627690999, 0.81540161877241935, 0.81541014102112508, 0.81542677492342275, 0.81543402735122616, 0.81545011431928871, 0.81546370025514658, 0.81551878748189977, 0.81559153580632238, 0.81560828368741145, 0.81562062387146794, 0.81562736904511568, 0.81567688357405999, 0.81568682271976767, 0.81570057998368384, 0.81572314953149028, 0.81572661394027701, 0.81574329801109136, 0.81576116818564892, 0.81576923164861959, 0.81578091601847758, 0.81579458415365003, 0.81579990879820219, 0.81581256825036363, 0.81584329556172419, 0.81585400302693911, 0.81586331152868508, 0.81587926741208483, 0.81588037561365545, 0.81589831212650543, 0.81590639653541996, 0.81591206741004696, 0.81592209534278537, 0.81594228572006633, 0.81595053770931225, 0.81595484855631495, 0.81597716449158408, 0.8159885422381089, 0.8159980614506257, 0.81601464828561632, 0.81602538245507961, 0.81604251495954272, 0.81605123316988148, 0.81606083691030107, 0.81607021219986076, 0.81607949722640227, 0.8160991900989425, 0.81614783179832029, 0.81615610184967946, 0.81616349248872644, 0.81618206601931287, 0.81620269351894914, 0.81621624782030389, 0.81621835487980365, 0.81624348056370355, 0.81628466268852229, 0.8162998287347607, 0.81631406686705144, 0.81633301445987883, 0.81638464199430039, 0.81639905224654785, 0.81643015499391436, 0.81645799664022578, 0.81648195223823161, 0.81651049386145158, 0.8165176196304742, 0.8165738442918592, 0.81659158403577725, 0.81660342244553275, 0.81660731581759261, 0.81663111763459173, 0.8166387989159436, 0.81665836471833253, 0.81669353800108979, 0.81669705978207019, 0.81671222037018898, 0.81671878786849528, 0.81676617588347056, 0.81678142153711875, 0.8168002619374457, 0.81683396423368726, 0.81690985075944622, 0.8169472078459451, 0.81695473966485976, 0.81696096202932966, 0.81696418531984238, 0.81696898764711823, 0.81698523583568172, 0.81699627334288638, 0.81701269935980536, 0.81702103317589003, 0.81704280971665533, 0.81706056560209961, 0.81708642926477792, 0.81710113468916801, 0.81711471675114922, 0.8171511449761828, 0.81715897080879341, 0.81716834914728653, 0.81717409154673371, 0.81717751936418392, 0.81718086992798022, 0.8171871819134221, 0.81718816673343753, 0.8171960137991956, 0.8172038973830773, 0.81726204944790415, 0.81727636216006394, 0.81730934854356507, 0.81732014931363661, 0.81733684416939578, 0.81737726140033506, 0.81738722026170885, 0.81740741970821307, 0.81742228686866303, 0.81743446900033168, 0.81748648451815031, 0.81750280338433168, 0.8175319007667452, 0.81755760184475734, 0.81756113560041666, 0.81756742827169548, 0.81757615492703262, 0.81763243985921741, 0.81764120556461817, 0.8176588712727515, 0.81768412011802138, 0.81769599418011651, 0.81770951697801364, 0.81774445191154665, 0.8177682120416504, 0.81778535614741477, 0.81780041496357436, 0.81780888925671769, 0.81781390982304314, 0.81784676990252658, 0.8178551931713407, 0.81786304933529563, 0.81787590979458891, 0.81788065905320817, 0.81788754071748726, 0.81789667923076004, 0.8179498543404099, 0.81799130669007325, 0.8179977428030405, 0.81800773877225375, 0.81801390558043574, 0.81809024633876515, 0.81810232575573549, 0.81811014832793061, 0.81811240261701923, 0.81812168704298094, 0.818195293453997, 0.81821049826018177, 0.81824185374001446, 0.81824419807311111, 0.81825195208461798, 0.81825290027119324, 0.81826342928377027, 0.8182844939755054, 0.8183010213248656, 0.81830764594393968, 0.81833854475010381, 0.81836718664450991, 0.81844223277721295, 0.81849033030596874, 0.81850352023011452, 0.81850872007237019, 0.81855671389030715, 0.81856756973753053, 0.81857244211188385, 0.81857679214308177, 0.81858879548543739, 0.81863808360441337, 0.8186559842762472, 0.81865979630808872, 0.81865879692642252, 0.8186893048519398, 0.81870404393907759, 0.81872058232653599, 0.81872242780166093, 0.81872348476071566, 0.81874381402923946, 0.81875240073250599, 0.81876809702694031, 0.81877540615112088, 0.81877817026980593, 0.81880494135839155, 0.81886018715978837, 0.81887460806169299, 0.8188795379239302, 0.81888994600442166, 0.81889954255792718, 0.81892601662172326, 0.81892180518734359, 0.81893244651066655, 0.81898741826250954, 0.81898417908224874, 0.81899119456314973, 0.81899787417631809, 0.81900645256339644, 0.81901326954803155, 0.81900695384685862, 0.81903865042941426, 0.8190462280757782, 0.81907471844348034, 0.81908347408138149, 0.8191063761420514, 0.81914866639032691, 0.81915710461831925, 0.81916760522548004, 0.81917692963155386, 0.81918253999136825, 0.81919753294655551, 0.81921604802137638, 0.81922612024864938, 0.81924096710528382, 0.81925455925986768, 0.81927200288180391, 0.81927784726326913, 0.81928606342535226, 0.81928980739851642, 0.81930107913780414, 0.81933343025922079, 0.81933844306827619, 0.81935909331249057, 0.81939248974433665, 0.81940517280699954, 0.81941355229199164, 0.81946267961402464, 0.81950649416951715, 0.8195032401787673, 0.81951488189819199, 0.81951552103307301, 0.81951357597173491, 0.81951904863022418, 0.8195250759826328, 0.819532922939327, 0.81954098570172551, 0.81954886900454815, 0.81955428659529184, 0.81955717163085562, 0.81955766976297384, 0.81956699158175039, 0.81957346884930971, 0.8195817806612482, 0.81959350320711466, 0.8196078411447566, 0.8196191944060216, 0.81962533701350027, 0.81964879619486486, 0.81967295785644456, 0.81969758635957002, 0.81971842524947824, 0.81972912099410866, 0.81975404463926371, 0.81975795829603015, 0.81976674170307329, 0.81976763944055064, 0.81977105590060029, 0.8197796688581972, 0.81980672969110047, 0.81981436871551272, 0.81981493576396802, 0.81982523262654328, 0.81985921816604912, 0.81986444669695824, 0.8198756442643772, 0.81991387014404027, 0.81991626579175247, 0.81992406186821987, 0.81994184921952673, 0.81995109725726978, 0.81997720785616046, 0.81998249677901536, 0.81998892392851275, 0.81999260752179182, 0.81999507201674482, 0.81999509612640531, 0.81999655461196852, 0.82002415901601577, 0.8200462475808965, 0.8200556432658983, 0.82007053168940325, 0.82007096710054572, 0.82007736946576237, 0.82008160144211251, 0.82008455640723721, 0.82009104846093095, 0.82009387620194185, 0.82010629462071094, 0.82012080144972388, 0.82014181204695868, 0.82015999434008402, 0.82016941070165961, 0.82017498786067, 0.82018036727406507, 0.82018469824741103, 0.82018485777968608, 0.82019140087377251, 0.82020486840320339, 0.82020947719037129, 0.82022601778246229, 0.82022621884169067, 0.82024512696739826, 0.8202529616626435, 0.82026152611715164, 0.82026081440558474, 0.82026783278905913, 0.82029630629072992, 0.82030099514442101, 0.82031152486633729, 0.82032587901271781, 0.82032592414246575, 0.82034153396234422, 0.82034515182211565, 0.82036486209865911, 0.82036810446197139, 0.82037172110396761, 0.82038767292653669, 0.8203909682913022, 0.82039252403784102, 0.82041077325936507, 0.82041953577231541, 0.8204257828354502, 0.82043502190437001, 0.82044275494384977, 0.82044600331693773, 0.82045270664237824, 0.82047727280100025, 0.82048236084826676, 0.82055509247689939, 0.82056059175988172, 0.82056538530530843, 0.82057872539924936, 0.82060325638098863, 0.82065201923992659, 0.82065712591362716, 0.82065933248797884, 0.82067679421692075, 0.82069980835542344, 0.82070470049887168, 0.82071367050405608, 0.82072266033220242, 0.82073564690920175, 0.82078281302167144, 0.82078908075589818, 0.82078704759595611, 0.82079740682821678, 0.82082239363448828, 0.82082597445094874, 0.82083444778764691, 0.82083514382861333, 0.82085451771394402, 0.82086043369535733, 0.82087042748063033, 0.82087324929867367, 0.82087147757446088, 0.82087546998432104, 0.82087795883920711, 0.82087868850575541, 0.82088452971081638, 0.82089008804935604, 0.82090504631728733, 0.82091258756261332, 0.82093804073524534, 0.82095894270510306, 0.82099281643250299, 0.82099700068390891, 0.82100218524802637, 0.82101078761501634, 0.82102938762741362, 0.82103719851733137, 0.82106073554827042, 0.82106534106171747, 0.82106825609502165, 0.82107944407399724, 0.82108611023331668, 0.82109764145531661, 0.8211331922730235, 0.82115160547580801, 0.82120268895063364, 0.82123891820708261, 0.82124171048049688, 0.82123919960163594, 0.82126622087906809, 0.82127328449514203, 0.82128499077302419, 0.82130000150181703, 0.82130347825785999, 0.82133352371142299, 0.8213368015953636, 0.82134555201398318, 0.82134753942640815, 0.82134724819513261, 0.82136045035057048, 0.82140906261931035, 0.82142091833664388, 0.82142061800595056, 0.82142836603561553, 0.82143068320216095, 0.82144548953613161, 0.82145884289687598, 0.82145958476275927, 0.82148571050169661, 0.82149493907387128, 0.82150031230614606, 0.82151688531164679, 0.8215233892653433, 0.82153722968102838, 0.82156249238413337, 0.82157251044633006, 0.82157429900599743, 0.82157588115555846, 0.82159636463055818, 0.82159400224413193, 0.82160513249549305, 0.82161840922971141, 0.82162297132395212, 0.82162247292352908, 0.82161609098850941, 0.82162060920074986, 0.82163156310410979, 0.82164081535349953, 0.82164475031348749, 0.8216592494236783, 0.82167446577853376, 0.82168513701575863, 0.82169091203313072, 0.82170164295639159, 0.82171060991348421, 0.82172978599473834, 0.8217373346838901, 0.82174636110451049, 0.82174780809033021, 0.82174731052636696, 0.82175647540681696, 0.8217685541667763, 0.82177211169574882, 0.82179309559540692, 0.82179784147664103, 0.82180828533805828, 0.8218172575920365, 0.82182403086027878, 0.82182967825077802, 0.82183558881634422, 0.82184376898208433, 0.82185012386032752, 0.82185363391591815, 0.82186844793077274, 0.82187727000662891, 0.8218801161999153, 0.82189804650069331, 0.82189853800686907, 0.82189948994680828, 0.82190378843496725, 0.82193042303465091, 0.82194605327747505, 0.82194720064840787, 0.82194883027174437, 0.82195504680928233, 0.82195900948530909, 0.82196497446477745, 0.82197563485295766, 0.82198512343775965, 0.82198462210204704, 0.82199775791281726, 0.82199838504162082, 0.82200463748098307, 0.82201117578984417, 0.82200974281245254, 0.82203286949224785, 0.82203296895550437, 0.82205242378262167, 0.82205825023769852, 0.82206231752516512, 0.82206335322092949, 0.82206912548633038, 0.82207198325488373, 0.82207317499563359, 0.82207877635827464, 0.82208518229514616, 0.82209480038050875, 0.82209880265416013, 0.82209766255382011, 0.82210127677075806, 0.82211249725580515, 0.82212215091932495, 0.82212933112068998, 0.82213094906879536, 0.8221682080768753, 0.8221696452319065, 0.8221738147557115, 0.82217560352163177, 0.8221779632125592, 0.8221877641255666, 0.82219228457361138, 0.82219287574486599, 0.82222103043673778, 0.82222858928216758, 0.82223345553543337, 0.82223904296831773, 0.82224317942337666, 0.8222405988285546, 0.82224870066256384, 0.82225091832825525, 0.82225151846528988, 0.82226991057167553, 0.82227679644297891, 0.82228080040202656, 0.8223440919602355, 0.82234735175715579, 0.82235290810132811, 0.82240229794401165, 0.82240854218366954, 0.82240757262830244, 0.82240827825532326, 0.82241455241766004, 0.82243708949734595, 0.82244282232248234, 0.82245611753858316, 0.82246370042685157, 0.82246287444650901, 0.82247484697668161, 0.82248013972934597, 0.82248542491079013, 0.82249187610264707, 0.82255700179039359, 0.82256245649429638, 0.82256889539711042, 0.82257350598800638, 0.82257486933635549, 0.82257847149558239, 0.82257679336448142, 0.82257980461199909, 0.82259310588506429, 0.82259063298727852, 0.82259918159626577, 0.82260725526804723, 0.82260396876012065, 0.82260734884078057, 0.8226546757957891, 0.82265369453720116, 0.82265224430079731, 0.82265248161862137, 0.82265923925807161, 0.82266715490062337, 0.8226962799886921, 0.82269971722112467, 0.82272231237903881, 0.8227277962424262, 0.82274921732657114, 0.82275842994472703, 0.82276165020092251, 0.8227672783167479, 0.8227693494263365, 0.82277673941339224, 0.82277595029927075, 0.82282087570222695, 0.82282310196208219, 0.82282670955196302, 0.82284230958289173, 0.82283889633949392, 0.82287314219069696, 0.82287960820662198, 0.82287935577687055, 0.82288459108989576, 0.82289402895835395, 0.82292862370927344, 0.8229334958868687, 0.82293485265038835, 0.82297212056153624, 0.82298043331317561, 0.82301898807250817, 0.82303574145274128, 0.82303412631088002, 0.82303788494923324, 0.82305232698538067, 0.82305656739885968, 0.8230689534991672, 0.82308758260524595, 0.82309371885995419, 0.82309311848151123, 0.82310285632812863, 0.82311813816036339, 0.82313621511225354, 0.82318436735974176, 0.82319304179746955, 0.82319156183472919, 0.82319682192676491, 0.82321431019619451, 0.82321473312506299, 0.82321304355902014, 0.82321498260450066, 0.82322436353626449, 0.82323548717172168, 0.82323426808147793, 0.82325427630654069, 0.82325337231921591, 0.82329541052061828, 0.82329796464263028, 0.82330266574984545, 0.82331078483631948, 0.82331279580759398, 0.8233318321410279, 0.82333117094775721, 0.82334072701712524, 0.82335867759115655, 0.82340007908690682, 0.82341140803243607, 0.82341509713355465, 0.82341257915036825, 0.8234149825375916, 0.82342156258820132, 0.82342201334292875, 0.82342315459686444, 0.82342204000801666, 0.82344130377554769, 0.82345765342835686, 0.82345424041706594, 0.8234808668482444, 0.82350054348520307, 0.82350223693687175, 0.82356535644674145, 0.82357822317605256, 0.82358121587978594, 0.82360068999338032, 0.82360873061629614, 0.82361299956425249, 0.82365407307738026, 0.82365317852247633, 0.82365886665537347, 0.82367277394370153, 0.82367821994541079, 0.82368023493735287, 0.82367916847450595, 0.82368219383860131, 0.82370270570718129, 0.82370242098411595, 0.82372400966814807, 0.82377529725429466, 0.8237812798991202, 0.82377944046649831, 0.82377816167590479, 0.82379344429125045, 0.82379607397286936, 0.82379703480918187, 0.8238341666939929, 0.82383593848709824, 0.82384080173553542, 0.82387400315073678, 0.82387353265257413, 0.82391169477901882, 0.82391259808996398, 0.82393439122531087, 0.82394455809165079, 0.82394841155618614, 0.82395339481150009, 0.82397143333146727, 0.82398444692628914, 0.82398037380338618, 0.82397922328773865, 0.82398291859669404, 0.82400190258498307, 0.8240111118520087, 0.8240324859746273, 0.82403424729351415, 0.82405339588885551, 0.82405882635069561, 0.82407982436247451, 0.82410489809051213, 0.82410391103703362, 0.82410569128067446, 0.82410510834044248, 0.82410463404363199, 0.82410543883285325, 0.8241077982251277, 0.82411019325817514, 0.82410898042279557, 0.82413128205851804, 0.82413673075800153, 0.82415193205328341, 0.82415065343504157, 0.824167970773377, 0.82416936474445601, 0.82417156440428307, 0.82417254327731482, 0.82418613019386344, 0.82418664394785779, 0.82419425069815655, 0.82419346154658601, 0.82422550640440018, 0.82427277260382925, 0.82427385571984835, 0.82429126048855283, 0.82430992324239638, 0.82431005196231621, 0.82431810925438642, 0.82432378671082096, 0.82432198520621847, 0.82431803851770657, 0.82432320191677877, 0.82433250524326473, 0.82433675835835274, 0.82435181584464545, 0.82435854048059254, 0.82438367930307677, 0.82438317245889903, 0.82438713511278439, 0.8243890292318804, 0.82442730030268874, 0.82444525814403702, 0.82445371815182844, 0.8244545502350954, 0.82445554128928777, 0.82447291301054249, 0.82447556858261883, 0.82447953148855257, 0.82447912287663616, 0.82447686583042645, 0.82451388807889625, 0.82451966270802879, 0.8245216409108822, 0.82452144618678336, 0.82452629371203279, 0.82452830773270414, 0.82453252578510661, 0.82453225590020429, 0.82453620115971094, 0.82454131038409995, 0.82454064088901491, 0.82453757278460693, 0.82453690875834318, 0.82453711555343967, 0.8245366653564693, 0.8245418216103374, 0.82454414256372144, 0.82453921151640075, 0.82453979085207185, 0.82453830173067522, 0.82454495238251513, 0.82454602636636865, 0.82455020633664566, 0.82456221024036525, 0.82456801343421393, 0.82457164228061131, 0.82457164208994205, 0.82459662442827608, 0.82461787928984798, 0.82461366489564403, 0.82463386417785056, 0.82462935266111204, 0.82462751251557032, 0.82464453916470126, 0.82465020740137407, 0.82466415412980587, 0.82466447843495061, 0.82466322003964976, 0.82467892269575815, 0.82469135013189054, 0.82470340130411335, 0.82470232039541946, 0.82471924988997958, 0.82475466227472705, 0.82475564977653837, 0.82475248848059213, 0.82475672640522668, 0.82475714023465085, 0.82477276225672436, 0.82477012029541752, 0.82477617482224641, 0.82477507891176338, 0.82478326574821037, 0.8247949943729308, 0.82481619597122113, 0.82481254293546813, 0.8248290863456621, 0.82483191423023849, 0.82483210633503634, 0.82483130468000054, 0.82483442065213475, 0.82484485563337517, 0.82486665154015415, 0.82487030464773048, 0.82487155027034353, 0.82487354649183686, 0.82487987199410673, 0.8248827150553566, 0.82490168964687471, 0.82490440629669748, 0.82490619596257519, 0.8249188965214016, 0.82494602640413106, 0.82494374468732734, 0.82495508588713096, 0.82496500119604765, 0.82496718414317649, 0.82497716638041396, 0.8249947075513413, 0.82499104489268604, 0.82500175230254746, 0.82500620978480121, 0.82500782437739384, 0.82502126614384808, 0.82502674968930401, 0.82501969869133762, 0.82503450097451236, 0.82504594691310496, 0.82505013486808054, 0.82504902749950715, 0.82504865455925047, 0.82504947094687642, 0.82505062956055508, 0.82511042604380425, 0.82511664328633461, 0.82512040750475479, 0.82513367585360187, 0.8251330118127107, 0.82513675478118065, 0.82513711204681306, 0.82513446408609925, 0.82515886325196186, 0.82515676811761374, 0.82516680986976565, 0.82516306587113775, 0.82515919931708925, 0.82516003034556051, 0.82516071722685369, 0.82515956726301209, 0.82515806630324628, 0.8251542236216769, 0.82515827284237486, 0.82516969561291909, 0.82516735479943648, 0.82516766077497716, 0.82520623517387592, 0.82521307944552458, 0.82521943134044073, 0.82522759087180708, 0.82522501259662229, 0.82522467064401661, 0.82522485129603207, 0.82523691885925976, 0.82523860289882278, 0.82523727602934183, 0.82523455669304069, 0.82525487072642023, 0.82527311018990779, 0.82527423058884186, 0.82527532288721728, 0.82527358198151135, 0.82527657805928512, 0.82527809684763231, 0.82530159786179758, 0.82530505298035306, 0.82531322669159213, 0.82531297738922194, 0.82531500023735682, 0.82531871111506983, 0.8253328321568969, 0.82533423756707514, 0.82534144667297138, 0.82534398011793186, 0.82535649223336782, 0.82535890644000887, 0.82537299790473517, 0.82541263768081963, 0.82540991482724091, 0.82541365248146492, 0.82541403058474772, 0.82541356540027144, 0.82542492168455028, 0.82542187403264777, 0.82546141398510264, 0.82546266651066025, 0.8254585933982902, 0.82545904628896649, 0.82546412216551468, 0.82546299600020934, 0.82548479208686343, 0.82548993421176853, 0.82549893234764915, 0.82549982390777377, 0.82551328980125516, 0.82551852482108024, 0.82551917022937427, 0.82551188209647941, 0.82552271842370284, 0.82553429903692099, 0.82554034481714622, 0.82553873240279396, 0.82554498513351382, 0.82554360998272835, 0.8255441738463084, 0.82557577742067034, 0.82558677190873431, 0.82559533421127129, 0.82561251913666889, 0.82561431440331545, 0.82561028931840208, 0.82561500259926945, 0.82566579125554296, 0.82567020359500598, 0.82566940512749143, 0.82570045616780186, 0.82570693990870581, 0.82571672902471749, 0.8257223486290679, 0.82576052954284906, 0.82575911007210046, 0.82577648793188507, 0.82578639697734779, 0.82579262940358511, 0.82579797813799627, 0.82581827400237151, 0.8258491471213194, 0.82584650609463617, 0.82587817404206976, 0.82588034126929188, 0.82588004726229902, 0.825881637536658, 0.82591361640957595, 0.82591424684775172, 0.82591129339637281, 0.82594057776366636, 0.8259385929912838, 0.82594396965823069, 0.82594099193765247, 0.82594857779341202, 0.82596693462106219, 0.82599555743803332, 0.82601172727729555, 0.82602451557085954, 0.82602310192913342, 0.82603336799643912, 0.82602967272605343, 0.82603074091454887, 0.82603489651640627, 0.82605680089036149, 0.82605585497737322, 0.82605769209155822, 0.82607092729408715, 0.82607032738767727, 0.82608100229126502, 0.82607433765065397, 0.82607920445675254, 0.82607932199643819, 0.82608081088525365, 0.82608399296054102, 0.82608700751056574, 0.8261019008822863, 0.82609868324997549, 0.82610999715966749, 0.82612642382884827, 0.82612269246653047, 0.82612818194852333, 0.82613494573531854, 0.826132739944066, 0.82613101072547102, 0.82614077580169964, 0.82614109675584246, 0.82614784523434892, 0.82615446949248317, 0.82615278865246711, 0.82615488093828904, 0.82616376071826969, 0.82619732116094968, 0.82619480562725511, 0.82622509415231149, 0.82622899954240481, 0.82622933826231437, 0.82623942682321838, 0.82623961864669071, 0.82627245197057453, 0.82628103428516897, 0.82628102867379349, 0.82627978297879479, 0.82628069878592125, 0.82633182348243683, 0.82633420979454275, 0.82633553721825925, 0.82633779792817652, 0.8263514714203668, 0.82635584490362035, 0.8263538668883843, 0.8263554546697609, 0.82635696169924133, 0.82636843802566096, 0.8263673183302831, 0.82637504254685368, 0.82637909494091277, 0.8263881099993059, 0.82638854863474298, 0.82638443073786849, 0.82638995645888291, 0.82639570245049598, 0.82639343927684161, 0.82639444224934044, 0.82639259330021098, 0.82640028099898577, 0.82639894569608841, 0.82640202845405497, 0.82640065345811053, 0.82640204387076577, 0.82640859957388346, 0.82641150243881578, 0.82641680048189747, 0.82641552981907052, 0.82643042882676254, 0.82643051592599603, 0.8264330209790911, 0.82643178142538287, 0.82643583084838679, 0.82643233352498735, 0.82644200260720757, 0.82644059182213714, 0.82644145705749028, 0.8264440642416726, 0.8264459943280642, 0.826442025617521, 0.82644858760204953, 0.82644587102746081, 0.82644184289002298, 0.82644414181181425, 0.82644251423134063, 0.82645402936932721, 0.82645970936626656, 0.82651319909781196, 0.82651157247830087, 0.82651094183722462, 0.82652345398687788, 0.82652533008202911, 0.8265468136278965, 0.82656515472548586, 0.82656255812244694, 0.82656470488307965, 0.82656354867401638, 0.82656761823526459, 0.82656517786801209, 0.82657345064834831, 0.82657356472215859, 0.8265963399426417, 0.82659949217785389, 0.82660532802183728, 0.82662469003278771, 0.82664074114954289, 0.82665656382931663, 0.82665836723528785, 0.82665901882620629, 0.82665571060710952, 0.82665273847035381, 0.82665039754166647, 0.82664704779505005, 0.82664817939742719, 0.82665390672724315, 0.82665997347399855, 0.82668134400707183, 0.82668080113318643, 0.82669751051378348, 0.82671667927297321, 0.82672954237311325, 0.82673243626252613, 0.82674421338532089, 0.82674502465245747, 0.82677150116398668, 0.82677235676278615, 0.82677389369550003, 0.82679599086817768, 0.82679419888926931, 0.82681502005050156, 0.82681601011697448, 0.82681610023261309, 0.82682217662353952, 0.82682324536089491, 0.82682436730158559, 0.8268277270509754, 0.82683219352770931, 0.82683496368569198, 0.82683684045749872, 0.82683701396833675, 0.82683474518738098, 0.82683241524953444, 0.82685583648689587, 0.82685698920655248, 0.82685474136412773, 0.8268544204787005, 0.82685463999015185, 0.82686293723715987, 0.8268765207322577, 0.82687937839787651, 0.82688408230716381, 0.82688632237313675, 0.82690362262425821, 0.8269293994216802, 0.82693008383085387, 0.82693084565594999, 0.82695668286951585, 0.82695969325374519, 0.82696295647976559, 0.82696332977448372, 0.8269743592930231, 0.82698023015559097, 0.82698591813476729, 0.82698389783723736, 0.82698190229916302, 0.8269843696764424, 0.82697924002885159, 0.82697703075641671, 0.82698819153803937, 0.82698525828044767, 0.82699554184719959, 0.8270079211699507, 0.82700626113341813, 0.82701034971503873, 0.82701172774148224, 0.82704452682340079, 0.82704987911304639, 0.82705278206049504, 0.82705527010974789, 0.82705816944263544, 0.82706646815027207, 0.82706417559624035, 0.82706781059444801, 0.8270652955946709, 0.82707229605408372, 0.82707152503389181, 0.8270888097617608, 0.82711058009949112, 0.82711258246749275, 0.82714112999907352, 0.82715210811364925, 0.82715337799675537, 0.8271571397145866, 0.82715458540248998, 0.82715548341977507, 0.82715680687057314, 0.8271603219871313, 0.82715969542801759, 0.82716297103044811, 0.82716015163370005, 0.82716208421156234, 0.82716164004027137, 0.82716438356356914, 0.82716866731613448, 0.82717513127970843, 0.8271818133491875, 0.82718305601617581, 0.82717866371304694, 0.82721564273439019, 0.82721828764264238, 0.82721726370133219, 0.82721639001895275, 0.82721318443931158, 0.82723210861110608, 0.82723041557620647, 0.8272315539184627, 0.82723465205856805, 0.8272340397773863, 0.82723546471658782, 0.82723678544919266, 0.82724123634123148, 0.82724631289409234, 0.82724527674554515, 0.82724795159925235, 0.82724789163847634, 0.82726496591385534, 0.82726788372253535, 0.82727221807624962, 0.82728047059597853, 0.82729134060724685, 0.82730022402650838, 0.82730360062256469, 0.82730154493167496, 0.8273085296219479, 0.82731980492953539, 0.82733897748657326, 0.82734060174675295, 0.82733876392709005, 0.82733874352955306, 0.82733768417584808, 0.82733487747525358, 0.82733528250639166, 0.8273362642950518, 0.82733751211917439, 0.82733752732220689, 0.82734575151432599, 0.82734551341353324, 0.8273436012742712, 0.8273533456676837, 0.82735457329488504, 0.82735443483487559, 0.82738901674857002, 0.82738647999232207, 0.82738709824075518, 0.82738526403121748, 0.82738209720347045, 0.82739014136815281, 0.8273931549892648, 0.82739229625439692, 0.82739031527604256, 0.8273939265390613, 0.82739572780118475, 0.82740081597415238, 0.8273956043533659, 0.82740946356383982, 0.82740731792466737, 0.82740402832684712, 0.82740341012372287, 0.82741191960063976, 0.82741364603080425, 0.82740996305998171, 0.82740793948201163, 0.82741185410564899, 0.82741574188492406, 0.82741650679891043, 0.82741365185688154, 0.82744925130759006, 0.82748478203349851, 0.82750242971565524, 0.82751202708998917, 0.82750863762239601, 0.82752938094563788, 0.82754221350558377, 0.82759280111461897, 0.82759428743295937, 0.82759263357727553, 0.82759658366641131, 0.82759669173133132, 0.82759841434172132, 0.82759671850831285, 0.82759779605336781, 0.82759960320521286, 0.82760006812052023, 0.82760356831545112, 0.82760384192455272, 0.82760780939092038, 0.82761193329312233, 0.8276180174477199, 0.82761967444964313, 0.82763664059422415, 0.82763769128166209, 0.82763811159560219, 0.82763809360165497, 0.82765225992454872, 0.82765385395879787, 0.8276668941209977, 0.82767227115012165, 0.82768515239457852, 0.82769836380591122, 0.82769649702370884, 0.82770576087131253, 0.82773218617431699, 0.82773452151752003, 0.82773212928767559, 0.82773409010349464, 0.82773369985314837, 0.82773507770238552, 0.82774945968314173, 0.82775712079935437, 0.82775784828530763, 0.82775667735621727, 0.82775786625693237, 0.82776815057520226, 0.82776551509904195, 0.82777802681755708, 0.8277761231977957, 0.82778603615467627, 0.82778398882663728, 0.8277861410478351, 0.82783807651982522, 0.82784261576254825, 0.82784179010186665, 0.82788332123018371, 0.82787704163025888, 0.82788266338691641, 0.82788159187810173, 0.82788075054784027, 0.82788601005299345, 0.82790188652605834, 0.82791376511394632, 0.82792828247033512, 0.82795684998760777, 0.82798317262240817, 0.82798187594588857, 0.82797627479652758, 0.82797482825837709, 0.82800071570480305, 0.82801131231207825, 0.82801251294784739, 0.82800912726015097, 0.82800772247189847, 0.82800319831380842, 0.82800035883318723, 0.82800004361658119, 0.82801267504828358, 0.82801570128372626, 0.82801279215719492, 0.82802878654018774, 0.82803478447536505, 0.82804590634252428, 0.82805299984812653, 0.82805467554885848, 0.82806030977327727, 0.8280633902570953, 0.82806210609870345, 0.82810991667131328, 0.82811255536649031, 0.82811165051357205, 0.82811492238021889, 0.82812384076717138, 0.82812435470839585, 0.82812503341695154, 0.82812221418285303, 0.82811807490891487, 0.82811914282723842, 0.82812280467130728, 0.82812316830984189, 0.82812733509006298, 0.82812934673743555, 0.82812883593062525, 0.82813000665029401, 0.82814141751970638, 0.8281424742114899, 0.82814070902808656, 0.82816353539875287, 0.82816685826099368, 0.82817071820150423, 0.82818530775563703, 0.82818541240868748, 0.82818305333702558, 0.82819104153056478, 0.82819169932661529, 0.82818740097659393, 0.82818886325441277, 0.82818987234478103, 0.82819424891009574, 0.8281899384127771, 0.8281924689841953, 0.82818672649714764, 0.82819712152782776, 0.82819923748373747, 0.82822530543372519, 0.82823766458520487, 0.82823315619261206, 0.82823009142528647, 0.82822812423551029, 0.8282408724433814, 0.82825734068415391, 0.82826814110868074, 0.82826756208316699, 0.82826805965488082, 0.82826866669774879, 0.82827446613803191, 0.82827511479444094, 0.82827564947937071, 0.82827681076341997, 0.82828085715981281, 0.82828139787078514, 0.82828356290955951, 0.8282815605415268, 0.82831049920332833, 0.82831240801287298, 0.8283079022142521, 0.82830976963882219, 0.82831108508245666, 0.82832594370674195, 0.82833116742557833, 0.8283317648607218, 0.82832768208709806, 0.82832802159474284, 0.82835956372741815, 0.82837191624644846, 0.82838727743233243, 0.82838762291165846, 0.82838454843959342, 0.82837908812673522, 0.82839368577153372, 0.82840871304373676, 0.82841194846448918, 0.82841027667147082, 0.82841168499256879, 0.8284067252946764, 0.82840790799510489, 0.82840900962391417, 0.8284073552014114, 0.82840990105298862, 0.8284193692895625, 0.82842152499182742, 0.82842238060021156, 0.82841811507398122, 0.82841735052608212, 0.8284227534879729, 0.82844527546947, 0.82844706078573505, 0.82845109523149563, 0.8284612235127129, 0.82846277271607105, 0.82846158367328171, 0.82850034680896856, 0.8285034387796173, 0.82850539969980175, 0.82851200746266773, 0.8285211337194287, 0.82853993457799668, 0.82853980565713647, 0.82853607438131083, 0.82854769389085003, 0.8285466941986156, 0.82854267201905252, 0.82854229123992373, 0.82854157375313342, 0.82854570720926657, 0.82854784150872085, 0.82854890077517263, 0.82855536062521296, 0.8285543757587458, 0.8285529324287193, 0.82855438582296181, 0.82854922823826926, 0.82856294043162271], &#39;auc-stdv&#39;: [0.0039273400275318315, 0.0043085610327870533, 0.0038098819680477234, 0.0044687531596371384, 0.0042159047256537466, 0.0041820955664082426, 0.0039181504551309438, 0.0040467122737633011, 0.0038499171926264287, 0.003975363499636805, 0.0040591386585339716, 0.0039813032676712518, 0.0040543504232780029, 0.0039651715319890318, 0.0039253212388777126, 0.0039796634240438344, 0.0041949040906498151, 0.0040995772601997655, 0.0041888896892650733, 0.0041746895549988113, 0.0040479374230362975, 0.0040249444091543475, 0.0040325168745440915, 0.0040758462489215918, 0.0040031496052437311, 0.0039812583529241774, 0.004007050059018467, 0.0040330921757906124, 0.0041419079497246967, 0.004137865721464227, 0.004029587338402865, 0.003998224220121872, 0.0039914355100869785, 0.0040332856165308387, 0.0039980465065628714, 0.0040841263862979099, 0.0040893009447046883, 0.0040534875144092571, 0.0041337547768786924, 0.0041375506981059622, 0.0041633025330643789, 0.0041861340755737489, 0.0041493641959830622, 0.0041146399934458879, 0.0041134634644072977, 0.0040698946116284086, 0.0040779184976563873, 0.0041004904146003237, 0.0040431164921629929, 0.0040767672429831113, 0.0040238232638279249, 0.0040260316673452756, 0.0040336311808636975, 0.0040582711564072947, 0.0041005035081929566, 0.0041339386647654287, 0.0041679544991744723, 0.0041607647074213385, 0.0041542924733056633, 0.0041576905514160659, 0.004177956247698518, 0.0041354794510804977, 0.0041683336585451002, 0.0042081105810509644, 0.0041792946502947842, 0.0041706829660660746, 0.00416195597083663, 0.004179061923843921, 0.0041634675483034197, 0.0041537874949196156, 0.0041769434984590421, 0.0041999302717510723, 0.0041836003224976388, 0.0042221967955730884, 0.0042521635524697919, 0.0042441115104505569, 0.0042555725487108042, 0.0042402362910615938, 0.0042325906406968528, 0.0042156052384462635, 0.0042028394295368652, 0.0042225164101980343, 0.0042508071186334517, 0.0042410364966005056, 0.0042365984341337683, 0.0042262009278293336, 0.004209951174898181, 0.0042314835682438386, 0.0042310418058758249, 0.0042224943143976115, 0.0042296222204819892, 0.0042631883973510134, 0.0042642544336301312, 0.0042726660421907548, 0.0042701850275948337, 0.0042663030702482421, 0.0042701981355426208, 0.0042625693458570564, 0.0043074128804789563, 0.0043203670596254334, 0.0042913742473014663, 0.0042719718739918158, 0.0042846697823152365, 0.0042702176597815937, 0.0042409894295024004, 0.0042566547420705061, 0.0042743096994062937, 0.0042510772918715571, 0.004263141296828336, 0.0042285321390297895, 0.0042234568127571528, 0.0042324703449817293, 0.0042130746459049535, 0.0042135893278274035, 0.0042043631354464796, 0.0042362173939291172, 0.0042310394954777125, 0.0042054330252530641, 0.0042168037287100349, 0.0042163742122947982, 0.004222070835680056, 0.0042174647145333003, 0.0042496659202878352, 0.0042294789086274303, 0.0042608916068872273, 0.0042666869457289834, 0.0042673673548187458, 0.0042958330887072913, 0.0043143180449395939, 0.004330981889150814, 0.0043243147133044356, 0.0043356087828354305, 0.0043354738249671201, 0.0043389528160333512, 0.0043101451867629814, 0.0043423628294098444, 0.004362077281146186, 0.004356449463016397, 0.0043469750444222188, 0.0043520580198346485, 0.0043460301268782439, 0.0043371292290693194, 0.0043172629239779667, 0.0043032789521715055, 0.0043060799573335969, 0.004317743992986664, 0.0043160741582139728, 0.0043076050110587025, 0.0042965938397558434, 0.004314764568722775, 0.004302739380002348, 0.0043125444759343082, 0.0043212868075922523, 0.0043400826540523673, 0.0043345645762559006, 0.0043483440648628639, 0.0043528616356600866, 0.0043567326149632447, 0.0043499290552015662, 0.0043644193031286372, 0.0043679129873906925, 0.0043640835276342162, 0.0043798889499010781, 0.0043711970052474555, 0.0043761098458126278, 0.0043722195905294154, 0.0043623118381087046, 0.0043605875100869414, 0.0043700839388941806, 0.0043577661453508059, 0.0043448452145892569, 0.004370542906847681, 0.0043470534652196337, 0.0043652484454217806, 0.00436070104276127, 0.0043486131821600703, 0.0043540599228291416, 0.0043610333785329509, 0.0043599879398277964, 0.0043638294221163745, 0.0043633867585370426, 0.0043733981001275075, 0.0043685413038655266, 0.0043906042825306588, 0.0043993353690964495, 0.0044076578201035282, 0.0044170156867137508, 0.004443758840787358, 0.004436294427857479, 0.0044477760500035851, 0.0044478969445346682, 0.0044366194541044859, 0.0044225303800959834, 0.0044212443961471216, 0.00444971817598521, 0.0044441096222507708, 0.0044682898580986263, 0.0044769527422805358, 0.0044864301809988414, 0.0044939857629067159, 0.0044961225794170716, 0.0044818490019655462, 0.0044685997777751648, 0.0044841769832613477, 0.0044829880355300236, 0.0044799626335756638, 0.0044612116922485194, 0.0044518930507709293, 0.0044538830718877593, 0.0044417605361507413, 0.0044383869740073226, 0.0044376537400813067, 0.0044405657517280588, 0.0044269042920652563, 0.0044192204764503659, 0.0044190286597076706, 0.0044501116399677789, 0.0044362528194658553, 0.004434532327460561, 0.0044405407624541378, 0.0044492446461866552, 0.0044649145424266552, 0.00447961606635179, 0.0044852916077186469, 0.0044914271982562509, 0.0044808731332512831, 0.0044730884839506851, 0.0044615040297493408, 0.0044233186171467953, 0.0044741221448338754, 0.0044742717234207167, 0.0044566580854304748, 0.0044614440002801042, 0.0044694505135624086, 0.0044495578811830222, 0.0044544377899570272, 0.0044552335865434885, 0.0044515322771047057, 0.0044526804441496048, 0.0044521203464290872, 0.004443079896512183, 0.0044432721124830305, 0.004463866768750214, 0.004443495131885032, 0.004436103661236074, 0.0044326807031385626, 0.0044308086854508858, 0.0044371645828523319, 0.0044312783391603043, 0.004434900080054708, 0.0044306834016807142, 0.004403979249791825, 0.0043870545627506747, 0.0043796338891726495, 0.0044088942934947016, 0.0043941871477022826, 0.0043925873216590578, 0.0043768216768787869, 0.0043702127083314318, 0.0043812855822397941, 0.0043587418781745272, 0.004364713673480753, 0.0043639276017118289, 0.0043715829877016407, 0.004361500743875325, 0.004342649589228015, 0.0043426212693742135, 0.0043374402624709527, 0.0043306789421793434, 0.0043395315372973418, 0.0043588282895667902, 0.0043403073079488225, 0.0043384337399329786, 0.0043260579348333794, 0.0043251237848630532, 0.0043153455052323498, 0.0043351388930225106, 0.0043365678297430195, 0.0043461411068270337, 0.0043456607283676937, 0.004347395010962889, 0.004359621665120824, 0.0043590242093063314, 0.0043553651071455354, 0.004370687164804864, 0.0043625376859850124, 0.0043556216480761982, 0.0043694970778275733, 0.0043630229906539263, 0.0043651641587605988, 0.0043712554305904237, 0.0043671444800223993, 0.0043634700006395426, 0.0043596123079637852, 0.0043403102368230369, 0.0043271141039861322, 0.0043413442720303883, 0.0043560546603579582, 0.0043314845604404857, 0.0043267884614833938, 0.0043283422153702084, 0.0043051460585614709, 0.0042898085918564545, 0.0042984131535893149, 0.0043033467666135959, 0.0042917917485676832, 0.0043054051170574453, 0.0042939665399049694, 0.0043084859480845856, 0.0043107925351135055, 0.0043027044288465096, 0.0042936323168658013, 0.0042914352276422676, 0.0043085193843432944, 0.0043080447554630615, 0.0043156125159976941, 0.0043327389942966156, 0.0043213732793246891, 0.004307496745339888, 0.0043015815235921233, 0.0042949304784715564, 0.0043044110700578703, 0.0043002509936022606, 0.0042982748183861118, 0.0042800645661854458, 0.0042772457445511098, 0.0042943643755317456, 0.0042791142858027622, 0.0042808453924408673, 0.0042755242395192067, 0.0042660373925117621, 0.0042905060011373484, 0.0042964415714369514, 0.004280168507845075, 0.0042742643693796222, 0.0042645755076308901, 0.0042649641836052868, 0.0042724361866416814, 0.0042652275225431076, 0.0042563222738580273, 0.0042651978055170568, 0.0042579434631448447, 0.0042539130570817211, 0.0042477015743609186, 0.0042338157308441041, 0.004233470627909971, 0.0042347849230698993, 0.0042266356073683729, 0.0042249101501293735, 0.0042275782659592937, 0.0042327610328996446, 0.0042519890974544094, 0.0042566249125975753, 0.0042458026311211171, 0.0042530340593186907, 0.0042446518666042496, 0.004245582681068347, 0.0042741063071457495, 0.0042750967002535833, 0.0042815886282222999, 0.0042783719094684966, 0.0042654818997772976, 0.0042760827380632586, 0.0042784317229368875, 0.00427524286433369, 0.0042696787832035114, 0.0042800504529246601, 0.0042827641616653986, 0.0042968102068445405, 0.0042950572502742455, 0.0042828483418847147, 0.0042745068237143727, 0.0042624529488870895, 0.0042461097235301911, 0.0042473251318198776, 0.0042514030273038987, 0.0042475414873821248, 0.0042514538007938371, 0.0042522952060804762, 0.0042312628521139388, 0.0042266151668439575, 0.0042216727042514073, 0.004226442785273659, 0.0042187498117405887, 0.0042216664826731278, 0.0042255517253602737, 0.0042382292314823468, 0.0042590645770976777, 0.0042605187200894668, 0.0042666266140888708, 0.0042693334454005005, 0.0042695414415319585, 0.0042684652393532446, 0.0042598983994466579, 0.0042503353210961497, 0.0042568829134768272, 0.0042583682888961399, 0.0042613085341760915, 0.004260693088855497, 0.0042724004921915583, 0.0042664500564556767, 0.0042793291437730413, 0.0042806170072456932, 0.0042769109495258912, 0.0042861680613411569, 0.0042796787442996101, 0.0042861321296791734, 0.0042722318169709367, 0.0042760946852767028, 0.0042884748224469235, 0.0042788264095805443, 0.004261804091892793, 0.0042575383836278881, 0.0042638710097965844, 0.0042583337001534998, 0.0042643929646549433, 0.0042728190198479225, 0.0042691788234586001, 0.0042681306348727127, 0.004293737272591038, 0.0043018863061641916, 0.0043054938865564928, 0.0043128451364676376, 0.0043148892387915123, 0.0043124976386309157, 0.0043167266610352056, 0.0043309012757254033, 0.0043307661646817455, 0.0043295109867398018, 0.0043153231585767891, 0.0043049904370848794, 0.0043081659082665541, 0.0043168757822531229, 0.0043149890414883865, 0.0043023149279147067, 0.0042977880099363394, 0.004300656518760779, 0.0042985665275850678, 0.0043043775505082248, 0.0042941733386726881, 0.0042937622736323941, 0.0043004421373058609, 0.0042865663052212713, 0.0042767654038589542, 0.0042740111628259041, 0.0042715570515892923, 0.0042625534626574034, 0.0042543984941283627, 0.0042602641021526125, 0.0042612624977133541, 0.0042561930461377856, 0.0042552558817806332, 0.0042533424154821229, 0.0042672666360087982, 0.0042562373626990248, 0.0042609300684468311, 0.004267327464522395, 0.0042665689896131519, 0.0042619686523792857, 0.0042517570961723731, 0.0042494891470252196, 0.0042444232998300703, 0.0042454118845809668, 0.0042387773004342469, 0.0042371049557373254, 0.004235350669541363, 0.004235638077350585, 0.0042364352359288851, 0.0042409003386805321, 0.0042421955216615801, 0.0042414436627442615, 0.004239534783064108, 0.0042323197774988526, 0.004245778396832817, 0.0042371633027746738, 0.0042352991697381989, 0.0042365941599004121, 0.0042241166323757021, 0.0042241791138050997, 0.004223883259055301, 0.0042225486979205499, 0.004205767804731364, 0.0042057506069897296, 0.0042060661076769694, 0.0042042908671757195, 0.0042129868455968288, 0.0042047879770889147, 0.0042058416118518141, 0.0042045215881269943, 0.0042026773919627012, 0.0041994096533484807, 0.0042009594365101256, 0.0041997700185539104, 0.0042024584639120694, 0.004199872086154465, 0.0042028787851243069, 0.0041969884694174828, 0.0041900420699076939, 0.0041877906045804649, 0.0041826567440284785, 0.0041826220907809604, 0.0041835063746081674, 0.0041846659006798083, 0.0041882452661498653, 0.004189139923584685, 0.0041814557963475753, 0.0041744971531372972, 0.0041764695866184521, 0.0041769695340617956, 0.0041825443245828656, 0.0041921472299798306, 0.004200866858353935, 0.004202436555638912, 0.0041912778336778015, 0.0041803123164800334, 0.0041686308028195335, 0.0041712791897353472, 0.0041770272081267865, 0.0041709210018515269, 0.0041735927120467595, 0.0041796200301225652, 0.0041784892030469556, 0.004186764797914415, 0.0041832075750360834, 0.0041850396571041825, 0.0041854034480012601, 0.004196570530543244, 0.004194047966449314, 0.0042017210727363285, 0.0041824567853073038, 0.0041845794225135961, 0.0041880592901085683, 0.0041808592706988777, 0.004184592827635023, 0.0041795301976332031, 0.0041743762969815119, 0.0041750765512888876, 0.0041705636880204197, 0.0041633099366976513, 0.0041608874418441683, 0.0041637572128453149, 0.0041573278609600825, 0.0041561564408212019, 0.0041578024387534515, 0.0041676491932478759, 0.0041690360941952099, 0.0041622916903510908, 0.004162324851936231, 0.0041516981034336667, 0.0041474920207668778, 0.0041501564208609445, 0.0041392162041566831, 0.0041296053479313397, 0.0041337928681704876, 0.0041290563090840005, 0.0041300940887321102, 0.0041319503438085516, 0.0041238125271554132, 0.0041173859737520168, 0.0041211783422513571, 0.0041316366434576363, 0.0041315656573923032, 0.0041301451562913575, 0.0041285654505604091, 0.0041356394124545445, 0.0041299724657409397, 0.0041336900069682313, 0.0041326970263234957, 0.004123955211382993, 0.0041252245643525857, 0.0041251110306152498, 0.0041148724697484609, 0.0041109337719250384, 0.0041073346742551741, 0.0041003252588984384, 0.0041073191732223261, 0.0041018525568592918, 0.004099121708328473, 0.0041061024082414705, 0.0041066327720409371, 0.00410277138384583, 0.0041012729235823768, 0.0041004504191387951, 0.0040995013012201185, 0.0040974639025974039, 0.0041060786198333062, 0.0041074163443360189, 0.0040984409783708384, 0.0041021852428689632, 0.0040946228265191969, 0.0040867534996615129, 0.0040801369225855469, 0.0040794857095983227, 0.0040751898132095066, 0.004078980373567972, 0.004077142508003455, 0.0040740066852615409, 0.0040745896645934193, 0.0040732392916466272, 0.0040751012198177779, 0.0040751975427929556, 0.0040691984300168247, 0.0040701476744261266, 0.0040808857598621465, 0.0040712440721842489, 0.0040725617282104352, 0.0040681693894350736, 0.0040658058216126083, 0.0040678334561767143, 0.0040666484113791391, 0.0040724071669225531, 0.0040633803462592861, 0.0040599696988515149, 0.0040701735349796837, 0.0040705620158450516, 0.0040741163989759131, 0.0040815767385973889, 0.0040774724478194278, 0.0040768572800321803, 0.0040740713415157422, 0.0040721688875252062, 0.0040770438472886349, 0.0040636303288541066, 0.0040694353641276236, 0.0040782490677629445, 0.0040876579330291277, 0.0040880918625872979, 0.0040868986072407415, 0.0040862597569772674, 0.0040899640165293774, 0.0040854344200698459, 0.0040926234353535727, 0.0040908250372235421, 0.0040908450835307154, 0.0040837081382391156, 0.0040888566652701915, 0.0040853676703834513, 0.0040823432972496854, 0.0040802078464427143, 0.0040815163199108039, 0.0040858482795118533, 0.0040806679483353569, 0.0040643320561817974, 0.0040722288221638247, 0.0040694919816874793, 0.0040685107729019395, 0.0040727712415582007, 0.0040675707468154614, 0.0040677596799915594, 0.0040625510813292387, 0.0040602552355225559, 0.0040614768108836281, 0.0040603460175325963, 0.0040647770805185002, 0.0040620085100373524, 0.0040589567140667032, 0.0040659900670796823, 0.0040681116046846193, 0.0040672418143046802, 0.0040620485224778496, 0.0040650795560813495, 0.0040712670806744003, 0.0040710062535196014, 0.0040722471177420724, 0.0040657354100059953, 0.0040647275365220991, 0.004064074675757594, 0.0040560729202929859, 0.0040640724182951123, 0.0040682193619626531, 0.0040663381803616736, 0.004064681722285338, 0.004062728214290089, 0.0040652027175131543, 0.0040614420342543001, 0.0040584416116913064, 0.0040604295674791105, 0.0040642856281103033, 0.0040664769608054234, 0.0040681637522349606, 0.0040700743535353754, 0.0040727683776905482, 0.0040721155521539956, 0.0040681468888675628, 0.0040682293670596219, 0.0040745566751674758, 0.0040734278466857581, 0.0040752458295408214, 0.0040694707672036153, 0.0040713828491927995, 0.0040700476642189327, 0.0040719130155266086, 0.0040816265511960425, 0.0040772773261154522, 0.0040723292069161816, 0.0040687403798864144, 0.0040685331894812693, 0.0040695852813658331, 0.0040722563624996828, 0.0040784170659147806, 0.0040802941994357925, 0.0040847252651558912, 0.0040821175271847218, 0.0040856340333713971, 0.0040865656397334948, 0.0040893063091052726, 0.0040888609421133419, 0.0040764523303290761, 0.0040803398024818289, 0.0040794325765597123, 0.0040795556767371033, 0.0040745581772018754, 0.0040697528127612605, 0.0040709371353383622, 0.0040706786232695101, 0.0040765059259673326, 0.0040714251263843952, 0.0040693959850426351, 0.0040629769700684139, 0.0040680967750339279, 0.0040687764911738096, 0.0040638492315995143, 0.0040634884381031142, 0.0040632344101398333, 0.0040661135668045179, 0.0040686345739077033, 0.0040748694251050988, 0.0040680413954505695, 0.0040711842046117471, 0.0040668556869590654, 0.0040645281616024731, 0.0040620622269991555, 0.0040594299161595526, 0.004062284053567667, 0.0040604524261026627, 0.004055388059742147, 0.0040498878565785853, 0.0040534540511520303, 0.0040530792782964191, 0.0040577845683834029, 0.0040527403820526595, 0.0040434277643184051, 0.0040371978832008252, 0.0040415109487008305, 0.0040452266113274928, 0.0040424709721478625, 0.0040409103546651462, 0.0040505969797216389, 0.0040574035847206914, 0.0040539099860757944, 0.0040561374757025733, 0.0040530599781324556, 0.004044733079220415, 0.0040488285150444715, 0.0040386750941935187, 0.0040325421699458916, 0.0040355968264762128, 0.0040410965243866913, 0.0040429139506169189, 0.0040430509297813225, 0.0040374425328283262, 0.0040352210742057798, 0.004039240618019399, 0.0040440307772539544, 0.0040362211348944285, 0.0040424107162226278, 0.0040372128958831117, 0.0040282339689931956, 0.0040311632426238966, 0.0040337865312827011, 0.0040349702503607232, 0.0040280624180400884, 0.0040274611930811444, 0.0040320172826482596, 0.0040254415328722518, 0.0040276404803625768, 0.00402170731133435, 0.0040241764561866249, 0.0040287198681644527, 0.0040286622451171962, 0.0040315066009552498, 0.004033306642933116, 0.0040371033242929243, 0.0040210036905600488, 0.0040206883098505416, 0.0040172542677514621, 0.0040219973454852388, 0.0040238434897327847, 0.0040194630406143062, 0.0040210081789137974, 0.0040207735951193367, 0.0040186295301801473, 0.0040173064597783092, 0.0040233895495359537, 0.0040175628276606819, 0.0040184275407204101, 0.0040152270770425838, 0.0040091645377968105, 0.0040015413565334277, 0.0040066588155095988, 0.0040092526818167164, 0.0040023936172708897, 0.004004535178569058, 0.0039939026506983343, 0.0039959870306839427, 0.0039968237101083489, 0.0039918304833523504, 0.0039878661842296598, 0.0039922780914325787, 0.0039914261885984469, 0.0039981581845205862, 0.0040002891371823538, 0.0040063051889951936, 0.0040099647468930348, 0.0040167387636180791, 0.0040204560406582306, 0.0040153413217000628, 0.0040168103163837638, 0.0040151197479093882, 0.0040099579608498309, 0.0040152968395169878, 0.0040210432651295765, 0.0040254437824724827, 0.0040233634760141459, 0.0040196852967032872, 0.0040236501894427923, 0.0040225345525419891, 0.0040235379426494869, 0.004017103049135962, 0.004015517896972293, 0.0040201518993179287, 0.0040196407654444141, 0.0040201150229932361, 0.0040284436565492521, 0.0040315182634943943, 0.0040305379387101711, 0.0040361362631948939, 0.00403137348345259, 0.0040261307693053592, 0.0040265194858763497, 0.0040289506577399024, 0.0040313775846621738, 0.0040318810244177163, 0.0040320673240363137, 0.0040301536634494133, 0.0040321780536200251, 0.004034718301810597, 0.004037229457291726, 0.0040392317238655785, 0.0040384324274900657, 0.00404459877045572, 0.0040462038872276108, 0.004048674487022748, 0.0040496949756594503, 0.0040484090767439326, 0.0040467557597292604, 0.0040477704390166408, 0.0040480605407275392, 0.0040429411877173881, 0.0040376639983060224, 0.004049584046386466, 0.0040586230138171852, 0.0040542012444618244, 0.0040506032721915886, 0.0040439382435176726, 0.0040409552241427371, 0.0040378923392428876, 0.0040373434468262996, 0.0040370078798830367, 0.0040365908191890313, 0.0040386220203355435, 0.0040327002649619631, 0.004029607382397089, 0.0040237880781010208, 0.0040305255709199095, 0.0040281633191140368, 0.0040282758227457539, 0.0040355489368317024, 0.0040313043981710711, 0.0040324080464964312, 0.0040377956240424152, 0.0040377579940075398, 0.0040389414549447649, 0.0040418718427960845, 0.004042266731739296, 0.0040539335521112161, 0.0040557928784699087, 0.0040649373957652168, 0.0040639548649218163, 0.0040628199931810214, 0.004065304388777064, 0.0040584125391536115, 0.0040600156916343767, 0.0040560298546914626, 0.0040562561988817377, 0.0040590279249941644, 0.0040563347008371348, 0.0040560901224634396, 0.0040543209641765721, 0.0040527775809859048, 0.0040611372092656736, 0.004072026392948013, 0.0040731614173338745, 0.0040756414320899581, 0.0040755538800046716, 0.0040720531238947963, 0.0040671222556065258, 0.0040688421278865039, 0.0040641622547244972, 0.0040589796176240854, 0.0040559372475256587, 0.0040529873063944663, 0.0040575077779378072, 0.0040552235416458334, 0.0040528609005529492, 0.0040558224038882273, 0.0040529232188572751, 0.0040556974893381616, 0.0040551433911501294, 0.0040588452293875577, 0.004051792785950933, 0.0040570853854375042, 0.0040528329253235037, 0.0040432802749932103, 0.0040483390890794814, 0.0040522878266518163, 0.0040527872281907384, 0.00405627309668151, 0.0040567091049477054, 0.0040463173445504739, 0.0040448810331035505, 0.0040358263965181067, 0.0040420425534582532, 0.004043971316733147, 0.0040404861451768951, 0.0040347133310915688, 0.0040355733925598629, 0.004041248336193709, 0.0040416289328544562, 0.0040439036929549657, 0.0040464962091810146, 0.004050214308643019, 0.0040455413887616016, 0.004043302806609827, 0.0040448926257583675, 0.0040537526495390457, 0.0040525907625065588, 0.0040509240608212716, 0.0040529793776279549, 0.0040505786310696826, 0.0040555116842060169, 0.0040486390659807414, 0.0040539133595409839, 0.0040497020576206536, 0.0040506239094408245, 0.0040529071611376272, 0.0040650717110106149, 0.0040681318865817247, 0.0040622031196875241, 0.0040613980263868449, 0.0040559898445809489, 0.0040546958739142045, 0.0040556416099130233, 0.004056817220739699, 0.0040552922425347339, 0.0040442719493053187, 0.0040475435730097185, 0.0040491529491818775, 0.0040521978904863891, 0.0040622757161489463, 0.0040729992779263077, 0.004068641279882791, 0.0040672169179856599, 0.0040658244434502566, 0.0040576298340532337, 0.0040585196935963852, 0.0040599170730446195, 0.0040566310006959786, 0.0040542851398068331, 0.0040535841621102634, 0.0040575149023234329, 0.0040583698975387326, 0.0040614996548665348, 0.0040593075576102547, 0.0040568121082721811, 0.0040498240395372698, 0.0040547470428855484, 0.0040535927348172209, 0.0040552935350933227, 0.0040610959998800935, 0.0040588418418433091, 0.0040588929527189113, 0.0040620898678228644, 0.004058111758142571, 0.0040565386006359517, 0.0040536819013950679, 0.0040514085547725259, 0.0040511414143582693, 0.0040489864329043036, 0.0040492815186234567, 0.004050144042300881, 0.0040514782364529447, 0.0040498308895898227, 0.0040579582337666332, 0.0040637803648319679, 0.0040625400735735369, 0.0040595335879325557, 0.0040661890801310351, 0.0040655281755829697, 0.0040727615711597448, 0.0040695902060951606, 0.0040708951945047948, 0.0040642288420941345, 0.0040650721014407563, 0.0040618672127699269, 0.0040596402951546652, 0.0040608770016814094, 0.0040636787306924859, 0.0040628931640846275, 0.0040600565463466114, 0.0040639443403962068, 0.0040718801029738612, 0.0040749983079127316, 0.0040758244084400297, 0.0040794995797118934, 0.0040786006908032106, 0.0040779578270588373, 0.0040739434806771405, 0.0040706342848422514, 0.0040697568589782218, 0.004068508915032373, 0.0040678860465885609, 0.0040702644795216339, 0.0040686645578186121, 0.0040709560400149008, 0.0040616481724050435, 0.0040702381543794037, 0.0040704190550556333, 0.0040673244170585342, 0.0040662331351210826, 0.004067474926494119, 0.0040678860464404649, 0.0040648564677411552, 0.0040663520834007148, 0.0040714371827704392, 0.0040788184045874197, 0.0040869904236178695, 0.0040836551000930687, 0.0040796363569063059, 0.0040781795306553074, 0.0040809817915728472, 0.0040882786855570025, 0.0040849914222809936, 0.0040929821775607597, 0.0040939847829701758, 0.004096365461704529, 0.0040919845779911644, 0.0040983316782688311, 0.0040942229694967804, 0.0040952808404672398, 0.0040968858885524198, 0.0040961084343705241, 0.0040952801555954946, 0.0040968928321029382, 0.0040981150131591242, 0.0040993592404457739, 0.0041024892789441368, 0.004094654225808095, 0.004093900454491187, 0.004086052389314824, 0.0040876280911468351, 0.004089751907993551, 0.0040901573747541067, 0.0040853658752991306, 0.0040848972022873896, 0.0040849426713579348, 0.004094616737556827, 0.0040968571853613866, 0.0040992799365391734, 0.0040979109352026269, 0.0040958895942358508, 0.0041020342724784661, 0.0041105063822663218, 0.0041162976943036559, 0.0041084260123927744, 0.0041095112198119999, 0.004113065242078163, 0.0041105380284667346, 0.0041172005344569437, 0.0041152337292705253, 0.0041156958049184105, 0.0041209287391536075, 0.0041176346935752652, 0.0041204418730715559, 0.0041197105054780209, 0.0041213606535761196, 0.004123434605814635, 0.0041269215488213568, 0.0041294515918958424, 0.0041294102776897809, 0.004129050379492045, 0.0041291425266277437, 0.0041270419094391496, 0.0041240200128714877, 0.00412274149654588, 0.0041305149417099115, 0.0041275842464692184, 0.0041320345828680296, 0.0041380159418880243, 0.0041430955768442653, 0.0041410381145321577, 0.004140507999668073, 0.0041394825890260058, 0.0041448191791817598, 0.0041448857982085063, 0.0041436536501636432, 0.0041392668232579271, 0.0041406734307775596, 0.0041366480398844817, 0.004141328263313677, 0.0041397763887550546, 0.0041382059521010558, 0.0041326385945334339, 0.0041352988501787312, 0.0041315103065298741, 0.0041375242485719305, 0.0041347358559808284, 0.0041259397638608118, 0.0041183641412220759, 0.0041145839020857635, 0.0041167446177373807, 0.0041180104714097689, 0.0041278920185184916, 0.0041237848493737815, 0.0041265786881581916, 0.00412666281299313, 0.0041236780748782932, 0.004124983275735121, 0.0041298761550626027, 0.0041306149264591129, 0.0041282073705183296, 0.0041329086604885183, 0.0041289253873833592, 0.0041206031506915196, 0.0041128829871676636, 0.004112653970215892, 0.0041090022893199019, 0.0041076121087857236, 0.0041148461581637114, 0.0041164250089094929, 0.0041110739437673844, 0.0041082158046375711, 0.0040951647764692787, 0.004092593687969763, 0.0040929129875097309, 0.0040870790644217782, 0.0040829113562138994, 0.0040803601042685459, 0.0040780149248138154, 0.0040795323251386377, 0.0040745340389498784, 0.0040725876908065074, 0.0040688334100984561, 0.0040687862758004021, 0.0040668613109355687, 0.0040654588809530554, 0.0040634622051688779, 0.0040672583514350493, 0.0040756695466520835, 0.0040707901436062028, 0.0040729193700589222, 0.0040707852499009623, 0.0040639215666100938, 0.0040638051679914222, 0.0040630509102716336, 0.0040562870040455603, 0.0040600772066668456, 0.0040602188862163572, 0.0040555770963729629, 0.0040509490209567848, 0.0040494189521714814, 0.0040543415225454049, 0.0040546546177259689, 0.0040484314716350382, 0.0040389261933149369, 0.004039994578519523, 0.0040418873970303497, 0.0040331869388356414, 0.0040314112889750361, 0.0040336710416681429, 0.0040386184225779456, 0.0040350078570022705, 0.0040326668264339298, 0.0040413983034959453, 0.0040452526143483223, 0.0040477571740312277, 0.0040518483157899795, 0.0040514894487632463, 0.0040440601615783119, 0.0040386809164201978, 0.0040390995289469739, 0.004041324337428499, 0.0040377009447927527, 0.0040314661261264513, 0.0040280552456827025, 0.0040296712896175958, 0.0040307617314745622, 0.0040309332014244609, 0.0040342651590268303, 0.004037796402343123, 0.004038548905466603, 0.0040387784019831098, 0.0040432681329998464, 0.0040476499190908993, 0.0040513092485589602, 0.0040496618723753737, 0.0040501108748477515, 0.0040499786112828569, 0.0040538077427461497, 0.0040537897261828278, 0.0040512560131566906, 0.0040516851065059709, 0.0040492679437736396, 0.0040502318601827979, 0.0040547260811541657, 0.0040500176035443043, 0.0040508006999086712, 0.0040559940996777455, 0.0040516966782940386, 0.0040582865977566241, 0.0040572724459263113, 0.0040576493865467503, 0.0040615925350130644, 0.004065798555397287, 0.004063622963982866, 0.0040665921541802518, 0.0040665490989539527, 0.0040669653504581829, 0.0040684685332142596, 0.004068172870484589, 0.0040759028182741379, 0.0040731860161809371, 0.0040710419298010522, 0.0040716119961641001, 0.0040681047546030124, 0.0040711442510039508, 0.0040657134896367403, 0.0040693109405179817, 0.0040726092773704743, 0.0040725233557429973, 0.0040703986055198984, 0.0040702245775771761, 0.004063336418973734, 0.0040624936431687061, 0.0040687560127170026, 0.004069244784778262, 0.0040671659661878166, 0.0040693121346580567, 0.0040707406819322492, 0.0040656310067723692, 0.0040617667265352942, 0.0040650736641898721, 0.0040623674589116299, 0.0040544592421149651, 0.0040522790424020261, 0.0040558531196569674, 0.0040560769772566944, 0.0040522204976601557, 0.0040544906522553088, 0.004045439637842871, 0.0040498687555376627, 0.0040550574379370945, 0.0040515084546746596, 0.0040503793707960404, 0.0040438398911857559, 0.0040455474388983214, 0.0040462153504231902, 0.004042222453272278, 0.0040405041099269273, 0.0040435474265833804, 0.0040453998869471484, 0.0040480867610753098, 0.0040452538404022577, 0.0040454043844423467, 0.0040461482327253271, 0.0040469337181731338, 0.0040418322199949758, 0.0040440606604626129, 0.0040446312097695376, 0.0040411328882752184, 0.0040429975123798837, 0.0040466649482775174, 0.0040486301701041054, 0.0040433903527494711, 0.0040478374414247929, 0.0040416632435905818, 0.0040387670494955469, 0.0040331083410185915, 0.00403232222472788, 0.0040277711373117274, 0.0040233632648501846, 0.0040232119972430673, 0.0040255952085818045, 0.0040241808695365344, 0.004021813119901292, 0.004021646865040784, 0.0040210903989922765, 0.0040237799666035115, 0.0040242812887706387, 0.0040219239959605807, 0.0040243655163287274, 0.0040274235802487965, 0.0040243201050550841, 0.0040206121062138118, 0.0040222469920668299, 0.0040076538703014818, 0.0040093294993034552, 0.0040111120615620071, 0.0040098922087265388, 0.0040127494342112307, 0.0040129516194744516, 0.0040127733465379943, 0.0040174943141521929, 0.0040204665812167912, 0.0040189408685849526, 0.0040248181542670673, 0.0040379046585142164, 0.0040366208919429243, 0.0040392700459355932, 0.0040382245023740642, 0.004036453082812308, 0.0040412198171639029, 0.0040387646185296062, 0.0040437751049089742, 0.0040434534359968695, 0.004039085363464345, 0.0040443438519794121, 0.0040425425938939571, 0.0040426868239556383, 0.0040364522965766472, 0.0040390371705542371, 0.0040354435156813463, 0.004040569565266711, 0.0040422677657680588, 0.0040414848266358116, 0.0040459024768095247, 0.0040488248404264163, 0.0040450003709236364, 0.0040532551598465028, 0.004054498242601544, 0.0040436556955084418, 0.0040520406639043731, 0.0040516865463747767, 0.0040546077850882167, 0.0040526171510576791, 0.0040500402481960683, 0.0040530724294053555, 0.0040532352846546207, 0.0040482110604251097, 0.0040525949687390478, 0.0040582247194618859, 0.0040620418642486125, 0.0040565602894399068, 0.0040511204785383574, 0.0040484932828496094, 0.0040465639411772416, 0.0040368418835799724, 0.004042463830388969, 0.0040427156607371019, 0.0040416726251121292, 0.0040432662491666467, 0.0040463086886212305, 0.0040491554821024392, 0.0040490822826510699, 0.0040486884423360549, 0.0040469635551391549, 0.0040474450838320596, 0.004042812932599611, 0.0040407204887689284, 0.0040411070476002228, 0.0040417780874809064, 0.0040434779950838107, 0.0040446461167542103, 0.0040475025344852561, 0.0040518622925211113, 0.0040522670582471502, 0.0040509729902479417, 0.0040527573134258154, 0.0040521389588741652, 0.0040456796964850623, 0.0040488927800163979, 0.0040460101520594267, 0.0040443820858390277, 0.004050180717266939, 0.0040464760690466263, 0.0040428118388516968, 0.0040381989042240938, 0.0040409485421200051, 0.0040444895552127456, 0.0040433919281113954, 0.0040456947669355262, 0.0040452852598390479, 0.0040425723375807649, 0.0040441555828104594, 0.0040443816327841997, 0.0040406470442920159, 0.0040419313011151583, 0.0040462631791100953, 0.0040462499086338236, 0.004038242610299456, 0.0040358568188580413, 0.0040411414291431583, 0.0040392384378260581, 0.0040377673598231923, 0.004033232013624044, 0.0040311464814468097, 0.004032819906891757, 0.0040292651328369192, 0.0040345674380624233, 0.0040345331780587664, 0.0040337209950345107, 0.0040417488598765705, 0.0040447941357142116, 0.0040511907357537432, 0.0040486578224628597, 0.0040411815211425809, 0.0040399439874853511, 0.0040382950177275199, 0.0040414710787363079, 0.0040403675439220701, 0.0040407628824597292, 0.0040450634198711055, 0.0040461375941922822, 0.0040540488569876798, 0.0040617461582122694, 0.0040686447707743217, 0.0040621425243645487, 0.0040601214525160419, 0.0040583677182046054, 0.0040561812439261873, 0.0040582823600684194, 0.0040589964249030074, 0.0040592726173810639, 0.0040590339796216859, 0.0040543800726307797, 0.004055825760934992, 0.0040502590023425216, 0.0040519834527631866, 0.0040590261521166755, 0.0040623515794005237, 0.0040664258794478056, 0.0040677569697994027, 0.0040721239035392204, 0.0040649507848046799, 0.0040726018342257028, 0.0040737667908326584, 0.0040683976552069404, 0.0040659590654529738, 0.004066447724176349, 0.0040663334232411957, 0.0040655087180272434, 0.0040673624867909718, 0.004066475957664635, 0.0040645241104789879, 0.0040611592550413378, 0.0040611408132167227, 0.0040661668776249254, 0.0040689613526740638, 0.0040704204449937796, 0.0040684205195882809, 0.004063759620290491, 0.0040654330285727867, 0.0040662168261023932, 0.0040651902098119887, 0.0040717667523142032, 0.0040707583400810821, 0.0040734807790799629, 0.0040717709383830203, 0.0040746377013057356, 0.0040692415303008236, 0.0040753387467847577, 0.0040685580740647091, 0.0040704717819891175, 0.0040711619769051403, 0.0040730685731644933, 0.0040734981367144853, 0.0040728644449810536, 0.0040723482653717774, 0.0040723636888597104, 0.0040808008327517546, 0.0040719695129419952, 0.0040778792272135453, 0.0040806523025824683, 0.0040798612117440899, 0.0040768413782289242, 0.0040751941509128116, 0.0040713990901990651, 0.0040729503904691133, 0.0040661729987007235, 0.004065255535040454, 0.0040649125073174332, 0.0040680885140751931, 0.0040704511158783296, 0.0040694896416805819, 0.0040681096369566301, 0.0040701385027916339, 0.0040742068230820716, 0.004075335671609788, 0.004083770365648688, 0.0040852291952409416, 0.0040861536822840759, 0.0040838734409814241, 0.0040866004116301945, 0.0040877381278902804, 0.0040883301317600025, 0.0040865813035938989, 0.0040885847042523137, 0.0040910815592871495, 0.0040957018572900898, 0.0040997152635717593, 0.0041002832111952641, 0.0041016395565751148, 0.0041040099275760529, 0.0041054527049933153, 0.004102960647486489, 0.0040968481385112668, 0.0040994598222442004, 0.0041009689541385468, 0.0041035845787652774, 0.0041014404611230534, 0.0040982389738677683, 0.0040979853293799124, 0.0040949886365555981, 0.0040985538098909126, 0.0041022150249755427, 0.0041038531187457487, 0.0041014003100090339, 0.0041015760907297232, 0.0040974920333446997, 0.0041048812248600649, 0.0041051475264937808, 0.0041029016186081204, 0.0041070942190342963, 0.0041067611977703925, 0.0041060676521306374, 0.0041074724493309813, 0.0041067386116015406, 0.0041098434513042893, 0.0041133766421925034, 0.0041104358242561008, 0.004111618467397821, 0.0041084297354807902, 0.0041074498057399825, 0.0041079226086155485, 0.0041068199708813738, 0.004104629133365849, 0.0041015482340270842, 0.0041006653562397685, 0.0040932395975399882, 0.0040955588848407658, 0.0040964230709248008, 0.004098318674404829, 0.0040981241022533949, 0.0040947711719589754, 0.0040961672158851548, 0.0040966001882092188, 0.004097411344133898, 0.0040986443506921127, 0.0040924716248350382, 0.0040921283839013166, 0.0040921945560656488, 0.0040895227687037889, 0.0040885118162569548, 0.0040935370977361876, 0.0040923253344689976, 0.0040920093973061738, 0.0040929354484261363, 0.0040981931742892852, 0.0040996886154712397, 0.0041011043394443437, 0.0040973644365953693, 0.004095277258980126, 0.0040897493315094277, 0.0040861666663554886, 0.0040914494045082364, 0.0040930998820372377, 0.0040954995067631807, 0.0040976695381634417, 0.0040899844091603337, 0.0040854480196560452, 0.004084923094337215, 0.0040861224546873774, 0.0040893803377057324, 0.0040906903259077653, 0.0040901725257737912, 0.0040859985348246199, 0.0040851983503651353, 0.0040835350484961074, 0.004082463536168905, 0.0040840540400102392, 0.0040793412332388252, 0.0040765309197411765, 0.0040753681017476565, 0.0040798809142504888, 0.0040882211449676052, 0.0041010766790959188, 0.0041058368692917955, 0.0041017763880719527, 0.0041045920355508312, 0.0041020387000840538, 0.0041009659356559458, 0.0041053668333307343, 0.0041038300678266646, 0.0041065121956071144, 0.0041040259055784064, 0.0041089922760512423, 0.0041090986095470685, 0.0041074175564028226, 0.004107505861024807, 0.0041056541601257879, 0.0041070576599134372, 0.004114918909157576, 0.0041151911554786948, 0.0041104977667963151, 0.0041127034452775628, 0.0041124878902063459, 0.0041133384610420338, 0.0041155182092281663, 0.0041165321199834364, 0.0041196917694792, 0.0041096995882490533, 0.0041168775705990937, 0.0041177391600922368, 0.0041109195911115982, 0.0041096696687655679, 0.0041074399714756933, 0.0041039567026987505, 0.0041061723529687965, 0.0041059163988295269, 0.0041029952246520928, 0.0041042235659482732, 0.0041060008522655067, 0.0041070903977933866, 0.0041049742201431528, 0.0041050485483132455, 0.0041039293457107075, 0.004097320854076685, 0.0040980892775558785, 0.0040954840739229051, 0.0040986868513327229, 0.0040970780440727399, 0.0040941725576693899, 0.0040910029652487887, 0.0040911955146358432, 0.0040917791797880632, 0.0040876817710237138, 0.0040911772451359988, 0.0040937649865581769, 0.0040939579725034177, 0.0040880906047564254, 0.0040847533491130215, 0.0040887803934647167, 0.0040876435946040433, 0.0040862484979748061, 0.0040855688712208692, 0.0040832534839032666, 0.004083102452584004, 0.0040838663074712275, 0.0040809482802950901, 0.0040808594350839967, 0.0040821770042004839, 0.0040890547067563366, 0.0040897743699020014, 0.0040875096084104093, 0.0040925741445981883, 0.0040922173189430555, 0.0040900905868225951, 0.0040895407209192037, 0.0040914440367478553, 0.0040881409103713223, 0.0040817619767856929, 0.0040808033980674766, 0.0040800554660797113, 0.0040789950890607824, 0.0040826563124755477, 0.0040774184528697055, 0.0040791732512835358, 0.0040711460204209838, 0.0040688884476756584, 0.0040679038904096933, 0.00406461983931813, 0.0040642219534278816, 0.0040651842569692925, 0.0040656598425591484, 0.0040662064966451427, 0.0040628372838374073, 0.0040616455966886586, 0.004062134179200508, 0.0040621483568436768, 0.0040645701601543013, 0.004065117244655656, 0.0040683521371234436, 0.004071718139473054, 0.0040719759980289115, 0.0040711425705796077, 0.0040769261297914838, 0.0040758004050165378, 0.0040805187108473272, 0.0040734978497388909, 0.004074261979737784, 0.0040770867524033989, 0.0040759488674000669, 0.0040813078914016801, 0.0040806725360419249, 0.0040837071098342795, 0.0040814941322265394, 0.0040831516099233568, 0.0040810698268532106, 0.0040787090337458956, 0.0040779936536777415, 0.0040804081977857442, 0.0040827391549415702, 0.0040833295463894104, 0.0040874407914493404, 0.0040840405624593496, 0.0040810801381209703, 0.0040822901169962188, 0.0040871669897217687, 0.0040894494823205691, 0.0040958839552251037, 0.0040992696980510085, 0.0040982900918431174, 0.0041001558856701877, 0.0040954612996778432, 0.0040969676763060771, 0.004093582683048839, 0.00408933030009979, 0.0040949573901426263, 0.004095353081598742, 0.0040911614316714055, 0.0040885098420622122, 0.0040899115050278345, 0.0040888055505480395, 0.0040920037948255364, 0.0040948486859889877, 0.0040965436947373287, 0.0040991452633476946, 0.0041000192295573721, 0.0040984561474680927, 0.0040981211276930564, 0.0040939505688611438, 0.0040928614541900889, 0.0040917062432829282, 0.0040941032296587989, 0.0040958571599959692, 0.0040962229414240143, 0.0040972422966461947, 0.00409673463232876, 0.0041044240193550416, 0.0041023016207807931, 0.0041017691797947328, 0.0041012883746397661, 0.0041066131195460929, 0.0041052362684738586, 0.0041056983235491749, 0.0041082674473188127, 0.0041117638248856347, 0.0041075632504501839, 0.0041090847781231014, 0.0041060874338317054, 0.0041077693441831766, 0.0041059755492819172, 0.0041039595530019072, 0.0041006399004243736, 0.0041021094019578208, 0.004104196762698159, 0.0041105038565859165, 0.0041110164052980116, 0.0041123445081646575, 0.0041137991049838771, 0.0041152353052044338, 0.0041149213405980095, 0.0041146822826847078, 0.0041183879439774, 0.0041197275654179149, 0.0041189842489938672, 0.0041234779800880224, 0.0041297183928914695, 0.0041292244029886302, 0.0041370893189634564, 0.0041378010077677075, 0.0041391436720669252, 0.0041410153649450695, 0.0041418408295232139, 0.0041424660598888951, 0.0041449150868769543, 0.0041427256591972279, 0.0041409850685200133, 0.0041449341680087539, 0.0041463268964175978, 0.0041464869200748674, 0.0041493613408663966, 0.0041508309501155881, 0.0041520415800245748, 0.0041447199184286741, 0.004141495666628618, 0.0041392522319143215, 0.0041389759891037768, 0.0041393667000449441, 0.0041312225697611744, 0.0041305737893148203, 0.0041290408519443515, 0.0041298604779291285, 0.0041314645835313383, 0.0041302167546232883, 0.0041297616796073895, 0.0041291185281300278, 0.0041265678177375168, 0.0041290740477607363, 0.0041298272399878403, 0.0041353953474931116, 0.0041370474280692593, 0.004137294302251111, 0.0041328934522316361, 0.0041312077511909571, 0.0041297630136097798, 0.0041317207350064734, 0.0041332263251935347, 0.0041341157311620717, 0.0041364301974440576, 0.0041359603694609169, 0.0041371939764943758, 0.0041380203162048462, 0.0041347580485460445, 0.0041345210736703957, 0.0041379509722267709, 0.004132464129457304, 0.004132732500592394, 0.0041374634236176697, 0.0041411387706705152, 0.0041421774091416966, 0.0041415185267682795, 0.0041400686343773547, 0.0041402103745073679, 0.0041445102846992639, 0.0041440045048617997, 0.0041450413450715255, 0.0041487241263236481, 0.0041451163149261363, 0.004144121061673319, 0.0041455586439987941, 0.004145764023927332, 0.0041445145740668082, 0.0041403394483843512, 0.0041404581670122049, 0.0041376837099188577, 0.004135785975346612, 0.0041276726102485225, 0.0041283595821648075, 0.0041312075136983022, 0.0041282075243098612, 0.0041260803059474408, 0.0041280334434169947, 0.0041335119445993565, 0.0041347973049641901, 0.0041381451641848092, 0.0041406291094178297, 0.0041383452176559973, 0.0041416740365511299, 0.0041337269470842763, 0.0041400416228433001, 0.0041502420420701644, 0.0041535785902856613, 0.0041503286659040324, 0.0041502023395122919, 0.0041525588948347295, 0.0041594906644244166, 0.00416009363052773, 0.0041547844453782784, 0.0041568368556858856, 0.0041594394792902308, 0.0041604941896382181, 0.0041653495117422949, 0.004162582646824153, 0.0041591402478135166, 0.0041627014337829856, 0.0041572197377524522, 0.0041576711538511453, 0.0041537285175456138, 0.0041556617870094591, 0.0041551861503567877, 0.0041574632645767096, 0.0041526336766399951, 0.0041545604148376198, 0.0041571927031070419, 0.0041586907124524829, 0.0041599363308339023, 0.0041623214800878853, 0.0041624688008443742, 0.0041614867498157111, 0.0041647468749105177, 0.0041661765406955731, 0.0041612621625737659, 0.0041650231033801546, 0.0041659176475026495, 0.0041711422540617205, 0.0041661350264363597, 0.004164824589078105, 0.0041651828146127738, 0.0041689946893291828, 0.0041602068200824408, 0.0041614907891393965, 0.0041646239400084687, 0.0041629225194628533, 0.0041594969880749226, 0.0041625363607634873, 0.0041598771749597084, 0.004161709716502311, 0.0041588908114370991, 0.0041585944789337831, 0.004159110743533815, 0.0041621163779937062, 0.0041617124562684437, 0.0041584157075549568, 0.004159335566250014, 0.0041621565441061822, 0.0041605335150218838, 0.0041599291947345754, 0.0041593232242572633, 0.0041590475595330622, 0.0041590982948663154, 0.0041587424283913866, 0.0041567163093589182, 0.0041533662838803718, 0.0041518546817588906, 0.0041527203702074493, 0.0041507906075765053, 0.0041497401996442463, 0.0041531763996261531, 0.0041514218189944432, 0.0041548796634281408, 0.0041548347974319768, 0.004154059858836867, 0.0041568344058410967, 0.0041549008148478016, 0.0041523552572645447, 0.0041514704791444333, 0.0041516210108393922, 0.004147444024451827, 0.0041511100456499434, 0.004155290643329204, 0.0041602948857017725, 0.0041604806804840608, 0.0041619550858148892, 0.0041628641710927125, 0.0041611107370064394, 0.0041582005613598032, 0.0041597978141468027, 0.0041615589620167511, 0.0041624171018500647, 0.0041597566811650112, 0.0041542506634651361, 0.0041549066801118443, 0.0041594409238599897, 0.0041543478557007586, 0.0041598642920200263, 0.0041599982578547018, 0.0041596921533517475, 0.0041593705358764473, 0.0041575932508270642, 0.0041569005317410321, 0.0041532314441932892, 0.0041545325877207651, 0.0041546968803061269, 0.0041514844341652883, 0.0041523524009894862, 0.0041526305530171503, 0.0041535184385573789, 0.0041538106131049982, 0.0041540314913706758, 0.0041524379595138583, 0.0041543813636451439, 0.0041550505770363488, 0.0041599317145210009, 0.0041597335452537949, 0.0041521121692205164, 0.0041498559247970532, 0.0041457545247101379, 0.0041408116755820307, 0.0041459036372811986, 0.0041355961727889239, 0.0041370298035163337, 0.0041428879649473508, 0.0041426830816639719, 0.0041432265008024974, 0.0041412873103246189, 0.0041444705288087268, 0.0041429759316758755, 0.0041419531284013723, 0.0041433483057483518, 0.0041443879589567042, 0.0041425007940536608, 0.0041424042026596995, 0.0041394750218790369, 0.0041348669967219889, 0.0041367636004955213, 0.0041393981141242308, 0.0041388004902981976, 0.0041257964831385857, 0.0041247648256895458, 0.0041259837032703427, 0.0041265702985380671, 0.0041216774763803169, 0.0041213659028944746, 0.0041211349056245851, 0.0041249681328166237, 0.004123615245144879, 0.0041241376633384813, 0.0041237856934546371, 0.0041258175300671381, 0.0041215379970955691, 0.0041194005025260065, 0.0041174454474970012, 0.004118506516620375, 0.0041262535235232871, 0.0041262187833362848, 0.0041279170242520475, 0.0041329821420389148, 0.0041292032848243843, 0.0041251750506286324, 0.0041249370956604765, 0.0041247262867414565, 0.0041198131833622663, 0.0041195862195723115, 0.0041195793251764803, 0.0041173097272140507, 0.004116085788567707, 0.0041192708181372805, 0.0041167799605737743, 0.0041201409833664078, 0.0041215893919358381, 0.0041204491306746681, 0.0041173775954977121, 0.0041219921870340433, 0.0041243668873563351, 0.0041247725724461752, 0.0041263211746396748, 0.0041320711550648101, 0.0041324565504721198, 0.0041318139438764438, 0.0041289615020731318, 0.0041277028213538864, 0.0041296916323349505, 0.0041274698122257961, 0.0041264211337537215, 0.0041273804238683683, 0.0041292699088326433, 0.0041283279565713293, 0.0041285621043526729, 0.0041261325168177243, 0.0041269480947931647, 0.0041286798463846331, 0.0041285896733526803, 0.0041275589057450631, 0.0041262472416520157, 0.0041266449308321523, 0.0041261044886889099, 0.0041287323568808282, 0.0041268630342608066, 0.0041271698889537977, 0.0041242984818459947, 0.0041226717835659227, 0.0041233170098614628, 0.0041193740982431015, 0.0041222285466709736, 0.0041251956541538178, 0.0041252695287473376, 0.0041240815068348717, 0.0041268732981374172, 0.0041292876709645683, 0.0041251391093847078, 0.0041235554394378274, 0.0041246779064527854, 0.004121993520706857, 0.0041184579381388286, 0.0041209514676792524, 0.0041195923034120646, 0.0041190421242729347, 0.0041254034007210932, 0.0041237153560114165, 0.004121731914477525, 0.0041225392008101321, 0.0041224375097629311, 0.0041214600593397965, 0.0041220933425586545, 0.0041205352327263103, 0.0041222481224550177, 0.0041194453704447402, 0.0041169114045262953, 0.004117036973153518, 0.0041147526007004791, 0.0041163958678521097, 0.0041175247831781638, 0.0041153922532875392, 0.0041152804750902413, 0.0041151232319642343, 0.0041181545458570312, 0.0041121396084552279, 0.0041093284643011407, 0.004108878938159013, 0.0041041500918394206, 0.0041068111333851147, 0.0041069304688428239, 0.0041106285829082684, 0.0041050743327879493, 0.0041051759299150573, 0.0041052922110413797, 0.0041097648399002328, 0.0041019803004182015, 0.0041048263004858651, 0.004102764600239223, 0.0041010433774548776, 0.0041059734690363902, 0.0040991894733031931, 0.0041001471856685319, 0.004101495358906527, 0.0041005148298834514, 0.004101445560831537, 0.0041000473204796184, 0.0040994196636949176, 0.0040974157225061904, 0.0040988105690373793, 0.0040993383373658758, 0.0041033818537724431, 0.004099136769979808, 0.0041002754775745732, 0.0041030895552569766, 0.0041042191274178646, 0.0041077581552689648, 0.0041102681330333311, 0.0041102338935728967, 0.0041135446909697729, 0.0041086721653349487, 0.0041075856201947753, 0.0041054996639883498, 0.0041039869434019317, 0.004102734187471359, 0.0041051626819123077, 0.0041113431797706931, 0.0041108251295858119, 0.004110065502832703, 0.0041128207331201832, 0.0041125287932743714, 0.0041146331700089595, 0.0041111633898649634, 0.0041107897140041615, 0.0041058202747455055, 0.0041047047331959933, 0.0041069554998724488, 0.0041116120569319068, 0.0041085699798078471, 0.0041105450171816217]}
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cv_result</span><span class="p">[</span><span class="s1">&#39;auc-mean&#39;</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>2162
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[47]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Check AUC vs. iter</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cv_result</span><span class="p">[</span><span class="s1">&#39;auc-mean&#39;</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt output_prompt">Out[47]:</div>


<div class="output_text output_subarea output_execute_result">
<pre>[&lt;matplotlib.lines.Line2D at 0x119199518&gt;]</pre>
</div>

</div>

<div class="output_area"><div class="prompt"></div>


<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXcAAAD3CAYAAADmBxSSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[44]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Fit final LightGBM gradient-boosted tree model.</span>
<span class="c1"># Go ahead and use early stopping, although it&#39;s technically a minor source of overfitting.</span>
<span class="c1"># It would be more statistically &quot;correct&quot; to use CV rounds from above (2162), </span>
<span class="c1">#   but, empirically, using early stopping produces better results on e.g. Kaggle.</span>

<span class="n">gbm</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LGBMClassifier</span><span class="p">(</span>
    <span class="n">objective</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span>
    <span class="n">num_leaves</span><span class="o">=</span><span class="mi">127</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
    <span class="n">boosting_type</span><span class="o">=</span><span class="s1">&#39;gbdt&#39;</span><span class="p">,</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span>
    <span class="n">max_bin</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span>
    <span class="n">min_split_gain</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">min_child_weight</span><span class="o">=</span><span class="mf">6.5</span><span class="p">,</span>
    <span class="n">min_child_samples</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">subsample</span><span class="o">=.</span><span class="mi">76</span><span class="p">,</span>
    <span class="n">subsample_freq</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">colsample_bytree</span><span class="o">=.</span><span class="mi">73</span><span class="p">,</span>
    <span class="n">bin_construct_sample_cnt</span><span class="o">=</span><span class="mi">999999</span><span class="p">,</span> <span class="c1"># use entire dataset</span>
    <span class="n">min_data_in_bin</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">reg_alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">reg_lambda</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">nthread</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">silent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">gbm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_enc</span><span class="p">,</span> <span class="n">Y_train_enc</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
            <span class="n">eval_metric</span><span class="o">=</span><span class="s2">&quot;auc&quot;</span><span class="p">,</span> <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">X_test_enc</span><span class="p">,</span> <span class="n">Y_test_enc</span><span class="p">)])</span>

<span class="c1"># .825 AUC w/ 63 bins</span>
<span class="c1"># .8344 AUC w/ 255 bins</span>
<span class="c1"># .8338 AUC w/ 255 bins &amp; even lower lrate (.005)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Train until valid scores didn&#39;t improve in 40 rounds.
[40]	valid_0&#39;s auc: 0.778693
[80]	valid_0&#39;s auc: 0.781729
[120]	valid_0&#39;s auc: 0.784811
[160]	valid_0&#39;s auc: 0.788501
[200]	valid_0&#39;s auc: 0.791614
[240]	valid_0&#39;s auc: 0.794324
[280]	valid_0&#39;s auc: 0.797948
[320]	valid_0&#39;s auc: 0.800463
[360]	valid_0&#39;s auc: 0.802721
[400]	valid_0&#39;s auc: 0.804959
[440]	valid_0&#39;s auc: 0.807072
[480]	valid_0&#39;s auc: 0.80866
[520]	valid_0&#39;s auc: 0.81028
[560]	valid_0&#39;s auc: 0.811517
[600]	valid_0&#39;s auc: 0.812615
[640]	valid_0&#39;s auc: 0.813945
[680]	valid_0&#39;s auc: 0.814974
[720]	valid_0&#39;s auc: 0.815693
[760]	valid_0&#39;s auc: 0.816448
[800]	valid_0&#39;s auc: 0.816981
[840]	valid_0&#39;s auc: 0.817617
[880]	valid_0&#39;s auc: 0.818428
[920]	valid_0&#39;s auc: 0.819228
[960]	valid_0&#39;s auc: 0.81977
[1000]	valid_0&#39;s auc: 0.820248
[1040]	valid_0&#39;s auc: 0.820727
[1080]	valid_0&#39;s auc: 0.821267
[1120]	valid_0&#39;s auc: 0.821802
[1160]	valid_0&#39;s auc: 0.822288
[1200]	valid_0&#39;s auc: 0.822684
[1240]	valid_0&#39;s auc: 0.823015
[1280]	valid_0&#39;s auc: 0.823467
[1320]	valid_0&#39;s auc: 0.823928
[1360]	valid_0&#39;s auc: 0.824238
[1400]	valid_0&#39;s auc: 0.8247
[1440]	valid_0&#39;s auc: 0.825036
[1480]	valid_0&#39;s auc: 0.825325
[1520]	valid_0&#39;s auc: 0.825778
[1560]	valid_0&#39;s auc: 0.826156
[1600]	valid_0&#39;s auc: 0.826398
[1640]	valid_0&#39;s auc: 0.826728
[1680]	valid_0&#39;s auc: 0.827144
[1720]	valid_0&#39;s auc: 0.82741
[1760]	valid_0&#39;s auc: 0.827609
[1800]	valid_0&#39;s auc: 0.827835
[1840]	valid_0&#39;s auc: 0.828113
[1880]	valid_0&#39;s auc: 0.828347
[1920]	valid_0&#39;s auc: 0.828453
[1960]	valid_0&#39;s auc: 0.8285
[2000]	valid_0&#39;s auc: 0.828838
[2040]	valid_0&#39;s auc: 0.829177
[2080]	valid_0&#39;s auc: 0.829274
[2120]	valid_0&#39;s auc: 0.829447
[2160]	valid_0&#39;s auc: 0.829575
[2200]	valid_0&#39;s auc: 0.829745
[2240]	valid_0&#39;s auc: 0.830023
[2280]	valid_0&#39;s auc: 0.83021
[2320]	valid_0&#39;s auc: 0.830297
[2360]	valid_0&#39;s auc: 0.830392
[2400]	valid_0&#39;s auc: 0.83044
[2440]	valid_0&#39;s auc: 0.830568
[2480]	valid_0&#39;s auc: 0.830755
[2520]	valid_0&#39;s auc: 0.830954
[2560]	valid_0&#39;s auc: 0.830982
[2600]	valid_0&#39;s auc: 0.831125
[2640]	valid_0&#39;s auc: 0.831295
[2680]	valid_0&#39;s auc: 0.831433
[2720]	valid_0&#39;s auc: 0.831582
[2760]	valid_0&#39;s auc: 0.831722
[2800]	valid_0&#39;s auc: 0.831794
[2840]	valid_0&#39;s auc: 0.831921
[2880]	valid_0&#39;s auc: 0.832029
[2920]	valid_0&#39;s auc: 0.832183
[2960]	valid_0&#39;s auc: 0.832338
[3000]	valid_0&#39;s auc: 0.832413
[3040]	valid_0&#39;s auc: 0.83254
[3080]	valid_0&#39;s auc: 0.832563
[3120]	valid_0&#39;s auc: 0.832578
[3160]	valid_0&#39;s auc: 0.832753
[3200]	valid_0&#39;s auc: 0.832835
[3240]	valid_0&#39;s auc: 0.832998
[3280]	valid_0&#39;s auc: 0.833094
[3320]	valid_0&#39;s auc: 0.83323
[3360]	valid_0&#39;s auc: 0.833312
[3400]	valid_0&#39;s auc: 0.833413
[3440]	valid_0&#39;s auc: 0.833478
[3480]	valid_0&#39;s auc: 0.833581
[3520]	valid_0&#39;s auc: 0.833705
[3560]	valid_0&#39;s auc: 0.833778
[3600]	valid_0&#39;s auc: 0.833863
[3640]	valid_0&#39;s auc: 0.833975
[3680]	valid_0&#39;s auc: 0.834019
[3720]	valid_0&#39;s auc: 0.834104
[3760]	valid_0&#39;s auc: 0.834155
[3800]	valid_0&#39;s auc: 0.834218
[3840]	valid_0&#39;s auc: 0.834341
[3880]	valid_0&#39;s auc: 0.83434
[3920]	valid_0&#39;s auc: 0.834416
Early stopping, best iteration is:
[3909]	valid_0&#39;s auc: 0.834432
AUC Score:
0.834431701798

LogLoss:
0.391118041887

             precision    recall  f1-score   support

      False      0.835     0.967     0.896     25951
       True      0.779     0.381     0.512      8023

avg / total      0.822     0.828     0.805     33974

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[50]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">score_classif_on_test</span><span class="p">(</span><span class="n">gbm</span><span class="p">,</span> <span class="n">X_test_enc</span><span class="p">,</span> <span class="n">Y_test_enc</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>AUC Score:
0.833885028714

LogLoss:
0.391772387343

             precision    recall  f1-score   support

      False      0.834     0.969     0.896     25951
       True      0.788     0.378     0.511      8023

avg / total      0.823     0.829     0.805     33974

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Although LightGBM wasn't quite able to beat XGBoost on this dataset (.834 vs. .840 AUC), it came very close and while running several times faster. Random forest came pretty close (.817) with virtually zero tuning.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[51]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_test_enc</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
<span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Receiver Operating Characteristic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">false_positive_rate</span><span class="p">,</span> <span class="n">true_positive_rate</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span>
<span class="n">label</span><span class="o">=</span><span class="s1">&#39;AUC = </span><span class="si">%0.2f</span><span class="s1">&#39;</span><span class="o">%</span> <span class="n">roc_auc</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;r--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area"><div class="prompt"></div>


<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYAAAAETCAYAAAA/NdFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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==
"
>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>
</body>
</html>
